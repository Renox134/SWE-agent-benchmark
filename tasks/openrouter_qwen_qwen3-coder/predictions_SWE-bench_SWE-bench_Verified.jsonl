{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "matplotlib__matplotlib-26291", "model_patch": "diff --git a/lib/mpl_toolkits/axes_grid1/inset_locator.py b/lib/mpl_toolkits/axes_grid1/inset_locator.py\nindex 9d35051074..70132b0daa 100644\n--- a/lib/mpl_toolkits/axes_grid1/inset_locator.py\n+++ b/lib/mpl_toolkits/axes_grid1/inset_locator.py\n@@ -70,6 +70,9 @@ class AnchoredLocatorBase(AnchoredOffsetbox):\n \n     def __call__(self, ax, renderer):\n         self.axes = ax\n+        # Handle case where renderer is None and self.figure is None\n+        if renderer is None and self.figure is None and ax.figure is not None:\n+            renderer = ax.figure._get_renderer()\n         bbox = self.get_window_extent(renderer)\n         px, py = self.get_offset(bbox.width, bbox.height, 0, 0, renderer)\n         bbox_canvas = Bbox.from_bounds(px, py, bbox.width, bbox.height)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "matplotlib__matplotlib-26291", "model_patch": "diff --git a/lib/mpl_toolkits/axes_grid1/inset_locator.py b/lib/mpl_toolkits/axes_grid1/inset_locator.py\nindex 9d35051074..70132b0daa 100644\n--- a/lib/mpl_toolkits/axes_grid1/inset_locator.py\n+++ b/lib/mpl_toolkits/axes_grid1/inset_locator.py\n@@ -70,6 +70,9 @@ class AnchoredLocatorBase(AnchoredOffsetbox):\n \n     def __call__(self, ax, renderer):\n         self.axes = ax\n+        # Handle case where renderer is None and self.figure is None\n+        if renderer is None and self.figure is None and ax.figure is not None:\n+            renderer = ax.figure._get_renderer()\n         bbox = self.get_window_extent(renderer)\n         px, py = self.get_offset(bbox.width, bbox.height, 0, 0, renderer)\n         bbox_canvas = Bbox.from_bounds(px, py, bbox.width, bbox.height)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "matplotlib__matplotlib-26342", "model_patch": "diff --git a/lib/matplotlib/contour.py b/lib/matplotlib/contour.py\nindex 3f84250b64..2e3b35969b 100644\n--- a/lib/matplotlib/contour.py\n+++ b/lib/matplotlib/contour.py\n@@ -1475,6 +1475,30 @@ class ContourSet(ContourLabeler, mcoll.Collection):\n             ):\n                 super().draw(renderer)\n \n+    def set_paths(self, paths):\n+        \"\"\"\n+        Set the paths for the contour set.\n+        \n+        Parameters\n+        ----------\n+        paths : list of Path\n+            List of Path objects to set as the paths for this contour set.\n+        \"\"\"\n+        # Validate that paths is iterable\n+        try:\n+            iter(paths)\n+        except TypeError:\n+            raise TypeError(\"paths must be iterable\")\n+        \n+        # Convert to list and validate each element is a Path\n+        path_list = list(paths)\n+        for path in path_list:\n+            if not isinstance(path, Path):\n+                raise TypeError(\"All elements in paths must be Path objects\")\n+        \n+        self._paths = path_list\n+        self.stale = True\n+\n \n @_docstring.dedent_interpd\n class QuadContourSet(ContourSet):\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "matplotlib__matplotlib-26466", "model_patch": "diff --git a/lib/matplotlib/text.py b/lib/matplotlib/text.py\nindex d4922fc517..a0858e6205 100644\n--- a/lib/matplotlib/text.py\n+++ b/lib/matplotlib/text.py\n@@ -1456,7 +1456,7 @@ class _AnnotationBase:\n                  xycoords='data',\n                  annotation_clip=None):\n \n-        self.xy = xy\n+        self.xy = np.array(xy) if isinstance(xy, (list, tuple, np.ndarray)) else xy\n         self.xycoords = xycoords\n         self.set_annotation_clip(annotation_clip)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "mwaskom__seaborn-3187", "model_patch": "diff --git a/seaborn/_core/plot.py b/seaborn/_core/plot.py\nindex 8915616b..5ec74e6e 100644\n--- a/seaborn/_core/plot.py\n+++ b/seaborn/_core/plot.py\n@@ -1561,7 +1561,13 @@ class Plotter:\n         for var in legend_vars:\n             var_legend = scales[var]._legend\n             if var_legend is not None:\n-                values, labels = var_legend\n+                # Unpack the legend information\n+                if len(var_legend) == 3:  # New format with offset\n+                    values, labels, offset = var_legend\n+                else:  # Old format without offset\n+                    values, labels = var_legend\n+                    offset = None\n+                \n                 for (_, part_id), part_vars, _ in schema:\n                     if data.ids[var] == part_id:\n                         # Allow multiple plot semantics to represent same data variable\n@@ -1569,6 +1575,9 @@ class Plotter:\n                         break\n                 else:\n                     title = self._resolve_label(p, var, data.names[var])\n+                    # Add offset information to the title if available\n+                    if offset:\n+                        title = f\"{title} (+{offset})\"\n                     entry = (title, data.ids[var]), [var], (values, labels)\n                     schema.append(entry)\n \ndiff --git a/seaborn/_core/scales.py b/seaborn/_core/scales.py\nindex 55874bf2..6c716d3b 100644\n--- a/seaborn/_core/scales.py\n+++ b/seaborn/_core/scales.py\n@@ -379,7 +379,13 @@ class ContinuousBase(Scale):\n             locs = axis.major.locator()\n             locs = locs[(vmin <= locs) & (locs <= vmax)]\n             labels = axis.major.formatter.format_ticks(locs)\n-            new._legend = list(locs), list(labels)\n+            \n+            # Handle offset notation for ScalarFormatter\n+            offset = None\n+            if isinstance(axis.major.formatter, mpl.ticker.ScalarFormatter):\n+                offset = axis.major.formatter.get_offset()\n+            \n+            new._legend = list(locs), list(labels), offset\n \n         return new\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pallets__flask-5014", "model_patch": "diff --git a/src/flask/blueprints.py b/src/flask/blueprints.py\nindex eb50585d..d69945e8 100644\n--- a/src/flask/blueprints.py\n+++ b/src/flask/blueprints.py\n@@ -190,6 +190,9 @@ class Blueprint(Scaffold):\n             root_path=root_path,\n         )\n \n+        if not name or not name.strip():\n+            raise ValueError(\"'name' may not be empty.\")\n+        \n         if \".\" in name:\n             raise ValueError(\"'name' may not contain a dot '.' character.\")\n \ndiff --git a/tests/test_blueprints.py b/tests/test_blueprints.py\nindex d46b769b..32bb9880 100644\n--- a/tests/test_blueprints.py\n+++ b/tests/test_blueprints.py\n@@ -256,6 +256,13 @@ def test_dotted_name_not_allowed(app, client):\n         flask.Blueprint(\"app.ui\", __name__)\n \n \n+def test_empty_name_not_allowed(app, client):\n+    with pytest.raises(ValueError):\n+        flask.Blueprint(\"\", __name__)\n+    with pytest.raises(ValueError):\n+        flask.Blueprint(\"   \", __name__)\n+\n+\n def test_dotted_names_from_app(app, client):\n     test = flask.Blueprint(\"test\", __name__)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-1142", "model_patch": "diff --git a/requests/models.py b/requests/models.py\nindex 9926045..9238e6f 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -358,10 +358,14 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n             if files:\n                 raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n \n-            if length:\n-                self.headers['Content-Length'] = length\n+            if length and length != 'None':\n+                # Only set Content-Length for methods that can have a body\n+                if self.method not in ('GET', 'HEAD', 'OPTIONS'):\n+                    self.headers['Content-Length'] = length\n             else:\n-                self.headers['Transfer-Encoding'] = 'chunked'\n+                # Only set Transfer-Encoding for methods that can have a body\n+                if self.method not in ('GET', 'HEAD', 'OPTIONS'):\n+                    self.headers['Transfer-Encoding'] = 'chunked'\n         # Check if file, fo, generator, iterator.\n         # If not, run through normal process.\n \n@@ -377,7 +381,9 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n                     else:\n                         content_type = 'application/x-www-form-urlencoded'\n \n-            self.prepare_content_length(body)\n+            # Only set Content-Length for methods that can have a body\n+            if self.method not in ('GET', 'HEAD', 'OPTIONS'):\n+                self.prepare_content_length(body)\n \n             # Add content-type if it wasn't explicitly provided.\n             if (content_type) and (not 'content-type' in self.headers):\n@@ -386,13 +392,14 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n         self.body = body\n \n     def prepare_content_length(self, body):\n-        self.headers['Content-Length'] = '0'\n-        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n-            body.seek(0, 2)\n-            self.headers['Content-Length'] = str(body.tell())\n-            body.seek(0, 0)\n-        elif body is not None:\n-            self.headers['Content-Length'] = str(len(body))\n+        if body is not None:\n+            self.headers['Content-Length'] = '0'\n+            if hasattr(body, 'seek') and hasattr(body, 'tell'):\n+                body.seek(0, 2)\n+                self.headers['Content-Length'] = str(body.tell())\n+                body.seek(0, 0)\n+            else:\n+                self.headers['Content-Length'] = str(len(body))\n \n     def prepare_auth(self, auth):\n         \"\"\"Prepares the given HTTP auth data.\"\"\"\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-1724", "model_patch": "diff --git a/requests/models.py b/requests/models.py\nindex f82f56a..6dba959 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -307,7 +307,8 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n         \"\"\"Prepares the given HTTP method.\"\"\"\n         self.method = method\n         if self.method is not None:\n-            self.method = self.method.upper()\n+            # Convert to string first, then upper case, then to native string\n+            self.method = to_native_string(str(self.method).upper())\n \n     def prepare_url(self, url, params):\n         \"\"\"Prepares the given HTTP URL.\"\"\"\ndiff --git a/requests/sessions.py b/requests/sessions.py\nindex cc72f65..57ae31a 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -16,7 +16,7 @@ from .compat import cookielib, OrderedDict, urljoin, urlparse, urlunparse\n from .cookies import cookiejar_from_dict, extract_cookies_to_jar, RequestsCookieJar\n from .models import Request, PreparedRequest\n from .hooks import default_hooks, dispatch_hook\n-from .utils import to_key_val_list, default_headers\n+from .utils import to_key_val_list, default_headers, to_native_string\n from .exceptions import TooManyRedirects, InvalidSchema\n from .structures import CaseInsensitiveDict\n \n@@ -311,7 +311,7 @@ class Session(SessionRedirectMixin):\n         \"\"\"\n         # Create the Request.\n         req = Request(\n-            method = method.upper(),\n+            method = to_native_string(str(method).upper()),\n             url = url,\n             headers = headers,\n             files = files,\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-1766", "model_patch": "diff --git a/requests/auth.py b/requests/auth.py\nindex a3de123..48416e5 100644\n--- a/requests/auth.py\n+++ b/requests/auth.py\n@@ -144,7 +144,7 @@ class HTTPDigestAuth(AuthBase):\n         if entdig:\n             base += ', digest=\"%s\"' % entdig\n         if qop:\n-            base += ', qop=auth, nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n+            base += ', qop=\"auth\", nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n \n         return 'Digest %s' % (base)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-1921", "model_patch": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex a023e4e..4a9ac9e 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -54,10 +54,16 @@ def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n     merged_setting = dict_class(to_key_val_list(session_setting))\n     merged_setting.update(to_key_val_list(request_setting))\n \n-    # Remove keys that are set to None.\n+    # Remove keys that are set to None in either request or session settings.\n     for (k, v) in request_setting.items():\n         if v is None:\n             del merged_setting[k]\n+    \n+    for (k, v) in session_setting.items():\n+        if v is None:\n+            # Only delete if not explicitly set in request\n+            if k not in request_setting:\n+                del merged_setting[k]\n \n     return merged_setting\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-2317", "model_patch": ""}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-2931", "model_patch": "diff --git a/requests/models.py b/requests/models.py\nindex 9c624d3..0e5c3de 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -444,11 +444,15 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n                 (body, content_type) = self._encode_files(files, data)\n             else:\n                 if data:\n-                    body = self._encode_params(data)\n-                    if isinstance(data, basestring) or hasattr(data, 'read'):\n+                    if isinstance(data, bytes):\n+                        body = data\n                         content_type = None\n                     else:\n-                        content_type = 'application/x-www-form-urlencoded'\n+                        body = self._encode_params(data)\n+                        if isinstance(data, basestring) or hasattr(data, 'read'):\n+                            content_type = None\n+                        else:\n+                            content_type = 'application/x-www-form-urlencoded'\n \n             self.prepare_content_length(body)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-5414", "model_patch": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex fe22ff45..36b0be17 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -28,6 +28,7 @@ from urllib3.exceptions import ReadTimeoutError\n from urllib3.exceptions import SSLError as _SSLError\n from urllib3.exceptions import ResponseError\n from urllib3.exceptions import LocationValueError\n+from urllib3.exceptions import LocationParseError\n \n from .models import Response\n from .compat import urlparse, basestring\n@@ -524,6 +525,9 @@ class HTTPAdapter(BaseAdapter):\n         except _ProxyError as e:\n             raise ProxyError(e)\n \n+        except LocationParseError as e:\n+            raise InvalidURL('URL has an invalid label.', request=request)\n+\n         except (_SSLError, _HTTPError) as e:\n             if isinstance(e, _SSLError):\n                 # This branch is for urllib3 versions earlier than v1.22\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "psf__requests-6028", "model_patch": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex fe22ff45..ca04ba0f 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -178,6 +178,27 @@ class HTTPAdapter(BaseAdapter):\n         \"\"\"\n         if proxy in self.proxy_manager:\n             manager = self.proxy_manager[proxy]\n+            # Ensure proxy headers are up-to-date\n+            proxy_headers = self.proxy_headers(proxy)\n+            # Update proxy headers for HTTP/HTTPS proxies\n+            if hasattr(manager, 'proxy_headers'):\n+                manager.proxy_headers = proxy_headers\n+            # For SOCKS proxies, we might need to recreate the manager if auth changes\n+            elif proxy.lower().startswith('socks'):\n+                cached_username = getattr(manager, 'username', None)\n+                cached_password = getattr(manager, 'password', None)\n+                username, password = get_auth_from_url(proxy)\n+                if cached_username != username or cached_password != password:\n+                    # Recreate the manager with new credentials\n+                    manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n+                        proxy,\n+                        username=username,\n+                        password=password,\n+                        num_pools=self._pool_connections,\n+                        maxsize=self._pool_maxsize,\n+                        block=self._pool_block,\n+                        **proxy_kwargs\n+                    )\n         elif proxy.lower().startswith('socks'):\n             username, password = get_auth_from_url(proxy)\n             manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-2905", "model_patch": "diff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex 45553eb9..87eda607 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -218,7 +218,13 @@ def as_compatible_data(data, fastpath=False):\n         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n \n     # we don't want nested self-described arrays\n-    data = getattr(data, \"values\", data)\n+    # Only extract values from DataArray and Variable objects\n+    if hasattr(data, \"values\") and (hasattr(data, \"__class__\") and \n+                                    (data.__class__.__name__ in (\"DataArray\", \"Variable\") or\n+                                     (hasattr(data.__class__, \"__module__\") and \n+                                      data.__class__.__module__ is not None and\n+                                      \"xarray\" in data.__class__.__module__))):\n+        data = data.values\n \n     if isinstance(data, np.ma.MaskedArray):\n         mask = np.ma.getmaskarray(data)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-3095", "model_patch": "diff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex 379be8f5..3f1bcfa6 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -1947,7 +1947,9 @@ class IndexVariable(Variable):\n                 # point, which doesn't have a copy method, so make a deep copy\n                 # of the underlying `pandas.MultiIndex` and create a new\n                 # `PandasIndexAdapter` instance with it.\n-                data = PandasIndexAdapter(self._data.array.copy(deep=True))\n+                # Preserve the original dtype to avoid casting unicode to object\n+                data = PandasIndexAdapter(self._data.array.copy(deep=True), \n+                                          dtype=self._data.dtype)\n             else:\n                 data = self._data\n         else:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-3151", "model_patch": "diff --git a/xarray/core/combine.py b/xarray/core/combine.py\nindex 37ae903b..9eab67e6 100644\n--- a/xarray/core/combine.py\n+++ b/xarray/core/combine.py\n@@ -501,7 +501,8 @@ def combine_by_coords(datasets, compat='no_conflicts', data_vars='all',\n                                    fill_value=fill_value)\n \n         # Check the overall coordinates are monotonically increasing\n-        for dim in concatenated.dims:\n+        # Only check dimensions that were actually concatenated (i.e., varied between datasets)\n+        for dim in concat_dims:\n             if dim in concatenated:\n                 indexes = concatenated.indexes.get(dim)\n                 if not (indexes.is_monotonic_increasing\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-3305", "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..3ee7b371\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,8 @@\n+import xarray as xr\n+\n+# Reproduce the issue\n+da = xr.DataArray([0, 0], dims=\"x\", attrs={'units':'K'})\n+out = da.quantile(.9, dim='x', keep_attrs=True)\n+print(\"Attributes:\", out.attrs)\n+print(\"Expected: OrderedDict([('units', 'K')])\")\n+print(\"Actual:\", out.attrs)\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-3677", "model_patch": "diff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex 10c7804d..c164c487 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -809,6 +809,13 @@ def dataset_merge_method(\n     # method due for backwards compatibility\n     # TODO: consider deprecating it?\n \n+    from .dataarray import DataArray\n+    from .dataset import Dataset\n+\n+    # Convert DataArray to Dataset if needed\n+    if isinstance(other, DataArray):\n+        other = other.to_dataset()\n+\n     if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):\n         overwrite_vars = set(overwrite_vars)\n     else:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-3993", "model_patch": "diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex 2fef3edb..33eb8040 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -3481,18 +3481,20 @@ class DataArray(AbstractArray, DataWithCoords):\n         return self._from_temp_dataset(ds)\n \n     def integrate(\n-        self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n+        self, coord=None, dim=None, datetime_unit: str = None\n     ) -> \"DataArray\":\n         \"\"\" integrate the array with the trapezoidal rule.\n \n         .. note::\n-            This feature is limited to simple cartesian geometry, i.e. dim\n+            This feature is limited to simple cartesian geometry, i.e. coord\n             must be one dimensional.\n \n         Parameters\n         ----------\n-        dim : hashable, or sequence of hashable\n+        coord : hashable, or sequence of hashable\n             Coordinate(s) used for the integration.\n+        dim : hashable, or sequence of hashable, optional\n+            Deprecated. Use `coord` instead.\n         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \\\n                          \"ps\", \"fs\", \"as\"}, optional\n             Can be used to specify the unit if datetime coordinate is used.\n@@ -3523,12 +3525,28 @@ class DataArray(AbstractArray, DataWithCoords):\n           * x        (x) float64 0.0 0.1 1.1 1.2\n         Dimensions without coordinates: y\n         >>>\n-        >>> da.integrate(\"x\")\n+        >>> da.integrate(coord=\"x\")\n         <xarray.DataArray (y: 3)>\n         array([5.4, 6.6, 7.8])\n         Dimensions without coordinates: y\n         \"\"\"\n-        ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n+        # Handle deprecation of 'dim' parameter\n+        if dim is not None:\n+            import warnings\n+            warnings.warn(\n+                \"The `dim` parameter in DataArray.integrate is deprecated and will be removed \"\n+                \"in a future version. Please use `coord` instead.\",\n+                FutureWarning,\n+                stacklevel=2\n+            )\n+            if coord is not None:\n+                raise ValueError(\"Cannot specify both 'coord' and 'dim' parameters.\")\n+            coord = dim\n+        \n+        if coord is None:\n+            raise ValueError(\"Must specify 'coord' parameter.\")\n+            \n+        ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n         return self._from_temp_dataset(ds)\n \n     def unify_chunks(self) -> \"DataArray\":\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4075", "model_patch": "diff --git a/xarray/core/weighted.py b/xarray/core/weighted.py\nindex 996d2e4c..b99ce305 100644\n--- a/xarray/core/weighted.py\n+++ b/xarray/core/weighted.py\n@@ -142,7 +142,12 @@ class Weighted:\n         # we need to mask data values that are nan; else the weights are wrong\n         mask = da.notnull()\n \n-        sum_of_weights = self._reduce(mask, self.weights, dim=dim, skipna=False)\n+        # Convert weights to float to ensure correct calculation with boolean weights\n+        weights = self.weights\n+        if weights.dtype == bool:\n+            weights = weights * 1.0\n+\n+        sum_of_weights = self._reduce(mask, weights, dim=dim, skipna=False)\n \n         # 0-weights are not valid\n         valid_weights = sum_of_weights != 0.0\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4094", "model_patch": "diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex b0df8749..eaa28e81 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -1961,7 +1961,14 @@ class DataArray(AbstractArray, DataWithCoords):\n         # pull variables out of datarray\n         data_dict = {}\n         for k in variables:\n-            data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n+            # Select the variable and drop the stacked coordinate\n+            selected = self.sel({variable_dim: k})\n+            squeezed = selected.squeeze(drop=True)\n+            # Drop the stacked coordinate that's not a dimension, if it exists\n+            if dim in squeezed.coords:\n+                data_dict[k] = squeezed.drop_vars(dim)\n+            else:\n+                data_dict[k] = squeezed\n \n         # unstacked dataset\n         return Dataset(data_dict)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4356", "model_patch": "diff --git a/xarray/core/nanops.py b/xarray/core/nanops.py\nindex 41c8d258..c6c6b817 100644\n--- a/xarray/core/nanops.py\n+++ b/xarray/core/nanops.py\n@@ -26,21 +26,42 @@ def _maybe_null_out(result, axis, mask, min_count=1):\n     \"\"\"\n     xarray version of pandas.core.nanops._maybe_null_out\n     \"\"\"\n-    if hasattr(axis, \"__len__\"):  # if tuple or list\n-        raise ValueError(\n-            \"min_count is not available for reduction with more than one dimensions.\"\n-        )\n-\n     if axis is not None and getattr(result, \"ndim\", False):\n-        null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\n+        # Handle both single axis and multiple axes\n+        if hasattr(axis, \"__len__\"):  # if tuple or list\n+            # For multiple axes, calculate the product of the dimensions being reduced\n+            reduced_size = np.prod(np.take(mask.shape, axis))\n+            valid_count = reduced_size - mask.sum(axis=axis)\n+            null_mask = (valid_count - min_count) < 0\n+        else:\n+            # For single axis, use the original logic\n+            valid_count = mask.shape[axis] - mask.sum(axis)\n+            null_mask = (valid_count - min_count) < 0\n+        \n         if null_mask.any():\n             dtype, fill_value = dtypes.maybe_promote(result.dtype)\n             result = result.astype(dtype)\n             result[null_mask] = fill_value\n \n+    elif axis is not None:  # For scalar results (0-dimensional)\n+        # Handle both single axis and multiple axes for scalar results\n+        if hasattr(axis, \"__len__\"):  # if tuple or list\n+            # For multiple axes, calculate the product of the dimensions being reduced\n+            reduced_size = np.prod(np.take(mask.shape, axis))\n+            valid_count = reduced_size - mask.sum(axis=axis)\n+            null_mask = (valid_count - min_count) < 0\n+        else:\n+            # For single axis, use the original logic\n+            valid_count = mask.shape[axis] - mask.sum(axis)\n+            null_mask = (valid_count - min_count) < 0\n+        \n+        if null_mask:\n+            result = np.nan\n+\n     elif getattr(result, \"dtype\", None) not in dtypes.NAT_TYPES:\n-        null_mask = mask.size - mask.sum()\n-        if null_mask < min_count:\n+        valid_count = mask.size - mask.sum()\n+        null_mask = (valid_count - min_count) < 0\n+        if null_mask:\n             result = np.nan\n \n     return result\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4629", "model_patch": "diff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex ca4e29b6..dff00804 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -501,7 +501,7 @@ def merge_attrs(variable_attrs, combine_attrs):\n     if combine_attrs == \"drop\":\n         return {}\n     elif combine_attrs == \"override\":\n-        return variable_attrs[0]\n+        return dict(variable_attrs[0])\n     elif combine_attrs == \"no_conflicts\":\n         result = dict(variable_attrs[0])\n         for attrs in variable_attrs[1:]:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4687", "model_patch": "diff --git a/xarray/core/computation.py b/xarray/core/computation.py\nindex 9fe93c88..eb2fb675 100644\n--- a/xarray/core/computation.py\n+++ b/xarray/core/computation.py\n@@ -1727,7 +1727,7 @@ def dot(*arrays, dims=None, **kwargs):\n     return result.transpose(*all_dims, missing_dims=\"ignore\")\n \n \n-def where(cond, x, y):\n+def where(cond, x, y, keep_attrs=None):\n     \"\"\"Return elements from `x` or `y` depending on `cond`.\n \n     Performs xarray-like broadcasting across input arguments.\n@@ -1743,6 +1743,10 @@ def where(cond, x, y):\n         values to choose from where `cond` is True\n     y : scalar, array, Variable, DataArray or Dataset\n         values to choose from where `cond` is False\n+    keep_attrs : bool, optional\n+        If True, the attributes (``attrs``) will be copied from the first\n+        array to the new one. If False (default), the new object will be\n+        returned without attributes.\n \n     Returns\n     -------\n@@ -1809,7 +1813,10 @@ def where(cond, x, y):\n         equivalent methods\n     \"\"\"\n     # alignment for three arguments is complicated, so don't support it yet\n-    return apply_ufunc(\n+    if keep_attrs is None:\n+        keep_attrs = _get_keep_attrs(default=False)\n+        \n+    result = apply_ufunc(\n         duck_array_ops.where,\n         cond,\n         x,\n@@ -1817,7 +1824,19 @@ def where(cond, x, y):\n         join=\"exact\",\n         dataset_join=\"exact\",\n         dask=\"allowed\",\n+        keep_attrs=keep_attrs,\n     )\n+    \n+    # For where, we want to take attributes from the first DataArray argument\n+    # that has non-empty attributes if keep_attrs is True\n+    if keep_attrs:\n+        # Check all arguments for DataArrays with attributes\n+        for arg in [cond, x, y]:\n+            if hasattr(arg, \"attrs\") and arg.attrs:\n+                result.attrs = arg.attrs.copy()\n+                break\n+        \n+    return result\n \n \n def polyval(coord, coeffs, degree_dim=\"degree\"):\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4695", "model_patch": "diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex 32b24ef1..c16f39c9 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -196,7 +196,29 @@ class _LocIndexer:\n             # expand the indexer so we can handle Ellipsis\n             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n             key = dict(zip(self.data_array.dims, labels))\n-        return self.data_array.sel(**key)\n+        \n+        # Handle the case where a dimension name conflicts with a parameter name\n+        # For example, if we have a dimension named \"method\", we need to pass it\n+        # as a positional argument to avoid conflict with the method parameter\n+        sel_kwargs = {}\n+        sel_indexers = {}\n+        for k, v in key.items():\n+            if k in [\"method\", \"tolerance\", \"drop\"]:\n+                # These are parameter names of the sel method\n+                if k in self.data_array.dims:\n+                    # This is actually a dimension name, so we need to pass it as an indexer\n+                    sel_indexers[k] = v\n+                else:\n+                    # This is actually a parameter, so we pass it as a keyword argument\n+                    sel_kwargs[k] = v\n+            else:\n+                # This is not a parameter name, so we pass it as an indexer\n+                sel_indexers[k] = v\n+        \n+        if sel_indexers:\n+            return self.data_array.sel(indexers=sel_indexers, **sel_kwargs)\n+        else:\n+            return self.data_array.sel(**sel_kwargs)\n \n     def __setitem__(self, key, value) -> None:\n         if not utils.is_dict_like(key):\ndiff --git a/xarray/core/indexing.py b/xarray/core/indexing.py\nindex 843feb04..45102ae4 100644\n--- a/xarray/core/indexing.py\n+++ b/xarray/core/indexing.py\n@@ -105,7 +105,7 @@ def get_indexer_nd(index, labels, method=None, tolerance=None):\n     return indexer\n \n \n-def convert_label_indexer(index, label, index_name=\"\", method=None, tolerance=None):\n+def convert_label_indexer(index, label, index_name=\"\", indexer_method=None, tolerance=None):\n     \"\"\"Given a pandas.Index and labels (e.g., from __getitem__) for one\n     dimension, return an indexer suitable for indexing an ndarray along that\n     dimension. If `index` is a pandas.MultiIndex and depending on `label`,\n@@ -114,7 +114,7 @@ def convert_label_indexer(index, label, index_name=\"\", method=None, tolerance=No\n     new_index = None\n \n     if isinstance(label, slice):\n-        if method is not None or tolerance is not None:\n+        if indexer_method is not None or tolerance is not None:\n             raise NotImplementedError(\n                 \"cannot use ``method`` argument if any indexers are slice objects\"\n             )\n@@ -178,7 +178,7 @@ def convert_label_indexer(index, label, index_name=\"\", method=None, tolerance=No\n             if isinstance(index, pd.MultiIndex):\n                 indexer, new_index = index.get_loc_level(label_value, level=0)\n             elif isinstance(index, pd.CategoricalIndex):\n-                if method is not None:\n+                if indexer_method is not None:\n                     raise ValueError(\n                         \"'method' is not a valid kwarg when indexing using a CategoricalIndex.\"\n                     )\n@@ -188,7 +188,7 @@ def convert_label_indexer(index, label, index_name=\"\", method=None, tolerance=No\n                     )\n                 indexer = index.get_loc(label_value)\n             else:\n-                indexer = index.get_loc(label_value, method=method, tolerance=tolerance)\n+                indexer = index.get_loc(label_value, method=indexer_method, tolerance=tolerance)\n         elif label.dtype.kind == \"b\":\n             indexer = label\n         else:\n@@ -197,7 +197,7 @@ def convert_label_indexer(index, label, index_name=\"\", method=None, tolerance=No\n                     \"Vectorized selection is not available along \"\n                     \"MultiIndex variable: \" + index_name\n                 )\n-            indexer = get_indexer_nd(index, label, method, tolerance)\n+            indexer = get_indexer_nd(index, label, indexer_method, tolerance)\n             if np.any(indexer < 0):\n                 raise KeyError(f\"not all values found in index {index_name!r}\")\n     return indexer, new_index\n@@ -267,7 +267,7 @@ def remap_label_indexers(data_obj, indexers, method=None, tolerance=None):\n         else:\n             coords_dtype = data_obj.coords[dim].dtype\n             label = maybe_cast_to_coords_dtype(label, coords_dtype)\n-            idxr, new_idx = convert_label_indexer(index, label, dim, method, tolerance)\n+            idxr, new_idx = convert_label_indexer(index, label, dim, indexer_method=method, tolerance=tolerance)\n             pos_indexers[dim] = idxr\n             if new_idx is not None:\n                 new_indexes[dim] = new_idx\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-4966", "model_patch": "diff --git a/xarray/coding/variables.py b/xarray/coding/variables.py\nindex b035ff82..938752c4 100644\n--- a/xarray/coding/variables.py\n+++ b/xarray/coding/variables.py\n@@ -316,6 +316,14 @@ class UnsignedIntegerCoder(VariableCoder):\n                     if \"_FillValue\" in attrs:\n                         new_fill = unsigned_dtype.type(attrs[\"_FillValue\"])\n                         attrs[\"_FillValue\"] = new_fill\n+            elif data.dtype.kind == \"u\":\n+                if unsigned == \"false\":\n+                    signed_dtype = np.dtype(\"i%s\" % data.dtype.itemsize)\n+                    transform = partial(np.asarray, dtype=signed_dtype)\n+                    data = lazy_elemwise_func(data, transform, signed_dtype)\n+                    if \"_FillValue\" in attrs:\n+                        new_fill = signed_dtype.type(attrs[\"_FillValue\"])\n+                        attrs[\"_FillValue\"] = new_fill\n             else:\n                 warnings.warn(\n                     \"variable %r has _Unsigned attribute but is not \"\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6461", "model_patch": "diff --git a/xarray/core/computation.py b/xarray/core/computation.py\nindex 7676d8e5..885c1b51 100644\n--- a/xarray/core/computation.py\n+++ b/xarray/core/computation.py\n@@ -1829,7 +1829,7 @@ def where(cond, x, y, keep_attrs=None):\n     if keep_attrs is True:\n         # keep the attributes of x, the second parameter, by default to\n         # be consistent with the `where` method of `DataArray` and `Dataset`\n-        keep_attrs = lambda attrs, context: attrs[1]\n+        keep_attrs = lambda attrs, context: attrs[1] if len(attrs) > 1 else {}\n \n     # alignment for three arguments is complicated, so don't support it yet\n     return apply_ufunc(\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6599", "model_patch": "diff --git a/xarray/core/computation.py b/xarray/core/computation.py\nindex 823cbe02..cb8aba70 100644\n--- a/xarray/core/computation.py\n+++ b/xarray/core/computation.py\n@@ -1892,6 +1892,7 @@ def polyval(\n     xarray.DataArray.polyfit\n     numpy.polynomial.polynomial.polyval\n     \"\"\"\n+    from .dataarray import DataArray\n \n     if degree_dim not in coeffs._indexes:\n         raise ValueError(\n@@ -1905,20 +1906,47 @@ def polyval(\n     coeffs = coeffs.reindex(\n         {degree_dim: np.arange(max_deg + 1)}, fill_value=0, copy=False\n     )\n-    coord = _ensure_numeric(coord)  # type: ignore # https://github.com/python/mypy/issues/1533 ?\n+    \n+    # Extract coordinate values for evaluation while preserving original coordinates\n+    original_coords = None\n+    if isinstance(coord, DataArray):\n+        # For DataArray, use the coordinate values, not the data values\n+        if len(coord.dims) == 1:\n+            dim_name = coord.dims[0]\n+            if dim_name in coord.coords:\n+                # Store original coordinates to preserve them in the result\n+                original_coords = {dim_name: coord.coords[dim_name]}\n+                # Use the coordinate values for this dimension for computation\n+                coord_values = coord.coords[dim_name]\n+                # Apply numeric conversion to coordinate values for computation\n+                coord_for_computation = _ensure_numeric(coord_values)\n+            else:\n+                # Fallback to using the data values\n+                coord_for_computation = _ensure_numeric(coord)\n+        else:\n+            # For multi-dimensional DataArrays, use the data values\n+            coord_for_computation = _ensure_numeric(coord)\n+    else:\n+        # For Dataset or already processed coordinates, apply numeric conversion\n+        coord_for_computation = _ensure_numeric(coord)\n+        coord = coord_for_computation\n \n     # using Horner's method\n     # https://en.wikipedia.org/wiki/Horner%27s_method\n-    res = coeffs.isel({degree_dim: max_deg}, drop=True) + zeros_like(coord)\n+    res = coeffs.isel({degree_dim: max_deg}, drop=True) + zeros_like(coord_for_computation)\n     for deg in range(max_deg - 1, -1, -1):\n-        res *= coord\n+        res *= coord_for_computation\n         res += coeffs.isel({degree_dim: deg}, drop=True)\n \n+    # Restore original coordinates if we modified them\n+    if original_coords is not None and isinstance(res, DataArray):\n+        res = res.assign_coords(original_coords)\n+\n     return res\n \n \n def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n-    \"\"\"Converts all datetime64 variables to float64\n+    \"\"\"Converts all datetime64 and timedelta64 variables to float64\n \n     Parameters\n     ----------\n@@ -1928,12 +1956,13 @@ def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n     Returns\n     -------\n     DataArray or Dataset\n-        Variables with datetime64 dtypes converted to float64.\n+        Variables with datetime64/timedelta64 dtypes converted to float64.\n     \"\"\"\n     from .dataset import Dataset\n+    from .dataarray import DataArray\n \n     def to_floatable(x: DataArray) -> DataArray:\n-        if x.dtype.kind in \"mM\":\n+        if x.dtype.kind == \"M\":  # datetime64\n             return x.copy(\n                 data=datetime_to_numeric(\n                     x.data,\n@@ -1941,10 +1970,27 @@ def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n                     datetime_unit=\"ns\",\n                 ),\n             )\n+        elif x.dtype.kind == \"m\":  # timedelta64\n+            # For timedelta64, convert directly to numeric without offset\n+            data = x.data\n+            if data.dtype != np.dtype('timedelta64[ns]'):\n+                # Convert to nanoseconds if needed\n+                data = data / np.timedelta64(1, 'ns')\n+            return x.copy(data=data.astype(float))\n         return x\n \n     if isinstance(data, Dataset):\n         return data.map(to_floatable)\n+    elif isinstance(data, DataArray):\n+        # Process the data variable\n+        result = to_floatable(data)\n+        # Also process the coordinates\n+        new_coords = {}\n+        for name, coord in result.coords.items():\n+            new_coords[name] = to_floatable(coord)\n+        if new_coords:\n+            result = result.assign_coords(new_coords)\n+        return result\n     else:\n         return to_floatable(data)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6721", "model_patch": "diff --git a/xarray/core/common.py b/xarray/core/common.py\nindex 3c328f42..c1283b70 100644\n--- a/xarray/core/common.py\n+++ b/xarray/core/common.py\n@@ -2023,8 +2023,10 @@ def get_chunksizes(\n \n     chunks: dict[Any, tuple[int, ...]] = {}\n     for v in variables:\n-        if hasattr(v.data, \"chunks\"):\n-            for dim, c in v.chunksizes.items():\n+        # Use the chunksizes property which handles different data types correctly\n+        var_chunks = v.chunksizes\n+        if var_chunks:  # Only process if there are chunks\n+            for dim, c in var_chunks.items():\n                 if dim in chunks and c != chunks[dim]:\n                     raise ValueError(\n                         f\"Object has inconsistent chunks along dimension {dim}. \"\ndiff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex 2d115ff0..ceaf1aea 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -1014,6 +1014,21 @@ class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):\n         Variable.chunks\n         xarray.unify_chunks\n         \"\"\"\n+        # Check if we have encoding with chunks information (e.g., for zarr arrays)\n+        if \"chunks\" in self.encoding and self.encoding[\"chunks\"] is not None:\n+            chunks = self.encoding[\"chunks\"]\n+            # Handle different types of chunks in encoding:\n+            # 1. Integer: same chunk size for all dimensions\n+            # 2. Tuple: explicit chunk sizes for each dimension\n+            if isinstance(chunks, int):\n+                # Same chunk size for all dimensions\n+                return Frozen({dim: chunks for dim in self.dims})\n+            elif isinstance(chunks, (tuple, list)):\n+                # Explicit chunk sizes for each dimension\n+                return Frozen({dim: c for dim, c in zip(self.dims, chunks)})\n+            else:\n+                # Unexpected type, fall back to default behavior\n+                pass\n         if hasattr(self._data, \"chunks\"):\n             return Frozen({dim: c for dim, c in zip(self.dims, self.data.chunks)})\n         else:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6744", "model_patch": "diff --git a/xarray/core/rolling.py b/xarray/core/rolling.py\nindex aef290f6..340d1cf5 100644\n--- a/xarray/core/rolling.py\n+++ b/xarray/core/rolling.py\n@@ -270,9 +270,28 @@ class DataArrayRolling(Rolling[\"DataArray\"]):\n     def __iter__(self) -> Iterator[tuple[RollingKey, DataArray]]:\n         if self.ndim > 1:\n             raise ValueError(\"__iter__ is only supported for 1d-rolling\")\n-        stops = np.arange(1, len(self.window_labels) + 1)\n-        starts = stops - int(self.window[0])\n-        starts[: int(self.window[0])] = 0\n+        \n+        # Handle centering\n+        if self.center[0]:\n+            # For centered windows, we need to adjust the start and stop indices\n+            # to center the window around each label\n+            window_size = int(self.window[0])\n+            half_window = window_size // 2\n+            \n+            # Calculate the start and stop indices for centered windows\n+            indices = np.arange(len(self.window_labels))\n+            starts = indices - half_window\n+            stops = indices + half_window + (window_size % 2)  # Add 1 for odd-sized windows\n+            \n+            # Handle edge cases where windows extend beyond the array bounds\n+            starts = np.maximum(starts, 0)\n+            stops = np.minimum(stops, len(self.window_labels))\n+        else:\n+            # Original logic for non-centered windows\n+            stops = np.arange(1, len(self.window_labels) + 1)\n+            starts = stops - int(self.window[0])\n+            starts[: int(self.window[0])] = 0\n+            \n         for (label, start, stop) in zip(self.window_labels, starts, stops):\n             window = self.obj.isel({self.dim[0]: slice(start, stop)})\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6938", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex c775d05c..681de65e 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3772,7 +3772,7 @@ class Dataset(\n         for k, v in self.variables.items():\n             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n             if k in result_dims:\n-                var = v.to_index_variable()\n+                var = v.copy(deep=False)\n                 var.dims = dims\n                 if k in self._indexes:\n                     indexes[k] = self._indexes[k]\n@@ -3783,7 +3783,7 @@ class Dataset(\n                     variables.update(index_vars)\n                     coord_names.update(index_vars)\n             else:\n-                var = v.to_base_variable()\n+                var = v.copy(deep=False)\n                 var.dims = dims\n                 variables[k] = var\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6992", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 620f32f8..4e9ea060 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -365,7 +365,11 @@ class DataVariables(Mapping[Any, \"DataArray\"]):\n         )\n \n     def __len__(self) -> int:\n-        return len(self._dataset._variables) - len(self._dataset._coord_names)\n+        count = 0\n+        for key in self._dataset._variables:\n+            if key not in self._dataset._coord_names:\n+                count += 1\n+        return count\n \n     def __contains__(self, key: Hashable) -> bool:\n         return key in self._dataset._variables and key not in self._dataset._coord_names\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7229", "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..9d027844\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,23 @@\n+import xarray as xr\n+\n+# Load the tutorial dataset\n+ds = xr.tutorial.load_dataset(\"air_temperature\")\n+\n+# Print the original time attributes\n+print(\"Original time attributes:\")\n+print(ds.time.attrs)\n+\n+# Apply xr.where with keep_attrs=True\n+result = xr.where(True, ds.air, ds.air, keep_attrs=True)\n+\n+# Print the time attributes after applying xr.where\n+print(\"\\nTime attributes after xr.where:\")\n+print(result.time.attrs)\n+\n+# Check if the attributes are the same\n+if ds.time.attrs == result.time.attrs:\n+    print(\"\\nSUCCESS: Coordinate attributes are preserved!\")\n+else:\n+    print(\"\\nERROR: Coordinate attributes are overwritten!\")\n+    print(\"Expected:\", ds.time.attrs)\n+    print(\"Got:\", result.time.attrs)\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7233", "model_patch": "diff --git a/xarray/core/rolling.py b/xarray/core/rolling.py\nindex c194ec08..fb5b2bbb 100644\n--- a/xarray/core/rolling.py\n+++ b/xarray/core/rolling.py\n@@ -973,7 +973,8 @@ class Coarsen(CoarsenArithmetic, Generic[T_Xarray]):\n             else:\n                 reshaped[key] = var\n \n-        should_be_coords = set(window_dim) & set(self.obj.coords)\n+        # Preserve all coordinates from the original dataset\n+        should_be_coords = set(obj.coords)\n         result = reshaped.set_coords(should_be_coords)\n         if isinstance(self.obj, DataArray):\n             return self.obj._from_temp_dataset(result)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7393", "model_patch": "diff --git a/xarray/core/indexing.py b/xarray/core/indexing.py\nindex ba937183..3abb0c85 100644\n--- a/xarray/core/indexing.py\n+++ b/xarray/core/indexing.py\n@@ -1532,7 +1532,10 @@ class PandasMultiIndexingAdapter(PandasIndexingAdapter):\n \n     def __array__(self, dtype: DTypeLike = None) -> np.ndarray:\n         if self.level is not None:\n-            return self.array.get_level_values(self.level).values\n+            level_values = self.array.get_level_values(self.level).values\n+            if dtype is None:\n+                dtype = self.dtype\n+            return np.asarray(level_values, dtype=dtype)\n         else:\n             return super().__array__(dtype)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-4551", "model_patch": "diff --git a/pylint/pyreverse/diagrams.py b/pylint/pyreverse/diagrams.py\nindex bf9f67af..72aff5da 100644\n--- a/pylint/pyreverse/diagrams.py\n+++ b/pylint/pyreverse/diagrams.py\n@@ -84,6 +84,29 @@ class ClassDiagram(Figure, FilterMixIn):\n             for n, m in node.items()\n             if isinstance(m, astroid.FunctionDef) and decorated_with_property(m)\n         ]\n+        \n+        # Get type annotations for __init__ method parameters\n+        param_annotations = {}\n+        init_method = node.locals.get('__init__', [None])[0]\n+        if init_method and hasattr(init_method, 'args') and init_method.args:\n+            # Use the annotations attribute of the Arguments node\n+            annotations = getattr(init_method.args, 'annotations', [])\n+            for i, arg in enumerate(init_method.args.args):\n+                if i < len(annotations) and annotations[i]:\n+                    try:\n+                        inferred_types = list(annotations[i].infer())\n+                        type_names = []\n+                        for inferred_type in inferred_types:\n+                            if hasattr(inferred_type, 'name'):\n+                                type_names.append(inferred_type.name)\n+                            elif hasattr(inferred_type, 'qname'):\n+                                type_names.append(inferred_type.qname().split('.')[-1])\n+                            else:\n+                                type_names.append(str(inferred_type))\n+                        param_annotations[arg.name] = \", \".join(type_names)\n+                    except astroid.InferenceError:\n+                        pass\n+        \n         for node_name, associated_nodes in (\n             list(node.instance_attrs_type.items())\n             + list(node.locals_type.items())\n@@ -91,9 +114,14 @@ class ClassDiagram(Figure, FilterMixIn):\n         ):\n             if not self.show_attr(node_name):\n                 continue\n-            names = self.class_names(associated_nodes)\n-            if names:\n-                node_name = \"{} : {}\".format(node_name, \", \".join(names))\n+            \n+            # Check if this is an instance attribute that corresponds to an __init__ parameter with type annotation\n+            if node_name in param_annotations:\n+                node_name = \"{} : {}\".format(node_name, param_annotations[node_name])\n+            else:\n+                names = self.class_names(associated_nodes)\n+                if names:\n+                    node_name = \"{} : {}\".format(node_name, \", \".join(names))\n             attrs.append(node_name)\n         return sorted(attrs)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6938", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex c775d05c..681de65e 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3772,7 +3772,7 @@ class Dataset(\n         for k, v in self.variables.items():\n             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n             if k in result_dims:\n-                var = v.to_index_variable()\n+                var = v.copy(deep=False)\n                 var.dims = dims\n                 if k in self._indexes:\n                     indexes[k] = self._indexes[k]\n@@ -3783,7 +3783,7 @@ class Dataset(\n                     variables.update(index_vars)\n                     coord_names.update(index_vars)\n             else:\n-                var = v.to_base_variable()\n+                var = v.copy(deep=False)\n                 var.dims = dims\n                 variables[k] = var\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-6992", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 620f32f8..4e9ea060 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -365,7 +365,11 @@ class DataVariables(Mapping[Any, \"DataArray\"]):\n         )\n \n     def __len__(self) -> int:\n-        return len(self._dataset._variables) - len(self._dataset._coord_names)\n+        count = 0\n+        for key in self._dataset._variables:\n+            if key not in self._dataset._coord_names:\n+                count += 1\n+        return count\n \n     def __contains__(self, key: Hashable) -> bool:\n         return key in self._dataset._variables and key not in self._dataset._coord_names\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7229", "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..9d027844\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,23 @@\n+import xarray as xr\n+\n+# Load the tutorial dataset\n+ds = xr.tutorial.load_dataset(\"air_temperature\")\n+\n+# Print the original time attributes\n+print(\"Original time attributes:\")\n+print(ds.time.attrs)\n+\n+# Apply xr.where with keep_attrs=True\n+result = xr.where(True, ds.air, ds.air, keep_attrs=True)\n+\n+# Print the time attributes after applying xr.where\n+print(\"\\nTime attributes after xr.where:\")\n+print(result.time.attrs)\n+\n+# Check if the attributes are the same\n+if ds.time.attrs == result.time.attrs:\n+    print(\"\\nSUCCESS: Coordinate attributes are preserved!\")\n+else:\n+    print(\"\\nERROR: Coordinate attributes are overwritten!\")\n+    print(\"Expected:\", ds.time.attrs)\n+    print(\"Got:\", result.time.attrs)\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7233", "model_patch": "diff --git a/xarray/core/rolling.py b/xarray/core/rolling.py\nindex c194ec08..fb5b2bbb 100644\n--- a/xarray/core/rolling.py\n+++ b/xarray/core/rolling.py\n@@ -973,7 +973,8 @@ class Coarsen(CoarsenArithmetic, Generic[T_Xarray]):\n             else:\n                 reshaped[key] = var\n \n-        should_be_coords = set(window_dim) & set(self.obj.coords)\n+        # Preserve all coordinates from the original dataset\n+        should_be_coords = set(obj.coords)\n         result = reshaped.set_coords(should_be_coords)\n         if isinstance(self.obj, DataArray):\n             return self.obj._from_temp_dataset(result)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pydata__xarray-7393", "model_patch": "diff --git a/xarray/core/indexing.py b/xarray/core/indexing.py\nindex ba937183..3abb0c85 100644\n--- a/xarray/core/indexing.py\n+++ b/xarray/core/indexing.py\n@@ -1532,7 +1532,10 @@ class PandasMultiIndexingAdapter(PandasIndexingAdapter):\n \n     def __array__(self, dtype: DTypeLike = None) -> np.ndarray:\n         if self.level is not None:\n-            return self.array.get_level_values(self.level).values\n+            level_values = self.array.get_level_values(self.level).values\n+            if dtype is None:\n+                dtype = self.dtype\n+            return np.asarray(level_values, dtype=dtype)\n         else:\n             return super().__array__(dtype)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-4551", "model_patch": "diff --git a/pylint/pyreverse/diagrams.py b/pylint/pyreverse/diagrams.py\nindex bf9f67af..72aff5da 100644\n--- a/pylint/pyreverse/diagrams.py\n+++ b/pylint/pyreverse/diagrams.py\n@@ -84,6 +84,29 @@ class ClassDiagram(Figure, FilterMixIn):\n             for n, m in node.items()\n             if isinstance(m, astroid.FunctionDef) and decorated_with_property(m)\n         ]\n+        \n+        # Get type annotations for __init__ method parameters\n+        param_annotations = {}\n+        init_method = node.locals.get('__init__', [None])[0]\n+        if init_method and hasattr(init_method, 'args') and init_method.args:\n+            # Use the annotations attribute of the Arguments node\n+            annotations = getattr(init_method.args, 'annotations', [])\n+            for i, arg in enumerate(init_method.args.args):\n+                if i < len(annotations) and annotations[i]:\n+                    try:\n+                        inferred_types = list(annotations[i].infer())\n+                        type_names = []\n+                        for inferred_type in inferred_types:\n+                            if hasattr(inferred_type, 'name'):\n+                                type_names.append(inferred_type.name)\n+                            elif hasattr(inferred_type, 'qname'):\n+                                type_names.append(inferred_type.qname().split('.')[-1])\n+                            else:\n+                                type_names.append(str(inferred_type))\n+                        param_annotations[arg.name] = \", \".join(type_names)\n+                    except astroid.InferenceError:\n+                        pass\n+        \n         for node_name, associated_nodes in (\n             list(node.instance_attrs_type.items())\n             + list(node.locals_type.items())\n@@ -91,9 +114,14 @@ class ClassDiagram(Figure, FilterMixIn):\n         ):\n             if not self.show_attr(node_name):\n                 continue\n-            names = self.class_names(associated_nodes)\n-            if names:\n-                node_name = \"{} : {}\".format(node_name, \", \".join(names))\n+            \n+            # Check if this is an instance attribute that corresponds to an __init__ parameter with type annotation\n+            if node_name in param_annotations:\n+                node_name = \"{} : {}\".format(node_name, param_annotations[node_name])\n+            else:\n+                names = self.class_names(associated_nodes)\n+                if names:\n+                    node_name = \"{} : {}\".format(node_name, \", \".join(names))\n             attrs.append(node_name)\n         return sorted(attrs)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-4604", "model_patch": "diff --git a/pylint/checkers/variables.py b/pylint/checkers/variables.py\nindex 3ddafd59..dfba536c 100644\n--- a/pylint/checkers/variables.py\n+++ b/pylint/checkers/variables.py\n@@ -1826,6 +1826,19 @@ class VariablesChecker(BaseChecker):\n             self._type_annotation_names.append(type_annotation.name)\n             return\n \n+        if isinstance(type_annotation, astroid.Attribute):\n+            # For attribute nodes like abc.ABC, we need to store all parts of the chain\n+            # For a.b.C, we need to store 'a', 'b', and 'C'\n+            current = type_annotation\n+            while isinstance(current, astroid.Attribute):\n+                # Store the attribute name (e.g., 'C' in a.b.C)\n+                self._type_annotation_names.append(current.attrname)\n+                current = current.expr\n+            # Store the final name (e.g., 'a' in a.b.C)\n+            if isinstance(current, astroid.Name):\n+                self._type_annotation_names.append(current.name)\n+            return\n+\n         if not isinstance(type_annotation, astroid.Subscript):\n             return\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-4661", "model_patch": "diff --git a/pylint/config/__init__.py b/pylint/config/__init__.py\nindex 84774d0e..31553f18 100644\n--- a/pylint/config/__init__.py\n+++ b/pylint/config/__init__.py\n@@ -63,7 +63,11 @@ if \"PYLINTHOME\" in os.environ:\n elif USER_HOME == \"~\":\n     PYLINT_HOME = \".pylint.d\"\n else:\n-    PYLINT_HOME = os.path.join(USER_HOME, \".pylint.d\")\n+    # XDG Base Directory Specification compliant path\n+    xdg_data_home = os.environ.get(\"XDG_DATA_HOME\")\n+    if not xdg_data_home:\n+        xdg_data_home = os.path.join(USER_HOME, \".local\", \"share\")\n+    PYLINT_HOME = os.path.join(xdg_data_home, \"pylint\")\n \n \n def _get_pdata_path(base_name, recurs):\n@@ -110,7 +114,7 @@ ENV_HELP = (\n The following environment variables are used:\n     * PYLINTHOME\n     Path to the directory where persistent data for the run will be stored. If\n-not found, it defaults to ~/.pylint.d/ or .pylint.d (in the current working\n+not found, it defaults to ~/.local/share/pylint/ or .pylint.d (in the current working\n directory).\n     * PYLINTRC\n     Path to the configuration file. See the documentation for the method used\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-4970", "model_patch": "diff --git a/debug_test.py b/debug_test.py\nnew file mode 100644\nindex 00000000..7aca3328\n--- /dev/null\n+++ b/debug_test.py\n@@ -0,0 +1,95 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Script to debug the issue with min-similarity-lines=1\n+\"\"\"\n+\n+import tempfile\n+import os\n+from pylint.lint import PyLinter\n+from pylint.checkers import similar\n+from pylint.testutils import GenericTestReporter as Reporter\n+\n+def debug_min_similarity_lines():\n+    \"\"\"Debug what happens with min-similarity-lines=1\"\"\"\n+    # Create two identical files with more content\n+    content = \"\"\"\n+def func1():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\n+def func2():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\"\"\"\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f1:\n+        f1.write(content)\n+        file1 = f1.name\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f2:\n+        f2.write(content)\n+        file2 = f2.name\n+    \n+    try:\n+        # Test with min-similarity-lines=1\n+        print(\"Testing with min-similarity-lines=1:\")\n+        linter = PyLinter(reporter=Reporter())\n+        checker = similar.SimilarChecker(linter)\n+        linter.register_checker(checker)\n+        print(f\"Before set_option: checker.min_lines = {checker.min_lines}\")\n+        print(f\"Before set_option: checker.config.min_similarity_lines = {checker.config.min_similarity_lines}\")\n+        checker.set_option(\"min-similarity-lines\", 1)\n+        print(f\"After set_option: checker.min_lines = {checker.min_lines}\")\n+        print(f\"After set_option: checker.config.min_similarity_lines = {checker.config.min_similarity_lines}\")\n+        \n+        # Check the files\n+        linter.check([file1, file2])\n+        \n+        # Check if any duplicate code messages were reported\n+        print(f\"DEBUG: Reporter has {len(linter.reporter.messages)} messages\")\n+        for i, msg in enumerate(linter.reporter.messages):\n+            print(f\"DEBUG: Message {i}: {msg}\")\n+        messages = linter.reporter.messages\n+        duplicate_messages = [msg for msg in messages if \"Similar lines\" in str(msg)]\n+        \n+        print(f\"Number of duplicate-code messages: {len(duplicate_messages)}\")\n+        for msg in duplicate_messages:\n+            print(f\"Message: {msg}\")\n+            \n+        # Let's also check the internal state\n+        print(f\"Checker min_lines: {checker.min_lines}\")\n+        print(f\"Number of linesets: {len(checker.linesets)}\")\n+        if len(checker.linesets) >= 2:\n+            print(f\"Lineset 1 length: {len(checker.linesets[0])}\")\n+            print(f\"Lineset 2 length: {len(checker.linesets[1])}\")\n+            \n+        # Let's see what _compute_sims returns\n+        sims = checker._compute_sims()\n+        print(f\"Number of similarities found: {len(sims)}\")\n+        for num, couples in sims:\n+            print(f\"Similarity with {num} lines:\")\n+            for lineset, start, end in couples:\n+                print(f\"  File: {lineset.name}, lines {start}-{end}\")\n+            \n+        return True\n+            \n+    finally:\n+        # Clean up temporary files\n+        os.unlink(file1)\n+        os.unlink(file2)\n+\n+if __name__ == \"__main__\":\n+    debug_min_similarity_lines()\n\\ No newline at end of file\ndiff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex c65b1c89..456fd841 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -394,6 +394,7 @@ class Similar:\n \n     def _compute_sims(self) -> List[Tuple[int, Set[LinesChunkLimits_T]]]:\n         \"\"\"compute similarities in appended files\"\"\"\n+        print(f\"DEBUG: _compute_sims called with min_lines={self.min_lines}\")\n         no_duplicates: Dict[int, List[Set[LinesChunkLimits_T]]] = defaultdict(list)\n \n         for commonality in self._iter_sims():\n@@ -467,7 +468,7 @@ class Similar:\n         This the core of the algorithm.\n         The idea is to compute the hashes of a minimal number of successive lines of each lineset and then compare the hashes.\n         Every match of such comparison is stored in a dict that links the couple of starting indices in both linesets to\n-        the couple of corresponding starting and ending lines in both files.\n+        the couple of corresponding starting and ending number lines in both files.\n         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more\n         than the minimal number of successive lines required.\n         \"\"\"\n@@ -524,7 +525,7 @@ class Similar:\n                 lineset1, start_index_1, lineset2, start_index_2, nb_common_lines\n             )\n \n-            if eff_cmn_nb > self.min_lines:\n+            if self.min_lines > 0 and eff_cmn_nb >= self.min_lines:\n                 yield com\n \n     def _iter_sims(self) -> Generator[Commonality, None, None]:\n@@ -857,8 +858,11 @@ class SimilarChecker(BaseChecker, Similar, MapReduceMixin):\n         \"\"\"Reduces and recombines data into a format that we can report on\n \n         The partner function of get_map_data()\"\"\"\n+        print(f\"DEBUG: reduce_map_data called with self.min_lines={self.min_lines}\")\n         recombined = SimilarChecker(linter)\n+        print(f\"DEBUG: recombined.min_lines after init={recombined.min_lines}\")\n         recombined.min_lines = self.min_lines\n+        print(f\"DEBUG: recombined.min_lines after assignment={recombined.min_lines}\")\n         recombined.ignore_comments = self.ignore_comments\n         recombined.ignore_docstrings = self.ignore_docstrings\n         recombined.ignore_imports = self.ignore_imports\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..f4d61bf5\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,72 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Script to reproduce the issue with min-similarity-lines=0\n+\"\"\"\n+\n+import tempfile\n+import os\n+import sys\n+from pylint.checkers import similar\n+from io import StringIO\n+from contextlib import redirect_stdout\n+\n+def test_min_similarity_lines_zero():\n+    \"\"\"Test that setting min-similarity-lines to 0 disables duplicate code checking\"\"\"\n+    # Create two identical files\n+    content = \"\"\"\n+def func1():\n+    x = 1\n+    y = 2\n+    return x + y\n+\n+def func2():\n+    x = 1\n+    y = 2\n+    return x + y\n+\"\"\"\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f1:\n+        f1.write(content)\n+        file1 = f1.name\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f2:\n+        f2.write(content)\n+        file2 = f2.name\n+    \n+    try:\n+        # Test with duplicates=0 using command line\n+        print(\"Testing with duplicates=0:\")\n+        output = StringIO()\n+        with redirect_stdout(output):\n+            try:\n+                similar.Run([\"--duplicates=0\", file1, file2])\n+            except SystemExit as e:\n+                if e.code != 0:\n+                    print(f\"ERROR: Command failed with exit code {e.code}\")\n+                    return False\n+        print(\"Output with duplicates=0:\")\n+        print(output.getvalue())\n+        \n+        # Test with duplicates=4 (default) to see if it detects duplicates\n+        print(\"\\nTesting with duplicates=4 (default):\")\n+        output = StringIO()\n+        with redirect_stdout(output):\n+            try:\n+                similar.Run([\"--duplicates=4\", file1, file2])\n+            except SystemExit as e:\n+                if e.code != 0:\n+                    print(f\"ERROR: Command failed with exit code {e.code}\")\n+                    return False\n+        print(\"Output with duplicates=4:\")\n+        print(output.getvalue())\n+        \n+        return True\n+            \n+    finally:\n+        # Clean up temporary files\n+        os.unlink(file1)\n+        os.unlink(file2)\n+\n+if __name__ == \"__main__\":\n+    test_min_similarity_lines_zero()\n\\ No newline at end of file\ndiff --git a/test_rcfile.ini b/test_rcfile.ini\nnew file mode 100644\nindex 00000000..19393d7b\n--- /dev/null\n+++ b/test_rcfile.ini\n@@ -0,0 +1,2 @@\n+[tool:pylint]\n+min-similarity-lines=0\n\\ No newline at end of file\ndiff --git a/test_rcfile.py b/test_rcfile.py\nnew file mode 100644\nindex 00000000..c7576c12\n--- /dev/null\n+++ b/test_rcfile.py\n@@ -0,0 +1,70 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Script to reproduce the issue with min-similarity-lines=0 in rcfile\n+\"\"\"\n+\n+import tempfile\n+import os\n+from pylint.lint import PyLinter\n+from pylint.checkers import similar\n+from pylint.testutils import GenericTestReporter as Reporter\n+\n+def test_min_similarity_lines_zero_rcfile():\n+    \"\"\"Test that setting min-similarity-lines to 0 in rcfile disables duplicate code checking\"\"\"\n+    # Create two identical files\n+    content = \"\"\"\n+def func1():\n+    x = 1\n+    y = 2\n+    return x + y\n+\n+def func2():\n+    x = 1\n+    y = 2\n+    return x + y\n+\"\"\"\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f1:\n+        f1.write(content)\n+        file1 = f1.name\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f2:\n+        f2.write(content)\n+        file2 = f2.name\n+    \n+    try:\n+        # Create a linter with rcfile that sets min-similarity-lines=0\n+        linter = PyLinter(reporter=Reporter())\n+        linter.read_config_file(config_file=\"/testbed/test_rcfile.ini\")\n+        linter.load_config_file()\n+        \n+        # Register the similar checker\n+        checker = similar.SimilarChecker(linter)\n+        linter.register_checker(checker)\n+        \n+        # Check the files\n+        linter.check([file1, file2])\n+        \n+        # Check if any duplicate code messages were reported\n+        messages = linter.reporter.messages\n+        duplicate_messages = [msg for msg in messages if \"duplicate-code\" in str(msg)]\n+        \n+        print(f\"Number of duplicate-code messages: {len(duplicate_messages)}\")\n+        for msg in duplicate_messages:\n+            print(f\"Message: {msg}\")\n+            \n+        if len(duplicate_messages) > 0:\n+            print(\"ERROR: Setting min-similarity-lines=0 in rcfile should disable duplicate code checking\")\n+            return False\n+        else:\n+            print(\"SUCCESS: Setting min-similarity-lines=0 in rcfile disabled duplicate code checking\")\n+            return True\n+            \n+    finally:\n+        # Clean up temporary files\n+        os.unlink(file1)\n+        os.unlink(file2)\n+\n+if __name__ == \"__main__\":\n+    test_min_similarity_lines_zero_rcfile()\n\\ No newline at end of file\ndiff --git a/test_rcfile_detailed.py b/test_rcfile_detailed.py\nnew file mode 100644\nindex 00000000..2eb3f209\n--- /dev/null\n+++ b/test_rcfile_detailed.py\n@@ -0,0 +1,113 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Script to reproduce the issue with min-similarity-lines=0 in rcfile\n+\"\"\"\n+\n+import tempfile\n+import os\n+from pylint.lint import PyLinter\n+from pylint.checkers import similar\n+from pylint.testutils import GenericTestReporter as Reporter\n+\n+def test_min_similarity_lines_zero_rcfile_detailed():\n+    \"\"\"Test that setting min-similarity-lines to 0 in rcfile disables duplicate code checking\"\"\"\n+    # Create two identical files with more content\n+    content = \"\"\"\n+def func1():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\n+def func2():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\n+def func3():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\"\"\"\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f1:\n+        f1.write(content)\n+        file1 = f1.name\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f2:\n+        f2.write(content)\n+        file2 = f2.name\n+    \n+    try:\n+        # Test with min-similarity-lines=0\n+        print(\"Testing with min-similarity-lines=0:\")\n+        linter = PyLinter(reporter=Reporter())\n+        checker = similar.SimilarChecker(linter)\n+        checker.set_option(\"min-similarity-lines\", 0)\n+        linter.register_checker(checker)\n+        \n+        # Check the files\n+        linter.check([file1, file2])\n+        \n+        # Check if any duplicate code messages were reported\n+        messages = linter.reporter.messages\n+        duplicate_messages = [msg for msg in messages if \"duplicate-code\" in str(msg)]\n+        \n+        print(f\"Number of duplicate-code messages: {len(duplicate_messages)}\")\n+        for msg in duplicate_messages:\n+            print(f\"Message: {msg}\")\n+            \n+        if len(duplicate_messages) > 0:\n+            print(\"ERROR: Setting min-similarity-lines=0 should disable duplicate code checking\")\n+            return False\n+        else:\n+            print(\"SUCCESS: Setting min-similarity-lines=0 disabled duplicate code checking\")\n+            \n+        # Test with min-similarity-lines=1\n+        print(\"\\nTesting with min-similarity-lines=1:\")\n+        linter = PyLinter(reporter=Reporter())\n+        checker = similar.SimilarChecker(linter)\n+        checker.set_option(\"min-similarity-lines\", 1)\n+        linter.register_checker(checker)\n+        \n+        # Check the files\n+        linter.check([file1, file2])\n+        \n+        # Check if any duplicate code messages were reported\n+        messages = linter.reporter.messages\n+        duplicate_messages = [msg for msg in messages if \"duplicate-code\" in str(msg)]\n+        \n+        print(f\"Number of duplicate-code messages: {len(duplicate_messages)}\")\n+        for msg in duplicate_messages:\n+            print(f\"Message: {msg}\")\n+            \n+        if len(duplicate_messages) > 0:\n+            print(\"With min-similarity-lines=1, duplicate code is detected (expected)\")\n+        else:\n+            print(\"ERROR: With min-similarity-lines=1, duplicate code should be detected\")\n+            return False\n+            \n+        return True\n+            \n+    finally:\n+        # Clean up temporary files\n+        os.unlink(file1)\n+        os.unlink(file2)\n+\n+if __name__ == \"__main__\":\n+    test_min_similarity_lines_zero_rcfile_detailed()\n\\ No newline at end of file\ndiff --git a/test_zero.py b/test_zero.py\nnew file mode 100644\nindex 00000000..fbd51994\n--- /dev/null\n+++ b/test_zero.py\n@@ -0,0 +1,81 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Script to test that min-similarity-lines=0 disables duplicate code checking\n+\"\"\"\n+\n+import tempfile\n+import os\n+from pylint.lint import PyLinter\n+from pylint.checkers import similar\n+from pylint.testutils import GenericTestReporter as Reporter\n+\n+def test_min_similarity_lines_zero():\n+    \"\"\"Test that setting min-similarity-lines to 0 disables duplicate code checking\"\"\"\n+    # Create two identical files with more content\n+    content = \"\"\"\n+def func1():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\n+def func2():\n+    # This is a comment\n+    x = 1\n+    y = 2\n+    z = 3\n+    a = 4\n+    b = 5\n+    c = 6\n+    return x + y + z + a + b + c\n+\"\"\"\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f1:\n+        f1.write(content)\n+        file1 = f1.name\n+    \n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f2:\n+        f2.write(content)\n+        file2 = f2.name\n+    \n+    try:\n+        # Test with min-similarity-lines=0\n+        print(\"Testing with min-similarity-lines=0:\")\n+        linter = PyLinter(reporter=Reporter())\n+        checker = similar.SimilarChecker(linter)\n+        linter.register_checker(checker)\n+        checker.set_option(\"min-similarity-lines\", 0)\n+        \n+        # Check the files\n+        linter.check([file1, file2])\n+        \n+        # Check if any duplicate code messages were reported\n+        print(f\"DEBUG: Reporter has {len(linter.reporter.messages)} messages\")\n+        for i, msg in enumerate(linter.reporter.messages):\n+            print(f\"DEBUG: Message {i}: {msg}\")\n+        messages = linter.reporter.messages\n+        duplicate_messages = [msg for msg in messages if \"Similar lines\" in str(msg)]\n+        \n+        print(f\"Number of duplicate-code messages: {len(duplicate_messages)}\")\n+        for msg in duplicate_messages:\n+            print(f\"Message: {msg}\")\n+            \n+        if len(duplicate_messages) > 0:\n+            print(\"ERROR: Setting min-similarity-lines=0 should disable duplicate code checking\")\n+            return False\n+        else:\n+            print(\"SUCCESS: Setting min-similarity-lines=0 disabled duplicate code checking\")\n+            return True\n+            \n+    finally:\n+        # Clean up temporary files\n+        os.unlink(file1)\n+        os.unlink(file2)\n+\n+if __name__ == \"__main__\":\n+    test_min_similarity_lines_zero()\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-6386", "model_patch": "diff --git a/pylint/lint/base_options.py b/pylint/lint/base_options.py\nindex c793cc85..56b5cbcd 100644\n--- a/pylint/lint/base_options.py\n+++ b/pylint/lint/base_options.py\n@@ -535,17 +535,7 @@ def _make_run_options(self: Run) -> Options:\n                 \"hide_from_config_file\": True,\n             },\n         ),\n-        (\n-            \"verbose\",\n-            {\n-                \"action\": _DoNothingAction,\n-                \"kwargs\": {},\n-                \"short\": \"v\",\n-                \"help\": \"In verbose mode, extra non-checker-related info \"\n-                \"will be displayed.\",\n-                \"hide_from_config_file\": True,\n-            },\n-        ),\n+        [['verbose', {'action': 'store_true', 'default': False, 'short': 'v', 'help': 'In verbose mode, extra non-checker-related info will be displayed.', 'hide_from_config_file': True}]]\n         (\n             \"enable-all-extensions\",\n             {\ndiff --git a/simple_test.py b/simple_test.py\nnew file mode 100644\nindex 00000000..3f394b4d\n--- /dev/null\n+++ b/simple_test.py\n@@ -0,0 +1,24 @@\n+import sys\n+import argparse\n+from pylint.config.callback_actions import _DoNothingAction\n+\n+# Create a simple argument parser to test the issue\n+parser = argparse.ArgumentParser()\n+\n+# Add the verbose option with _DoNothingAction (current implementation - without nargs)\n+parser.add_argument('-v', '--verbose', action=_DoNothingAction, help='Verbose mode')\n+\n+# Test parsing\n+try:\n+    args = parser.parse_args(['-v'])\n+    print(\"Success: Short form (-v) worked\")\n+    print(f\"Args: {args}\")\n+except SystemExit as e:\n+    print(f\"Error: Short form (-v) failed with exit code {e.code}\")\n+\n+try:\n+    args = parser.parse_args(['--verbose'])\n+    print(\"Success: Long form (--verbose) worked\")\n+    print(f\"Args: {args}\")\n+except SystemExit as e:\n+    print(f\"Error: Long form (--verbose) failed with exit code {e.code}\")\n\\ No newline at end of file\ndiff --git a/test_verbose.py b/test_verbose.py\nnew file mode 100644\nindex 00000000..a61789b5\n--- /dev/null\n+++ b/test_verbose.py\n@@ -0,0 +1,35 @@\n+import subprocess\n+import sys\n+import tempfile\n+import os\n+\n+# Create a temporary Python file for testing\n+with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n+    test_file = f.name\n+    f.write(\"print('Hello, world!')\\n\")\n+\n+try:\n+    # Test the long form (should work)\n+    print(\"Testing long form (--verbose):\")\n+    result = subprocess.run([sys.executable, \"-m\", \"pylint\", \"--verbose\", test_file], \n+                          capture_output=True, text=True)\n+    print(f\"Return code: {result.returncode}\")\n+    if result.stdout:\n+        print(f\"Stdout: {result.stdout}\")\n+    if result.stderr:\n+        print(f\"Stderr: {result.stderr}\")\n+    \n+    print(\"\\n\" + \"=\"*50 + \"\\n\")\n+    \n+    # Test the short form (should fail with current code)\n+    print(\"Testing short form (-v):\")\n+    result = subprocess.run([sys.executable, \"-m\", \"pylint\", \"-v\", test_file], \n+                          capture_output=True, text=True)\n+    print(f\"Return code: {result.returncode}\")\n+    if result.stdout:\n+        print(f\"Stdout: {result.stdout}\")\n+    if result.stderr:\n+        print(f\"Stderr: {result.stderr}\")\n+finally:\n+    # Clean up the temporary file\n+    os.unlink(test_file)\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-6528", "model_patch": "diff --git a/pylint/lint/pylinter.py b/pylint/lint/pylinter.py\nindex 53345a6b..b8bcaaf1 100644\n--- a/pylint/lint/pylinter.py\n+++ b/pylint/lint/pylinter.py\n@@ -564,8 +564,9 @@ class PyLinter(\n             if not msg.may_be_emitted():\n                 self._msgs_state[msg.msgid] = False\n \n-    @staticmethod\n-    def _discover_files(files_or_modules: Sequence[str]) -> Iterator[str]:\n+    def _discover_files(\n+        self, files_or_modules: Sequence[str]\n+    ) -> Iterator[str]:\n         \"\"\"Discover python modules and packages in sub-directory.\n \n         Returns iterator of paths to discovered modules and packages.\n@@ -576,6 +577,15 @@ class PyLinter(\n             ):\n                 skip_subtrees: list[str] = []\n                 for root, _, files in os.walk(something):\n+                    # Check if the current directory should be ignored\n+                    basename = os.path.basename(root)\n+                    if (\n+                        basename in self.config.ignore\n+                        or any(pattern.match(basename) for pattern in self.config.ignore_patterns)\n+                        or any(pattern.match(root) for pattern in self.config.ignore_paths)\n+                    ):\n+                        continue\n+                    \n                     if any(root.startswith(s) for s in skip_subtrees):\n                         # Skip subtree of already discovered package.\n                         continue\n@@ -587,6 +597,11 @@ class PyLinter(\n                             os.path.join(root, file)\n                             for file in files\n                             if file.endswith(\".py\")\n+                            and not (\n+                                file in self.config.ignore\n+                                or any(pattern.match(file) for pattern in self.config.ignore_patterns)\n+                                or any(pattern.match(os.path.join(root, file)) for pattern in self.config.ignore_paths)\n+                            )\n                         )\n             else:\n                 yield something\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-6903", "model_patch": "diff --git a/pylint/lint/run.py b/pylint/lint/run.py\nindex a7e5d557..297f84e5 100644\n--- a/pylint/lint/run.py\n+++ b/pylint/lint/run.py\n@@ -58,6 +58,10 @@ def _query_cpu() -> int | None:\n             cpu_shares = int(file.read().rstrip())\n         # For AWS, gives correct value * 1024.\n         avail_cpu = int(cpu_shares / 1024)\n+    \n+    # Ensure we return at least 1 CPU if we have a value\n+    if avail_cpu is not None:\n+        return max(1, avail_cpu)\n     return avail_cpu\n \n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-7080", "model_patch": "diff --git a/pylint/config/config_file_parser.py b/pylint/config/config_file_parser.py\nindex 3d769495..f69407f0 100644\n--- a/pylint/config/config_file_parser.py\n+++ b/pylint/config/config_file_parser.py\n@@ -85,16 +85,19 @@ class _ConfigurationFileParser:\n \n         config_content: dict[str, str] = {}\n         options: list[str] = []\n-        for opt, values in sections_values.items():\n-            if isinstance(values, dict):\n-                for config, value in values.items():\n+        for section_name, section_values in sections_values.items():\n+            if isinstance(section_values, dict):\n+                # This is a section (like [tool.pylint.MASTER] or [tool.pylint.main])\n+                # Extract all options from within this section\n+                for config, value in section_values.items():\n                     value = _parse_rich_type_value(value)\n                     config_content[config] = value\n                     options += [f\"--{config}\", value]\n             else:\n-                values = _parse_rich_type_value(values)\n-                config_content[opt] = values\n-                options += [f\"--{opt}\", values]\n+                # This is a direct option (not in a section)\n+                value = _parse_rich_type_value(section_values)\n+                config_content[section_name] = value\n+                options += [f\"--{section_name}\", value]\n         return config_content, options\n \n     def parse_config_file(\ndiff --git a/pylint/lint/pylinter.py b/pylint/lint/pylinter.py\nindex e2bbf061..29694b4c 100644\n--- a/pylint/lint/pylinter.py\n+++ b/pylint/lint/pylinter.py\n@@ -614,6 +614,12 @@ class PyLinter(\n                             os.path.join(root, file)\n                             for file in files\n                             if file.endswith(\".py\")\n+                            and not _is_ignored_file(\n+                                os.path.join(root, file),\n+                                self.config.ignore,\n+                                self.config.ignore_patterns,\n+                                self.config.ignore_paths,\n+                            )\n                         )\n             else:\n                 yield something\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-7277", "model_patch": "diff --git a/pylint/__init__.py b/pylint/__init__.py\nindex 9f3dd74e..af1b7393 100644\n--- a/pylint/__init__.py\n+++ b/pylint/__init__.py\n@@ -86,7 +86,7 @@ def modify_sys_path() -> None:\n     stdlib or pylint's own modules.\n     CPython issue: https://bugs.python.org/issue33053\n \n-    - Remove the first entry. This will always be either \"\" or the working directory\n+    - Remove the first entry only if it is \"\", \".\" or the current working directory\n     - Remove the working directory from the second and third entries\n       if PYTHONPATH includes a \":\" at the beginning or the end.\n       https://github.com/PyCQA/pylint/issues/3636\n@@ -96,9 +96,12 @@ def modify_sys_path() -> None:\n       if pylint is installed in an editable configuration (as the last item).\n       https://github.com/PyCQA/pylint/issues/4161\n     \"\"\"\n-    sys.path.pop(0)\n-    env_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n+    # Only remove the first entry if it is \"\", \".\" or the current working directory\n     cwd = os.getcwd()\n+    if sys.path and sys.path[0] in (\"\", \".\", cwd):\n+        sys.path.pop(0)\n+    \n+    env_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n     if env_pythonpath.startswith(\":\") and env_pythonpath not in (f\":{cwd}\", \":.\"):\n         sys.path.pop(0)\n     elif env_pythonpath.endswith(\":\") and env_pythonpath not in (f\"{cwd}:\", \".:\"):\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pylint-dev__pylint-8898", "model_patch": "diff --git a/pylint/config/argument.py b/pylint/config/argument.py\nindex 30a5fd175..0495d30c5 100644\n--- a/pylint/config/argument.py\n+++ b/pylint/config/argument.py\n@@ -113,8 +113,33 @@ def _regex_transformer(value: str) -> Pattern[str]:\n \n def _regexp_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n     \"\"\"Transforms a comma separated list of regular expressions.\"\"\"\n+    # Handle escaped commas in regexes\n+    # Split on commas that are not escaped with a backslash\n+    parts = []\n+    current_part = \"\"\n+    i = 0\n+    while i < len(value):\n+        if value[i] == '\\\\' and i + 1 < len(value) and value[i + 1] == ',':\n+            # Escaped comma, add the comma without the backslash\n+            current_part += ','\n+            i += 2\n+        elif value[i] == ',':\n+            # Unescaped comma, end of current part\n+            parts.append(current_part.strip())\n+            current_part = \"\"\n+            i += 1\n+        else:\n+            current_part += value[i]\n+            i += 1\n+    # Add the last part\n+    if current_part:\n+        parts.append(current_part.strip())\n+    \n+    # Filter out empty parts\n+    parts = [part for part in parts if part]\n+    \n     patterns: list[Pattern[str]] = []\n-    for pattern in _csv_transformer(value):\n+    for pattern in parts:\n         patterns.append(_regex_transformer(pattern))\n     return patterns\n \ndiff --git a/tests/config/test_config.py b/tests/config/test_config.py\nindex 83c8b072e..a67bbb28e 100644\n--- a/tests/config/test_config.py\n+++ b/tests/config/test_config.py\n@@ -137,7 +137,7 @@ def test_csv_regex_error(capsys: CaptureFixture) -> None:\n     \"\"\"\n     with pytest.raises(SystemExit):\n         Run(\n-            [str(EMPTY_MODULE), r\"--bad-names-rgx=(foo{1,3})\"],\n+            [str(EMPTY_MODULE), r\"--bad-names-rgxs=(foo{1,3})\"],\n             exit=False,\n         )\n     output = capsys.readouterr()\n@@ -147,6 +147,18 @@ def test_csv_regex_error(capsys: CaptureFixture) -> None:\n     )\n \n \n+def test_csv_regex_with_escaped_comma(capsys: CaptureFixture) -> None:\n+    \"\"\"Check that we correctly handle regexes with escaped commas.\"\"\"\n+    # This should not raise SystemExit because the regex is valid\n+    Run(\n+        [str(EMPTY_MODULE), r\"--bad-names-rgxs=(foo{1\\,3})\"],\n+        exit=False,\n+    )\n+    output = capsys.readouterr()\n+    # Should not have the regex error\n+    assert \"Error in provided regular expression\" not in output.err\n+\n+\n def test_short_verbose(capsys: CaptureFixture) -> None:\n     \"\"\"Check that we correctly handle the -v flag.\"\"\"\n     Run([str(EMPTY_MODULE), \"-v\"], exit=False)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-10051", "model_patch": "diff --git a/src/_pytest/logging.py b/src/_pytest/logging.py\nindex a4f4214b1..6709bf8b4 100644\n--- a/src/_pytest/logging.py\n+++ b/src/_pytest/logging.py\n@@ -441,6 +441,10 @@ class LogCaptureFixture:\n     def clear(self) -> None:\n         \"\"\"Reset the list of log records and the captured log text.\"\"\"\n         self.handler.reset()\n+        # Update the records in the stash for all phases\n+        records_dict = self._item.stash.get(caplog_records_key, {})\n+        for when in records_dict:\n+            records_dict[when] = self.handler.records\n \n     def set_level(self, level: Union[int, str], logger: Optional[str] = None) -> None:\n         \"\"\"Set the level of a logger for the duration of a test.\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-10081", "model_patch": "diff --git a/src/_pytest/unittest.py b/src/_pytest/unittest.py\nindex 851e4943b..fc8716454 100644\n--- a/src/_pytest/unittest.py\n+++ b/src/_pytest/unittest.py\n@@ -316,7 +316,7 @@ class TestCaseFunction(Function):\n             # Arguably we could always postpone tearDown(), but this changes the moment where the\n             # TestCase instance interacts with the results object, so better to only do it\n             # when absolutely needed.\n-            if self.config.getoption(\"usepdb\") and not _is_skipped(self.obj):\n+            if self.config.getoption(\"usepdb\") and not _is_skipped(self.obj) and not _is_skipped(self.parent.obj):\n                 self._explicit_tearDown = self._testcase.tearDown\n                 setattr(self._testcase, \"tearDown\", lambda *args: None)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-10356", "model_patch": "diff --git a/src/_pytest/mark/structures.py b/src/_pytest/mark/structures.py\nindex 800a25c92..fae5fef78 100644\n--- a/src/_pytest/mark/structures.py\n+++ b/src/_pytest/mark/structures.py\n@@ -357,10 +357,26 @@ class MarkDecorator:\n \n def get_unpacked_marks(obj: object) -> Iterable[Mark]:\n     \"\"\"Obtain the unpacked marks that are stored on an object.\"\"\"\n-    mark_list = getattr(obj, \"pytestmark\", [])\n-    if not isinstance(mark_list, list):\n-        mark_list = [mark_list]\n-    return normalize_mark_list(mark_list)\n+    # If obj is a class, consider marks from all classes in the MRO\n+    if isinstance(obj, type):\n+        marks = []\n+        seen_mark_names = set()\n+        for cls in obj.__mro__:\n+            mark_list = getattr(cls, \"pytestmark\", [])\n+            if not isinstance(mark_list, list):\n+                mark_list = [mark_list]\n+            # Add marks, but avoid duplicates by name (MRO order)\n+            for mark in mark_list:\n+                if mark.name not in seen_mark_names:\n+                    marks.append(mark)\n+                    seen_mark_names.add(mark.name)\n+        return normalize_mark_list(marks)\n+    else:\n+        # For non-class objects, use the original behavior\n+        mark_list = getattr(obj, \"pytestmark\", [])\n+        if not isinstance(mark_list, list):\n+            mark_list = [mark_list]\n+        return normalize_mark_list(mark_list)\n \n \n def normalize_mark_list(\ndiff --git a/testing/test_multiple_inheritance_marks.py b/testing/test_multiple_inheritance_marks.py\nnew file mode 100644\nindex 000000000..e8a84e084\n--- /dev/null\n+++ b/testing/test_multiple_inheritance_marks.py\n@@ -0,0 +1,55 @@\n+import pytest\n+\n+def test_multiple_inheritance_marks() -> None:\n+    \"\"\"Test that marks from multiple base classes are correctly merged.\"\"\"\n+    # Create a simple test to check our fix\n+    import sys\n+    from _pytest.mark.structures import get_unpacked_marks\n+    \n+    # Define classes with marks\n+    @pytest.mark.foo\n+    class Foo(object):\n+        pass\n+\n+    @pytest.mark.bar\n+    class Bar(object):\n+        pass\n+\n+    class TestDings(Foo, Bar):\n+        def test_dings(self):\n+            pass\n+    \n+    # Check that both marks are present\n+    marks = list(get_unpacked_marks(TestDings))\n+    mark_names = [mark.name for mark in marks]\n+    \n+    assert \"foo\" in mark_names\n+    assert \"bar\" in mark_names\n+    assert len(marks) == 2\n+\n+def test_multiple_inheritance_marks_with_same_name() -> None:\n+    \"\"\"Test that marks with the same name from different base classes are handled correctly.\"\"\"\n+    # Create a simple test to check our fix\n+    import sys\n+    from _pytest.mark.structures import get_unpacked_marks\n+    \n+    # Define classes with marks\n+    @pytest.mark.skip(param=\"base1\")\n+    class Base1(object):\n+        pass\n+\n+    @pytest.mark.skip(param=\"base2\")\n+    class Base2(object):\n+        pass\n+\n+    class TestDings(Base1, Base2):\n+        def test_dings(self):\n+            pass\n+    \n+    # Check that only one marker is present (from the first base class in MRO)\n+    marks = list(get_unpacked_marks(TestDings))\n+    mark_names = [mark.name for mark in marks]\n+    \n+    assert \"skip\" in mark_names\n+    assert len(marks) == 1\n+    assert marks[0].kwargs == {\"param\": \"base1\"}\n\\ No newline at end of file\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-5262", "model_patch": "diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py\nindex 25eab7fd..6759614e 100644\n--- a/src/_pytest/capture.py\n+++ b/src/_pytest/capture.py\n@@ -447,6 +447,15 @@ class EncodedFile(object):\n         \"\"\"Ensure that file.name is a string.\"\"\"\n         return repr(self.buffer)\n \n+    @property\n+    def mode(self):\n+        \"\"\"Return the mode of the file, without 'b' even if the underlying buffer is binary.\"\"\"\n+        buffer_mode = getattr(self.buffer, \"mode\", None)\n+        if buffer_mode and \"b\" in buffer_mode:\n+            # Remove 'b' from mode since EncodedFile handles text, not binary data\n+            return buffer_mode.replace(\"b\", \"\")\n+        return buffer_mode\n+\n     def __getattr__(self, name):\n         return getattr(object.__getattribute__(self, \"buffer\"), name)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-5631", "model_patch": "diff --git a/src/_pytest/compat.py b/src/_pytest/compat.py\nindex d238061b4..1c332d1a0 100644\n--- a/src/_pytest/compat.py\n+++ b/src/_pytest/compat.py\n@@ -67,9 +67,18 @@ def num_mock_patch_args(function):\n     mock_modules = [sys.modules.get(\"mock\"), sys.modules.get(\"unittest.mock\")]\n     if any(mock_modules):\n         sentinels = [m.DEFAULT for m in mock_modules if m is not None]\n-        return len(\n-            [p for p in patchings if not p.attribute_name and p.new in sentinels]\n-        )\n+        count = 0\n+        for p in patchings:\n+            if not p.attribute_name:\n+                try:\n+                    # Check if p.new is in sentinels, but handle the case where\n+                    # p.new is an array-like object that would cause a ValueError\n+                    if p.new in sentinels:\n+                        count += 1\n+                except ValueError:\n+                    # If p.new is an array-like object, it's not a sentinel\n+                    pass\n+        return count\n     return len(patchings)\n \n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-5787", "model_patch": "diff --git a/src/_pytest/reports.py b/src/_pytest/reports.py\nindex 4682d5b6e..649391836 100644\n--- a/src/_pytest/reports.py\n+++ b/src/_pytest/reports.py\n@@ -3,6 +3,7 @@ from typing import Optional\n \n import py\n \n+from _pytest._code.code import ExceptionChainRepr\n from _pytest._code.code import ExceptionInfo\n from _pytest._code.code import ReprEntry\n from _pytest._code.code import ReprEntryNative\n@@ -184,9 +185,37 @@ class BaseReport:\n                 \"sections\": rep.longrepr.sections,\n             }\n \n+        def disassembled_chainrep(rep):\n+            chain = []\n+            for reprtraceback, reprcrash, descr in rep.longrepr.chain:\n+                reprtraceback_data = reprtraceback.__dict__.copy()\n+                reprcrash_data = reprcrash.__dict__.copy() if reprcrash else None\n+\n+                new_entries = []\n+                for entry in reprtraceback_data[\"reprentries\"]:\n+                    entry_data = {\n+                        \"type\": type(entry).__name__,\n+                        \"data\": entry.__dict__.copy(),\n+                    }\n+                    for key, value in entry_data[\"data\"].items():\n+                        if hasattr(value, \"__dict__\"):\n+                            entry_data[\"data\"][key] = value.__dict__.copy()\n+                    new_entries.append(entry_data)\n+\n+                reprtraceback_data[\"reprentries\"] = new_entries\n+\n+                chain.append((reprtraceback_data, reprcrash_data, descr))\n+\n+            return {\n+                \"chain\": chain,\n+                \"sections\": rep.longrepr.sections,\n+            }\n+\n         d = self.__dict__.copy()\n         if hasattr(self.longrepr, \"toterminal\"):\n-            if hasattr(self.longrepr, \"reprtraceback\") and hasattr(\n+            if hasattr(self.longrepr, \"chain\"):\n+                d[\"longrepr\"] = disassembled_chainrep(self)\n+            elif hasattr(self.longrepr, \"reprtraceback\") and hasattr(\n                 self.longrepr, \"reprcrash\"\n             ):\n                 d[\"longrepr\"] = disassembled_report(self)\n@@ -255,6 +284,47 @@ class BaseReport:\n                     reprcrash=ReprFileLocation(**reprcrash),\n                 )\n \n+                for section in reportdict[\"longrepr\"][\"sections\"]:\n+                    exception_info.addsection(*section)\n+                reportdict[\"longrepr\"] = exception_info\n+            elif \"chain\" in reportdict[\"longrepr\"]:\n+                chain = []\n+                for reprtraceback_data, reprcrash_data, descr in reportdict[\"longrepr\"][\"chain\"]:\n+                    unserialized_entries = []\n+                    for entry_data in reprtraceback_data[\"reprentries\"]:\n+                        data = entry_data[\"data\"]\n+                        entry_type = entry_data[\"type\"]\n+                        if entry_type == \"ReprEntry\":\n+                            reprfuncargs = None\n+                            reprfileloc = None\n+                            reprlocals = None\n+                            if data[\"reprfuncargs\"]:\n+                                reprfuncargs = ReprFuncArgs(**data[\"reprfuncargs\"])\n+                            if data[\"reprfileloc\"]:\n+                                reprfileloc = ReprFileLocation(**data[\"reprfileloc\"])\n+                            if data[\"reprlocals\"]:\n+                                reprlocals = ReprLocals(data[\"reprlocals\"][\"lines\"])\n+\n+                            reprentry = ReprEntry(\n+                                lines=data[\"lines\"],\n+                                reprfuncargs=reprfuncargs,\n+                                reprlocals=reprlocals,\n+                                filelocrepr=reprfileloc,\n+                                style=data[\"style\"],\n+                            )\n+                        elif entry_type == \"ReprEntryNative\":\n+                            reprentry = ReprEntryNative(data[\"lines\"])\n+                        else:\n+                            _report_unserialization_failure(entry_type, cls, reportdict)\n+                        unserialized_entries.append(reprentry)\n+                    reprtraceback_data[\"reprentries\"] = unserialized_entries\n+\n+                    reprtraceback = ReprTraceback(**reprtraceback_data)\n+                    reprcrash = ReprFileLocation(**reprcrash_data) if reprcrash_data else None\n+                    chain.append((reprtraceback, reprcrash, descr))\n+\n+                exception_info = ExceptionChainRepr(chain)\n+\n                 for section in reportdict[\"longrepr\"][\"sections\"]:\n                     exception_info.addsection(*section)\n                 reportdict[\"longrepr\"] = exception_info\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-5809", "model_patch": "diff --git a/src/_pytest/pastebin.py b/src/_pytest/pastebin.py\nindex 41576a615..79b0d9d40 100644\n--- a/src/_pytest/pastebin.py\n+++ b/src/_pytest/pastebin.py\n@@ -79,7 +79,7 @@ def create_new_paste(contents):\n \n     params = {\n         \"code\": contents,\n-        \"lexer\": \"python3\" if sys.version_info[0] >= 3 else \"python\",\n+        \"lexer\": \"text\",\n         \"expiry\": \"1week\",\n     }\n     url = \"https://bpaste.net\"\ndiff --git a/testing/test_pastebin.py b/testing/test_pastebin.py\nindex bac8f9802..aac0b651e 100644\n--- a/testing/test_pastebin.py\n+++ b/testing/test_pastebin.py\n@@ -126,8 +126,7 @@ class TestPaste(object):\n         assert len(mocked_urlopen) == 1\n         url, data = mocked_urlopen[0]\n         assert type(data) is bytes\n-        lexer = \"python3\" if sys.version_info[0] >= 3 else \"python\"\n         assert url == \"https://bpaste.net\"\n-        assert \"lexer=%s\" % lexer in data.decode()\n+        assert \"lexer=text\" in data.decode()\n         assert \"code=full-paste-contents\" in data.decode()\n         assert \"expiry=1week\" in data.decode()\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-5840", "model_patch": "diff --git a/src/_pytest/config/__init__.py b/src/_pytest/config/__init__.py\nindex 3a0eca546..f26eed0cb 100644\n--- a/src/_pytest/config/__init__.py\n+++ b/src/_pytest/config/__init__.py\n@@ -435,14 +435,15 @@ class PytestPluginManager(PluginManager):\n         # Use realpath to avoid loading the same conftest twice\n         # with build systems that create build directories containing\n         # symlinks to actual files.\n-        conftestpath = unique_path(conftestpath)\n+        normalized_path = unique_path(conftestpath)\n         try:\n-            return self._conftestpath2mod[conftestpath]\n+            return self._conftestpath2mod[normalized_path]\n         except KeyError:\n-            pkgpath = conftestpath.pypkgpath()\n+            pkgpath = normalized_path.pypkgpath()\n             if pkgpath is None:\n-                _ensure_removed_sysmodule(conftestpath.purebasename)\n+                _ensure_removed_sysmodule(normalized_path.purebasename)\n             try:\n+                # Use the original path for importing to preserve case\n                 mod = conftestpath.pyimport()\n                 if (\n                     hasattr(mod, \"pytest_plugins\")\n@@ -454,7 +455,7 @@ class PytestPluginManager(PluginManager):\n                 raise ConftestImportFailure(conftestpath, sys.exc_info())\n \n             self._conftest_plugins.add(mod)\n-            self._conftestpath2mod[conftestpath] = mod\n+            self._conftestpath2mod[normalized_path] = mod\n             dirpath = conftestpath.dirpath()\n             if dirpath in self._dirpath2confmods:\n                 for path, mods in self._dirpath2confmods.items():\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-6197", "model_patch": "diff --git a/src/_pytest/main.py b/src/_pytest/main.py\nindex 3c205027d..eea8196c3 100644\n--- a/src/_pytest/main.py\n+++ b/src/_pytest/main.py\n@@ -564,13 +564,9 @@ class Session(nodes.FSCollector):\n             # Module itself, so just use that. If this special case isn't taken, then all\n             # the files in the package will be yielded.\n             if argpath.basename == \"__init__.py\":\n-                try:\n-                    yield next(m[0].collect())\n-                except StopIteration:\n-                    # The package collects nothing with only an __init__.py\n-                    # file in it, which gets ignored by the default\n-                    # \"python_files\" option.\n-                    pass\n+                collected = m[0].collect()\n+                if collected:\n+                    yield from collected\n                 return\n             yield from m\n \ndiff --git a/src/_pytest/python.py b/src/_pytest/python.py\nindex 306e5f217..6810bed6d 100644\n--- a/src/_pytest/python.py\n+++ b/src/_pytest/python.py\n@@ -176,9 +176,20 @@ def pytest_pyfunc_call(pyfuncitem):\n def pytest_collect_file(path, parent):\n     ext = path.ext\n     if ext == \".py\":\n-        if not parent.session.isinitpath(path):\n-            if not path_matches_patterns(\n-                path, parent.config.getini(\"python_files\") + [\"__init__.py\"]\n+        # If the path is explicitly specified by the user, collect it\n+        if parent.session.isinitpath(path):\n+            # Collect explicitly specified paths\n+            pass\n+        else:\n+            # For __init__.py files, only collect them if they match the python_files pattern\n+            if path.basename == \"__init__.py\":\n+                if not path_matches_patterns(\n+                    path, parent.config.getini(\"python_files\")\n+                ):\n+                    return\n+            # For other .py files, collect them if they match the python_files pattern\n+            elif not path_matches_patterns(\n+                path, parent.config.getini(\"python_files\")\n             ):\n                 return\n         ihook = parent.session.gethookproxy(path)\n@@ -192,6 +203,9 @@ def path_matches_patterns(path, patterns):\n \n def pytest_pycollect_makemodule(path, parent):\n     if path.basename == \"__init__.py\":\n+        # If the path is explicitly specified by the user, treat it as a Module\n+        if parent.session.isinitpath(path):\n+            return Module(path, parent)\n         return Package(path, parent)\n     return Module(path, parent)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-6202", "model_patch": "diff --git a/src/_pytest/python.py b/src/_pytest/python.py\nindex b8b365ad3..5f2af31e2 100644\n--- a/src/_pytest/python.py\n+++ b/src/_pytest/python.py\n@@ -286,7 +286,7 @@ class PyobjMixin(PyobjContext):\n             parts.append(name)\n         parts.reverse()\n         s = \".\".join(parts)\n-        return s.replace(\".[\", \"[\")\n+        return s\n \n     def reportinfo(self):\n         # XXX caching?\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7205", "model_patch": "diff --git a/src/_pytest/setuponly.py b/src/_pytest/setuponly.py\nindex c9cc589ff..9fc218a3e 100644\n--- a/src/_pytest/setuponly.py\n+++ b/src/_pytest/setuponly.py\n@@ -1,4 +1,5 @@\n import pytest\n+from _pytest._io.saferepr import saferepr\n \n \n def pytest_addoption(parser):\n@@ -66,7 +67,7 @@ def _show_fixture_action(fixturedef, msg):\n             tw.write(\" (fixtures used: {})\".format(\", \".join(deps)))\n \n     if hasattr(fixturedef, \"cached_param\"):\n-        tw.write(\"[{}]\".format(fixturedef.cached_param))\n+        tw.write(\"[{}]\".format(saferepr(fixturedef.cached_param)))\n \n     tw.flush()\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7236", "model_patch": "diff --git a/src/_pytest/unittest.py b/src/_pytest/unittest.py\nindex 773f545af..b07c7a6cb 100644\n--- a/src/_pytest/unittest.py\n+++ b/src/_pytest/unittest.py\n@@ -220,7 +220,15 @@ class TestCaseFunction(Function):\n             # arguably we could always postpone tearDown(), but this changes the moment where the\n             # TestCase instance interacts with the results object, so better to only do it\n             # when absolutely needed\n-            if self.config.getoption(\"usepdb\"):\n+            # However, we should not do this for skipped tests, as tearDown should not be called for them\n+            # Let's check if the test is skipped\n+            # Check if the test method itself is skipped\n+            test_method = getattr(self._testcase, self.name)\n+            method_is_skipped = getattr(test_method, \"__unittest_skip__\", False)\n+            # Check if the class is skipped\n+            class_is_skipped = getattr(self._testcase, \"__unittest_skip__\", False)\n+            is_skipped = method_is_skipped or class_is_skipped\n+            if self.config.getoption(\"usepdb\") and not is_skipped:\n                 self._explicit_tearDown = self._testcase.tearDown\n                 setattr(self._testcase, \"tearDown\", lambda *args: None)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7324", "model_patch": "diff --git a/src/_pytest/mark/expression.py b/src/_pytest/mark/expression.py\nindex 04c73411a..9abdecdc7 100644\n--- a/src/_pytest/mark/expression.py\n+++ b/src/_pytest/mark/expression.py\n@@ -161,7 +161,11 @@ def not_expr(s: Scanner) -> ast.expr:\n         return ret\n     ident = s.accept(TokenType.IDENT)\n     if ident:\n-        return ast.Name(ident.value, ast.Load())\n+        # Mangle Python keywords to avoid conflicts in Python 3.8+\n+        value = ident.value\n+        if value in (\"True\", \"False\", \"None\"):\n+            value = \"$\" + value\n+        return ast.Name(value, ast.Load())\n     s.reject((TokenType.NOT, TokenType.LPAREN, TokenType.IDENT))\n \n \n@@ -172,6 +176,9 @@ class MatcherAdapter(Mapping[str, bool]):\n         self.matcher = matcher\n \n     def __getitem__(self, key: str) -> bool:\n+        # Unmangle Python keywords\n+        if key.startswith(\"$\") and key[1:] in (\"True\", \"False\", \"None\"):\n+            key = key[1:]\n         return self.matcher(key)\n \n     def __iter__(self) -> Iterator[str]:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7432", "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 7bd975e5a..691347be8 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -262,6 +262,20 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n     xfailed = item._store.get(xfailed_key, None)\n+    # Handle skip location reporting first, regardless of --runxfail\n+    if (\n+        item._store.get(skipped_by_mark_key, True)\n+        and rep.skipped\n+        and type(rep.longrepr) is tuple\n+    ):\n+        # skipped by mark.skipif; change the location of the failure\n+        # to point to the item definition, otherwise it will display\n+        # the location of where the skip exception was raised within pytest\n+        _, _, reason = rep.longrepr\n+        filename, line = item.reportinfo()[:2]\n+        assert line is not None\n+        rep.longrepr = str(filename), line + 1, reason\n+\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n@@ -270,13 +284,11 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         else:\n             rep.longrepr = \"Unexpected success\"\n         rep.outcome = \"failed\"\n-    elif item.config.option.runxfail:\n-        pass  # don't interfere\n     elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n-    elif not rep.skipped and xfailed:\n+    elif not rep.skipped and xfailed and not item.config.option.runxfail:\n         if call.excinfo:\n             raises = xfailed.raises\n             if raises is not None and not isinstance(call.excinfo.value, raises):\n@@ -291,18 +303,6 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n             else:\n                 rep.outcome = \"passed\"\n                 rep.wasxfail = xfailed.reason\n-    elif (\n-        item._store.get(skipped_by_mark_key, True)\n-        and rep.skipped\n-        and type(rep.longrepr) is tuple\n-    ):\n-        # skipped by mark.skipif; change the location of the failure\n-        # to point to the item definition, otherwise it will display\n-        # the location of where the skip exception was raised within pytest\n-        _, _, reason = rep.longrepr\n-        filename, line = item.reportinfo()[:2]\n-        assert line is not None\n-        rep.longrepr = str(filename), line + 1, reason\n \n \n def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7490", "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..ff4dec977 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -276,21 +276,32 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n         assert call.excinfo.value.msg is not None\n         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n         rep.outcome = \"skipped\"\n-    elif not rep.skipped and xfailed:\n-        if call.excinfo:\n-            raises = xfailed.raises\n-            if raises is not None and not isinstance(call.excinfo.value, raises):\n-                rep.outcome = \"failed\"\n-            else:\n-                rep.outcome = \"skipped\"\n-                rep.wasxfail = xfailed.reason\n-        elif call.when == \"call\":\n-            if xfailed.strict:\n-                rep.outcome = \"failed\"\n-                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n-            else:\n-                rep.outcome = \"passed\"\n-                rep.wasxfail = xfailed.reason\n+    else:\n+        # Re-evaluate xfail marks if we have a failure and no xfail was previously detected.\n+        # This handles the case where xfail marks are added dynamically during test execution.\n+        # Only do this during the 'call' phase to avoid interfering with setup/teardown errors.\n+        if xfailed is None and call.excinfo and not rep.skipped and call.when == \"call\":\n+            try:\n+                xfailed = evaluate_xfail_marks(item)\n+            except Exception:\n+                # If evaluating xfail marks fails, we ignore it to avoid masking the original error.\n+                # This maintains the same behavior as the original code.\n+                pass\n+        if not rep.skipped and xfailed:\n+            if call.excinfo:\n+                raises = xfailed.raises\n+                if raises is not None and not isinstance(call.excinfo.value, raises):\n+                    rep.outcome = \"failed\"\n+                else:\n+                    rep.outcome = \"skipped\"\n+                    rep.wasxfail = xfailed.reason\n+            elif call.when == \"call\":\n+                if xfailed.strict:\n+                    rep.outcome = \"failed\"\n+                    rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n+                else:\n+                    rep.outcome = \"passed\"\n+                    rep.wasxfail = xfailed.reason\n \n     if (\n         item._store.get(skipped_by_mark_key, True)\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7521", "model_patch": "diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py\nindex 3f9c60fb9..01d2706f8 100644\n--- a/src/_pytest/capture.py\n+++ b/src/_pytest/capture.py\n@@ -388,6 +388,7 @@ class FDCaptureBinary:\n                 TemporaryFile(buffering=0),  # type: ignore[arg-type]\n                 encoding=\"utf-8\",\n                 errors=\"replace\",\n+                newline=\"\",  # Don't translate newlines\n                 write_through=True,\n             )\n             if targetfd in patchsysdict:\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7571", "model_patch": "diff --git a/src/_pytest/logging.py b/src/_pytest/logging.py\nindex 11031f2f2..a3f0b1521 100644\n--- a/src/_pytest/logging.py\n+++ b/src/_pytest/logging.py\n@@ -432,7 +432,8 @@ class LogCaptureFixture:\n         \"\"\"\n         logger_obj = logging.getLogger(logger)\n         # save the original log-level to restore it during teardown\n-        self._initial_logger_levels.setdefault(logger, logger_obj.level)\n+        if logger not in self._initial_logger_levels:\n+            self._initial_logger_levels[logger] = logger_obj.level\n         logger_obj.setLevel(level)\n         self.handler.setLevel(level)\n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-7982", "model_patch": "diff --git a/src/_pytest/pathlib.py b/src/_pytest/pathlib.py\nindex f0bdb1481..b96cba069 100644\n--- a/src/_pytest/pathlib.py\n+++ b/src/_pytest/pathlib.py\n@@ -558,7 +558,7 @@ def visit(\n     entries = sorted(os.scandir(path), key=lambda entry: entry.name)\n     yield from entries\n     for entry in entries:\n-        if entry.is_dir(follow_symlinks=False) and recurse(entry):\n+        if entry.is_dir() and recurse(entry):\n             yield from visit(entry.path, recurse)\n \n \n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "pytest-dev__pytest-8399", "model_patch": "diff --git a/src/_pytest/unittest.py b/src/_pytest/unittest.py\nindex 719eb4e88..3f88d7a9e 100644\n--- a/src/_pytest/unittest.py\n+++ b/src/_pytest/unittest.py\n@@ -144,7 +144,7 @@ def _make_xunit_fixture(\n         scope=scope,\n         autouse=True,\n         # Use a unique name to speed up lookup.\n-        name=f\"unittest_{setup_name}_fixture_{obj.__qualname__}\",\n+        name=f\"_unittest_{setup_name}_fixture_{obj.__qualname__}\",\n     )\n     def fixture(self, request: FixtureRequest) -> Generator[None, None, None]:\n         if _is_skipped(self):\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "scikit-learn__scikit-learn-10297", "model_patch": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 911c74ae3..6469cdbcf 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1301,6 +1301,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n         weights inversely proportional to class frequencies in the input data\n         as ``n_samples / (n_classes * np.bincount(y))``\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the `cv_values_` attribute (see\n+        below). This flag is only compatible with `cv=None` (i.e. using\n+        Generalized Cross-Validation).\n+\n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n@@ -1333,10 +1339,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n \n     def fit(self, X, y, sample_weight=None):\n"}
{"model_name_or_path": "openrouter_qwen_qwen3-coder", "instance_id": "scikit-learn__scikit-learn-10844", "model_patch": "diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py\nindex a987778ae..9bcca75a2 100644\n--- a/sklearn/metrics/cluster/supervised.py\n+++ b/sklearn/metrics/cluster/supervised.py\n@@ -856,7 +856,7 @@ def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):\n     tk = np.dot(c.data, c.data) - n_samples\n     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples\n     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples\n-    return tk / np.sqrt(pk * qk) if tk != 0. else 0.\n+    return np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.\n \n \n def entropy(labels):\n"}
