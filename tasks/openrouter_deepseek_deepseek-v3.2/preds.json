{
    "matplotlib__matplotlib-26291": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "matplotlib__matplotlib-26291",
        "model_patch": "diff --git a/reproduce_error.py b/reproduce_error.py\nnew file mode 100644\nindex 0000000000..a5bd52155a\n--- /dev/null\n+++ b/reproduce_error.py\n@@ -0,0 +1,7 @@\n+import matplotlib.pyplot as plt\n+from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n+\n+\n+fig, (ax, ax2) = plt.subplots(1, 2, figsize=[5.5, 2.8])\n+axins = inset_axes(ax, width=1.3, height=0.9)\n+plt.show()\n\\ No newline at end of file\ndiff --git a/reproduce_error2.py b/reproduce_error2.py\nnew file mode 100644\nindex 0000000000..eb62a31ffb\n--- /dev/null\n+++ b/reproduce_error2.py\n@@ -0,0 +1,16 @@\n+import matplotlib\n+matplotlib.use('Agg')  # Use non-interactive backend\n+import matplotlib.pyplot as plt\n+from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n+\n+fig, (ax, ax2) = plt.subplots(1, 2, figsize=[5.5, 2.8])\n+axins = inset_axes(ax, width=1.3, height=0.9)\n+\n+# Try to save the figure which should trigger the error\n+try:\n+    fig.savefig('test.png')\n+    print(\"Figure saved successfully\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n\\ No newline at end of file\n"
    },
    "matplotlib__matplotlib-26342": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "matplotlib__matplotlib-26342",
        "model_patch": ""
    },
    "matplotlib__matplotlib-26466": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "matplotlib__matplotlib-26466",
        "model_patch": "diff --git a/bug_output.png b/bug_output.png\nnew file mode 100644\nindex 0000000000..7b140b8a3e\nBinary files /dev/null and b/bug_output.png differ\ndiff --git a/reproduce_bug.py b/reproduce_bug.py\nnew file mode 100644\nindex 0000000000..81dbf4362a\n--- /dev/null\n+++ b/reproduce_bug.py\n@@ -0,0 +1,34 @@\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+fig = plt.figure(\"test\")\n+\n+ax = fig.add_axes([0.13, 0.15, .8, .8])\n+ax.set_xlim(-5, 5)\n+ax.set_ylim(-3, 3)\n+\n+xy_0 = np.array((-4, 1))\n+xy_f = np.array((-1, 1))\n+# this annotation is messed by later changing the array passed as xy kwarg\n+ax.annotate('', xy=xy_0, xytext=xy_f, arrowprops=dict(arrowstyle='<->'))\n+xy_0[1] = 3# <--this  updates the arrow position\n+\n+xy_0 = np.array((1, 1))\n+xy_f = np.array((4, 1))\n+# using a copy of the array helps spoting where the problem is\n+ax.annotate('', xy=xy_0.copy(), xytext=xy_f, arrowprops=dict(arrowstyle='<->'))\n+xy_0[1] = 3\n+\n+# Save the figure to see the bug\n+plt.savefig('/testbed/bug_output.png')\n+print(\"Figure saved to /testbed/bug_output.png\")\n+\n+# Also print the xy values to debug\n+print(\"First annotation xy:\", ax.texts[0].xy)\n+print(\"Second annotation xy:\", ax.texts[1].xy)\n+\n+# Check if modifying the original array affects the annotation\n+print(\"\\nChecking if modifying original array affects stored xy:\")\n+print(\"Original xy_0 after modification:\", xy_0)\n+print(\"First annotation xy (should be (-4, 1) but might be (-4, 3)):\", ax.texts[0].xy)\n+print(\"Second annotation xy (should be (1, 1)):\", ax.texts[1].xy)\n\\ No newline at end of file\n"
    },
    "mwaskom__seaborn-3069": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "mwaskom__seaborn-3069",
        "model_patch": ""
    },
    "mwaskom__seaborn-3187": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "mwaskom__seaborn-3187",
        "model_patch": "diff --git a/original_plot.png b/original_plot.png\nnew file mode 100644\nindex 00000000..72354204\nBinary files /dev/null and b/original_plot.png differ\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..73c345b4\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,63 @@\n+import seaborn as sns\n+import seaborn.objects as so\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+# Load penguins dataset\n+penguins = sns.load_dataset(\"penguins\")\n+penguins[\"body_mass_mg\"] = penguins[\"body_mass_g\"] * 1000\n+\n+print(\"Testing seaborn.objects plot...\")\n+# Create plot using seaborn.objects\n+plot = (\n+    so.Plot(\n+        penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n+        color=\"species\", pointsize=\"body_mass_mg\",\n+    )\n+    .add(so.Dot())\n+)\n+\n+# Save the plot\n+fig = plot.plot()\n+plt.savefig(\"/testbed/original_plot.png\")\n+print(\"Plot saved to /testbed/original_plot.png\")\n+\n+# Let's also test with seaborn's scatterplot\n+print(\"\\n\\nTesting with seaborn.scatterplot...\")\n+fig2, ax2 = plt.subplots()\n+scatter = sns.scatterplot(\n+    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n+    hue=\"species\", size=\"body_mass_mg\", ax=ax2\n+)\n+plt.savefig(\"/testbed/scatterplot.png\")\n+print(\"Scatterplot saved to /testbed/scatterplot.png\")\n+\n+# Get the legend handles and labels\n+handles, labels = ax2.get_legend_handles_labels()\n+print(f\"\\nScatterplot legend labels: {labels}\")\n+\n+# Now let's test the ScalarFormatter issue directly\n+print(\"\\n\\nTesting ScalarFormatter behavior...\")\n+import matplotlib as mpl\n+from matplotlib.ticker import ScalarFormatter\n+\n+# Create a mock axis to test formatter\n+fig3, ax3 = plt.subplots()\n+ax3.plot([1000000, 5000000], [0, 1])\n+\n+# Get the formatter from the axis\n+formatter = ax3.xaxis.get_major_formatter()\n+\n+# Test formatting some values\n+test_values = [1000000, 2000000, 3000000, 4000000, 5000000]\n+labels = formatter.format_ticks(test_values)\n+print(f\"Formatted labels: {labels}\")\n+print(f\"Formatter offset: {formatter.get_offset()}\")\n+print(f\"Formatter useOffset: {formatter.get_useOffset()}\")\n+\n+# Check what happens when we format individual values\n+for val in test_values:\n+    label = formatter(val)\n+    print(f\"Value {val}: '{label}'\")\n+\n+plt.close('all')\n\\ No newline at end of file\ndiff --git a/scatterplot.png b/scatterplot.png\nnew file mode 100644\nindex 00000000..55f0c363\nBinary files /dev/null and b/scatterplot.png differ\n"
    },
    "pallets__flask-5014": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pallets__flask-5014",
        "model_patch": "diff --git a/test_empty_blueprint.py b/test_empty_blueprint.py\nnew file mode 100644\nindex 00000000..81b98a83\n--- /dev/null\n+++ b/test_empty_blueprint.py\n@@ -0,0 +1,34 @@\n+#!/usr/bin/env python3\n+\"\"\"Test script to reproduce the empty blueprint name issue.\"\"\"\n+\n+import flask\n+\n+print(\"Testing Blueprint with empty name...\")\n+try:\n+    # Try to create a blueprint with an empty name\n+    bp = flask.Blueprint(\"\", __name__)\n+    print(\"ERROR: Blueprint with empty name was created without raising ValueError!\")\n+    print(f\"Blueprint name: '{bp.name}'\")\n+except ValueError as e:\n+    print(f\"SUCCESS: ValueError raised: {e}\")\n+except Exception as e:\n+    print(f\"ERROR: Unexpected exception raised: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTesting Blueprint with non-empty name...\")\n+try:\n+    # Try to create a blueprint with a valid name\n+    bp = flask.Blueprint(\"myblueprint\", __name__)\n+    print(f\"SUCCESS: Blueprint with name '{bp.name}' created successfully\")\n+except Exception as e:\n+    print(f\"ERROR: Unexpected exception raised: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTesting Blueprint with whitespace-only name...\")\n+try:\n+    # Try to create a blueprint with whitespace-only name\n+    bp = flask.Blueprint(\"   \", __name__)\n+    print(f\"Blueprint with whitespace-only name created: '{bp.name}'\")\n+    print(\"Should this be allowed?\")\n+except ValueError as e:\n+    print(f\"ValueError raised: {e}\")\n+except Exception as e:\n+    print(f\"ERROR: Unexpected exception raised: {type(e).__name__}: {e}\")\n\\ No newline at end of file\n"
    },
    "psf__requests-1142": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-1142",
        "model_patch": "diff --git a/test_content_length.py b/test_content_length.py\nnew file mode 100644\nindex 0000000..cb3de58\n--- /dev/null\n+++ b/test_content_length.py\n@@ -0,0 +1,38 @@\n+#!/usr/bin/env python\n+import requests\n+import json\n+\n+# Test to see if requests.get sends content-length header\n+# We'll use httpbin.org to inspect the headers sent\n+\n+print(\"Testing requests.get with no data...\")\n+r = requests.get('http://httpbin.org/get')\n+response = r.json()\n+print(\"Headers sent:\", json.dumps(response['headers'], indent=2))\n+\n+# Check if content-length was sent\n+if 'Content-Length' in response['headers']:\n+    print(\"\\nERROR: Content-Length header was sent with GET request!\")\n+    print(f\"Content-Length value: {response['headers']['Content-Length']}\")\n+else:\n+    print(\"\\nOK: No Content-Length header sent\")\n+\n+# Test with data parameter\n+print(\"\\n\\nTesting requests.get with data parameter...\")\n+r = requests.get('http://httpbin.org/get', data={'key': 'value'})\n+response = r.json()\n+print(\"Headers sent:\", json.dumps(response['headers'], indent=2))\n+\n+if 'Content-Length' in response['headers']:\n+    print(f\"\\nContent-Length value: {response['headers']['Content-Length']}\")\n+    print(\"Note: Content-Length is expected when data is provided\")\n+\n+# Test POST for comparison\n+print(\"\\n\\nTesting requests.post for comparison...\")\n+r = requests.post('http://httpbin.org/post', data={'key': 'value'})\n+response = r.json()\n+print(\"Headers sent:\", json.dumps(response['headers'], indent=2))\n+\n+if 'Content-Length' in response['headers']:\n+    print(f\"\\nContent-Length value: {response['headers']['Content-Length']}\")\n+    print(\"Note: Content-Length is expected for POST requests with data\")\n\\ No newline at end of file\ndiff --git a/test_issue.py b/test_issue.py\nnew file mode 100644\nindex 0000000..3e60ef2\n--- /dev/null\n+++ b/test_issue.py\n@@ -0,0 +1,36 @@\n+#!/usr/bin/env python\n+import requests\n+\n+# Test to see if requests.get sends content-length header\n+# by examining the prepared request\n+\n+print(\"Testing requests.get with no data...\")\n+req = requests.Request('GET', 'http://example.com')\n+prepared = req.prepare()\n+print(f\"Headers in prepared request: {dict(prepared.headers)}\")\n+\n+if 'Content-Length' in prepared.headers:\n+    print(f\"ERROR: Content-Length header was sent with GET request!\")\n+    print(f\"Content-Length value: {prepared.headers['Content-Length']}\")\n+else:\n+    print(\"OK: No Content-Length header sent\")\n+\n+# Test with data parameter\n+print(\"\\n\\nTesting requests.get with data parameter...\")\n+req = requests.Request('GET', 'http://example.com', data={'key': 'value'})\n+prepared = req.prepare()\n+print(f\"Headers in prepared request: {dict(prepared.headers)}\")\n+\n+if 'Content-Length' in prepared.headers:\n+    print(f\"Content-Length value: {prepared.headers['Content-Length']}\")\n+    print(\"Note: Content-Length is expected when data is provided\")\n+\n+# Test POST for comparison\n+print(\"\\n\\nTesting requests.post for comparison...\")\n+req = requests.Request('POST', 'http://example.com', data={'key': 'value'})\n+prepared = req.prepare()\n+print(f\"Headers in prepared request: {dict(prepared.headers)}\")\n+\n+if 'Content-Length' in prepared.headers:\n+    print(f\"Content-Length value: {prepared.headers['Content-Length']}\")\n+    print(\"Note: Content-Length is expected for POST requests with data\")\n\\ No newline at end of file\n"
    },
    "psf__requests-1724": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-1724",
        "model_patch": ""
    },
    "psf__requests-1766": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-1766",
        "model_patch": "diff --git a/requests/auth.py b/requests/auth.py\nindex a3de123..48416e5 100644\n--- a/requests/auth.py\n+++ b/requests/auth.py\n@@ -144,7 +144,7 @@ class HTTPDigestAuth(AuthBase):\n         if entdig:\n             base += ', digest=\"%s\"' % entdig\n         if qop:\n-            base += ', qop=auth, nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n+            base += ', qop=\"auth\", nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n \n         return 'Digest %s' % (base)\n \ndiff --git a/test_digest_comprehensive.py b/test_digest_comprehensive.py\nnew file mode 100644\nindex 0000000..a83024e\n--- /dev/null\n+++ b/test_digest_comprehensive.py\n@@ -0,0 +1,68 @@\n+#!/usr/bin/env python\n+import requests\n+from requests.auth import HTTPDigestAuth\n+\n+# Test various qop scenarios\n+def test_qop_scenario(qop_value, description):\n+    print(f\"\\n--- Testing: {description} (qop='{qop_value}') ---\")\n+    \n+    auth = HTTPDigestAuth('user', 'pass')\n+    auth.chal = {\n+        'realm': 'testrealm@host.com',\n+        'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+        'qop': qop_value,\n+        'opaque': '5ccc069c403ebaf9f0171e9517f40e41'\n+    }\n+    \n+    header = auth.build_digest_header('GET', 'http://example.com/')\n+    print(f\"Generated header: {header}\")\n+    \n+    if header is None:\n+        print(\"\u2713 No header generated (auth-int not supported)\")\n+        return None\n+    \n+    # Check if qop is quoted\n+    if 'qop=\"auth\"' in header:\n+        print(\"\u2713 qop is properly quoted\")\n+    elif 'qop=auth' in header:\n+        print(\"\u2717 qop is NOT quoted (should be qop=\\\"auth\\\")\")\n+    else:\n+        print(\"? qop not found in header\")\n+    \n+    return header\n+\n+# Test cases\n+print(\"=== Testing Digest Auth qop quoting ===\")\n+print(\"According to RFC 2617, qop value should be quoted.\")\n+print()\n+\n+# Test 1: qop='auth'\n+header1 = test_qop_scenario('auth', 'Single qop value')\n+\n+# Test 2: qop='auth,auth-int'\n+header2 = test_qop_scenario('auth,auth-int', 'Multiple qop values from server')\n+\n+# Test 3: qop='auth-int'\n+header3 = test_qop_scenario('auth-int', 'Only auth-int (should not include qop in response)')\n+\n+# Test 4: No qop\n+print(\"\\n--- Testing: No qop ---\")\n+auth = HTTPDigestAuth('user', 'pass')\n+auth.chal = {\n+    'realm': 'testrealm@host.com',\n+    'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+    'opaque': '5ccc069c403ebaf9f0171e9517f40e41'\n+}\n+header4 = auth.build_digest_header('GET', 'http://example.com/')\n+print(f\"Generated header: {header4}\")\n+if 'qop=' in header4:\n+    print(\"\u2717 qop should not be present when not in challenge\")\n+else:\n+    print(\"\u2713 No qop in header (correct)\")\n+\n+print(\"\\n=== Summary ===\")\n+print(\"The issue is on line 147 of requests/auth.py:\")\n+print(\"  Current: ', qop=auth, nc=%s, cnonce=\\\"%s\\\"'\")\n+print(\"  Should be: ', qop=\\\"auth\\\", nc=%s, cnonce=\\\"%s\\\"'\")\n+print(\"\\nNote: The client should always send 'auth' as the qop value (quoted)\")\n+print(\"even if the server offers multiple options like 'auth,auth-int'.\")\n\\ No newline at end of file\ndiff --git a/test_digest_edge_cases.py b/test_digest_edge_cases.py\nnew file mode 100644\nindex 0000000..6b9f742\n--- /dev/null\n+++ b/test_digest_edge_cases.py\n@@ -0,0 +1,85 @@\n+#!/usr/bin/env python\n+import requests\n+from requests.auth import HTTPDigestAuth\n+\n+def test_digest_header_generation():\n+    \"\"\"Test various edge cases for digest auth header generation.\"\"\"\n+    \n+    print(\"=== Testing Digest Auth Header Generation ===\")\n+    \n+    # Test 1: Basic case with qop='auth'\n+    print(\"\\n1. Testing with qop='auth':\")\n+    auth = HTTPDigestAuth('user', 'pass')\n+    auth.chal = {\n+        'realm': 'testrealm@host.com',\n+        'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+        'qop': 'auth',\n+        'opaque': '5ccc069c403ebaf9f0171e9517f40e41'\n+    }\n+    header = auth.build_digest_header('GET', 'http://example.com/path')\n+    print(f\"Header: {header}\")\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted\"\n+    assert 'nc=00000001' in header, \"nc should be present\"\n+    assert 'cnonce=\"' in header, \"cnonce should be quoted\"\n+    \n+    # Test 2: Case with qop='auth,auth-int' (server offers both)\n+    print(\"\\n2. Testing with qop='auth,auth-int':\")\n+    auth.chal['qop'] = 'auth,auth-int'\n+    header = auth.build_digest_header('GET', 'http://example.com/path')\n+    print(f\"Header: {header}\")\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted and should be 'auth'\"\n+    assert 'nc=00000002' in header, \"nc should increment\"\n+    \n+    # Test 3: Case without qop (RFC 2069 compatibility)\n+    print(\"\\n3. Testing without qop (RFC 2069):\")\n+    auth.chal.pop('qop', None)\n+    header = auth.build_digest_header('GET', 'http://example.com/path')\n+    print(f\"Header: {header}\")\n+    assert 'qop=' not in header, \"qop should not be present\"\n+    assert 'nc=' not in header, \"nc should not be present\"\n+    assert 'cnonce=' not in header, \"cnonce should not be present\"\n+    \n+    # Test 4: Case with algorithm specified\n+    print(\"\\n4. Testing with algorithm='MD5':\")\n+    auth.chal['qop'] = 'auth'\n+    auth.chal['algorithm'] = 'MD5'\n+    header = auth.build_digest_header('GET', 'http://example.com/path')\n+    print(f\"Header: {header}\")\n+    assert 'algorithm=\"MD5\"' in header, \"algorithm should be quoted\"\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted\"\n+    \n+    # Test 5: Case with algorithm='MD5-sess'\n+    print(\"\\n5. Testing with algorithm='MD5-sess':\")\n+    auth.chal['algorithm'] = 'MD5-sess'\n+    header = auth.build_digest_header('GET', 'http://example.com/path')\n+    print(f\"Header: {header}\")\n+    assert 'algorithm=\"MD5-sess\"' in header, \"algorithm should be quoted\"\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted\"\n+    \n+    # Test 6: Verify all quoted parameters\n+    print(\"\\n6. Verifying all parameters are properly quoted:\")\n+    auth.chal = {\n+        'realm': 'testrealm@host.com',\n+        'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+        'qop': 'auth',\n+        'opaque': '5ccc069c403ebaf9f0171e9517f40e41',\n+        'algorithm': 'MD5'\n+    }\n+    header = auth.build_digest_header('GET', 'http://example.com/path?query=value')\n+    print(f\"Header: {header}\")\n+    \n+    # Check all quoted parameters\n+    assert 'username=\"user\"' in header\n+    assert 'realm=\"testrealm@host.com\"' in header\n+    assert 'nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\"' in header\n+    assert 'uri=\"/path?query=value\"' in header\n+    assert 'response=\"' in header  # response is always quoted\n+    assert 'opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"' in header\n+    assert 'algorithm=\"MD5\"' in header\n+    assert 'qop=\"auth\"' in header\n+    assert 'cnonce=\"' in header\n+    \n+    print(\"\\n\u2713 All tests passed!\")\n+    \n+if __name__ == '__main__':\n+    test_digest_header_generation()\n\\ No newline at end of file\ndiff --git a/test_digest_qop.py b/test_digest_qop.py\nnew file mode 100644\nindex 0000000..efc9b39\n--- /dev/null\n+++ b/test_digest_qop.py\n@@ -0,0 +1,34 @@\n+#!/usr/bin/env python\n+import requests\n+from requests.auth import HTTPDigestAuth\n+\n+# Create a mock server response to test digest auth\n+class MockResponse:\n+    def __init__(self):\n+        self.headers = {\n+            'www-authenticate': 'Digest realm=\"testrealm@host.com\", qop=\"auth,auth-int\", nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\", opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"'\n+        }\n+        self.status_code = 401\n+        self.request = type('Request', (), {'method': 'GET', 'url': 'http://example.com/'})()\n+        self.connection = type('Connection', (), {'send': lambda x, **kwargs: None})()\n+\n+# Test the digest auth header generation\n+auth = HTTPDigestAuth('user', 'pass')\n+\n+# Simulate receiving a 401 challenge\n+response = MockResponse()\n+auth.handle_401(response)\n+\n+# Now check what header would be generated\n+header = auth.build_digest_header('GET', 'http://example.com/')\n+print(\"Generated Digest header:\")\n+print(header)\n+print()\n+\n+# Check if qop is quoted\n+if 'qop=\"auth\"' in header:\n+    print(\"\u2713 qop is properly quoted\")\n+elif 'qop=auth' in header:\n+    print(\"\u2717 qop is NOT quoted (should be qop=\\\\\\\"auth\\\\\\\")\")\n+else:\n+    print(\"? qop not found in header\")\n\\ No newline at end of file\ndiff --git a/test_qop_issue.py b/test_qop_issue.py\nnew file mode 100644\nindex 0000000..63cb87e\n--- /dev/null\n+++ b/test_qop_issue.py\n@@ -0,0 +1,43 @@\n+#!/usr/bin/env python\n+import requests\n+from requests.auth import HTTPDigestAuth\n+\n+# Let's directly test the build_digest_header method\n+auth = HTTPDigestAuth('user', 'pass')\n+\n+# Set up a mock challenge\n+auth.chal = {\n+    'realm': 'testrealm@host.com',\n+    'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+    'qop': 'auth',\n+    'opaque': '5ccc069c403ebaf9f0171e9517f40e41'\n+}\n+\n+# Generate the digest header\n+header = auth.build_digest_header('GET', 'http://example.com/')\n+print(\"Generated Digest header:\")\n+print(header)\n+print()\n+\n+# Check if qop is quoted\n+if 'qop=\"auth\"' in header:\n+    print(\"\u2713 qop is properly quoted\")\n+elif 'qop=auth' in header:\n+    print(\"\u2717 qop is NOT quoted (should be qop=\\\"auth\\\")\")\n+else:\n+    print(\"? qop not found in header\")\n+\n+# Also test with qop containing multiple values\n+print(\"\\n--- Testing with qop='auth,auth-int' ---\")\n+auth.chal['qop'] = 'auth,auth-int'\n+header2 = auth.build_digest_header('GET', 'http://example.com/')\n+print(\"Generated Digest header:\")\n+print(header2)\n+print()\n+\n+# Check RFC compliance\n+print(\"\\n--- RFC 2617 Compliance Check ---\")\n+print(\"According to RFC 2617, qop-options should be a quoted string.\")\n+print(\"The server sends: qop=\\\"auth,auth-int\\\" (quoted)\")\n+print(\"The client should send: qop=\\\"auth\\\" (quoted)\")\n+print(\"Current implementation sends: qop=auth (unquoted)\")\n\\ No newline at end of file\ndiff --git a/test_qop_with_spaces.py b/test_qop_with_spaces.py\nnew file mode 100644\nindex 0000000..4c57d98\n--- /dev/null\n+++ b/test_qop_with_spaces.py\n@@ -0,0 +1,25 @@\n+#!/usr/bin/env python\n+import requests\n+from requests.auth import HTTPDigestAuth\n+\n+# Test with qop that might have spaces (though RFC says it shouldn't)\n+auth = HTTPDigestAuth('user', 'pass')\n+auth.chal = {\n+    'realm': 'testrealm@host.com',\n+    'nonce': 'dcd98b7102dd2f0e8b11d0f600bfb0c093',\n+    'qop': 'auth, auth-int',  # Note: space after comma\n+    'opaque': '5ccc069c403ebaf9f0171e9517f40e41'\n+}\n+\n+header = auth.build_digest_header('GET', 'http://example.com/')\n+print('Generated header:')\n+print(header)\n+print()\n+print('Checking for qop=\\\\\\\"auth\\\\\\\":', 'qop=\\\"auth\\\"' in header)\n+\n+# Also test with just 'auth' but with spaces\n+auth.chal['qop'] = 'auth '\n+header2 = auth.build_digest_header('GET', 'http://example.com/')\n+print('\\nWith qop=\\\"auth \\\":')\n+print(header2)\n+print('Checking for qop=\\\\\\\"auth\\\\\\\":', 'qop=\\\"auth\\\"' in header2)\n\\ No newline at end of file\ndiff --git a/test_rfc_compliance.py b/test_rfc_compliance.py\nnew file mode 100644\nindex 0000000..78bd7fd\n--- /dev/null\n+++ b/test_rfc_compliance.py\n@@ -0,0 +1,101 @@\n+#!/usr/bin/env python\n+\"\"\"\n+Test RFC 2617 compliance for Digest Auth qop quoting.\n+\n+According to RFC 2617 section 3.2.2:\n+  qop\n+    This directive is optional, but if present it affects the\n+    computation of the request-digest.  It is a quoted string\n+    containing one of the values the server sent in the qop-options\n+    of the WWW-Authenticate response header.\n+    \n+The server sends qop-options as a quoted string (e.g., qop=\"auth,auth-int\")\n+and the client must send qop as a quoted string (e.g., qop=\"auth\").\n+\"\"\"\n+import re\n+from requests.auth import HTTPDigestAuth\n+\n+def parse_digest_header(header):\n+    \"\"\"Parse a Digest header into a dictionary.\"\"\"\n+    if not header.startswith('Digest '):\n+        return {}\n+    \n+    # Remove 'Digest ' prefix\n+    header = header[7:]\n+    \n+    # Parse key=value pairs\n+    pattern = r'(\\w+)=(\"([^\"]+)\"|([^\",]+))'\n+    matches = re.findall(pattern, header)\n+    \n+    result = {}\n+    for match in matches:\n+        key = match[0]\n+        # Use quoted value if available, otherwise unquoted\n+        value = match[2] if match[2] else match[3]\n+        result[key] = value\n+    \n+    return result\n+\n+def test_rfc_compliance():\n+    print(\"=== RFC 2617 Compliance Test ===\")\n+    print()\n+    \n+    # Test 1: Server sends qop=\"auth\"\n+    print(\"Test 1: Server sends qop=\\\"auth\\\"\")\n+    auth = HTTPDigestAuth('user', 'pass')\n+    auth.chal = {\n+        'realm': 'test',\n+        'nonce': '123456',\n+        'qop': 'auth',\n+        'opaque': 'opaque123'\n+    }\n+    \n+    header = auth.build_digest_header('GET', 'http://example.com/')\n+    parsed = parse_digest_header(header)\n+    \n+    print(f\"  Generated header: {header}\")\n+    print(f\"  Parsed qop value: {parsed.get('qop', 'NOT FOUND')}\")\n+    is_quoted = 'qop=\"auth\"' in header\n+    print(f\"  Is qop quoted in header? {is_quoted}\")\n+    assert parsed.get('qop') == 'auth', f\"Expected qop='auth', got {parsed.get('qop')}\"\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted\"\n+    print(\"  \u2713 PASS\")\n+    print()\n+    \n+    # Test 2: Server sends qop=\"auth,auth-int\"\n+    print('Test 2: Server sends qop=\"auth,auth-int\"')\n+    auth.chal['qop'] = 'auth,auth-int'\n+    header = auth.build_digest_header('GET', 'http://example.com/')\n+    parsed = parse_digest_header(header)\n+    \n+    print(f\"  Generated header: {header}\")\n+    print(f\"  Parsed qop value: {parsed.get('qop', 'NOT FOUND')}\")\n+    is_quoted = 'qop=\"auth\"' in header\n+    print(f\"  Is qop quoted in header? {is_quoted}\")\n+    assert parsed.get('qop') == 'auth', f\"Expected qop='auth', got {parsed.get('qop')}\"\n+    assert 'qop=\"auth\"' in header, \"qop should be quoted\"\n+    print(\"  \u2713 PASS\")\n+    print()\n+    \n+    # Test 3: Compare with other quoted values\n+    print(\"Test 3: All string values should be quoted\")\n+    auth.chal['algorithm'] = 'MD5'\n+    header = auth.build_digest_header('GET', 'http://example.com/')\n+    \n+    # Check that all string parameters are quoted\n+    string_params = ['username', 'realm', 'nonce', 'uri', 'response', 'opaque', 'algorithm', 'qop', 'cnonce']\n+    for param in string_params:\n+        if param in header:\n+            # Check if parameter is followed by =\"value\"\n+            pattern = f'{param}=\"[^\"]+\"'\n+            if not re.search(pattern, header):\n+                print(f\"  \u2717 {param} is not properly quoted\")\n+            else:\n+                print(f\"  \u2713 {param} is properly quoted\")\n+    \n+    print()\n+    print(\"=== All tests passed! ===\")\n+    print(\"The implementation is now RFC 2617 compliant for qop quoting.\")\n+\n+if __name__ == '__main__':\n+    test_rfc_compliance()\n\\ No newline at end of file\n"
    },
    "psf__requests-1921": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-1921",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 0000000..b46be1d\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,49 @@\n+#!/usr/bin/env python\n+\"\"\"Reproduce the issue where setting a session header to None sends 'None' as value.\"\"\"\n+\n+import requests\n+\n+# Create a session\n+session = requests.Session()\n+\n+# Print initial headers\n+print(\"Initial session headers:\")\n+print(session.headers)\n+print()\n+\n+# Set Accept-Encoding to None (should remove it, but currently sends 'None')\n+session.headers['Accept-Encoding'] = None\n+\n+print(\"After setting Accept-Encoding to None:\")\n+print(\"session.headers['Accept-Encoding'] =\", session.headers.get('Accept-Encoding'))\n+print()\n+\n+# Let's see what happens when we make a request\n+# We'll use a mock or print what would be sent\n+print(\"Creating a request to see what headers would be sent...\")\n+\n+# Create a request\n+req = requests.Request('GET', 'http://httpbin.org/get')\n+prepared = session.prepare_request(req)\n+\n+print(\"\\nHeaders that would be sent in request:\")\n+for key, value in prepared.headers.items():\n+    print(f\"{key}: {value}\")\n+\n+# Check if 'None' is being sent as string\n+if 'Accept-Encoding' in prepared.headers:\n+    value = prepared.headers['Accept-Encoding']\n+    print(f\"\\nAccept-Encoding header value: '{value}'\")\n+    print(f\"Is it the string 'None'? {value == 'None'}\")\n+    print(f\"Is it None? {value is None}\")\n+\n+# Also test the workaround mentioned in the PR\n+print(\"\\n\\n--- Testing workaround (del session.headers['Accept-Encoding']) ---\")\n+session2 = requests.Session()\n+del session2.headers['Accept-Encoding']\n+req2 = requests.Request('GET', 'http://httpbin.org/get')\n+prepared2 = session2.prepare_request(req2)\n+print(\"Headers after deleting Accept-Encoding:\")\n+for key, value in prepared2.headers.items():\n+    print(f\"{key}: {value}\")\n+print(f\"Is Accept-Encoding in headers? {'Accept-Encoding' in prepared2.headers}\")\n\\ No newline at end of file\n"
    },
    "psf__requests-2317": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-2317",
        "model_patch": ""
    },
    "psf__requests-2931": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-2931",
        "model_patch": "diff --git a/debug_flow.py b/debug_flow.py\nnew file mode 100644\nindex 0000000..89b5e49\n--- /dev/null\n+++ b/debug_flow.py\n@@ -0,0 +1,59 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+import sys\n+\n+# Monkey patch to_native_string to see what's happening\n+import requests.utils\n+original_to_native_string = requests.utils.to_native_string\n+\n+def debug_to_native_string(string, encoding='ascii'):\n+    print(f\"DEBUG to_native_string called with:\")\n+    print(f\"  Type: {type(string)}\")\n+    print(f\"  Value: {repr(string)}\")\n+    print(f\"  Encoding: {encoding}\")\n+    try:\n+        result = original_to_native_string(string, encoding)\n+        print(f\"  Result: {repr(result)}\")\n+        return result\n+    except Exception as e:\n+        print(f\"  ERROR: {type(e).__name__}: {e}\")\n+        raise\n+\n+requests.utils.to_native_string = debug_to_native_string\n+\n+# Also patch _encode_params to see when it's called\n+import requests.models\n+original_encode_params = requests.models.RequestEncodingMixin._encode_params\n+\n+@staticmethod\n+def debug_encode_params(data):\n+    print(f\"\\nDEBUG _encode_params called with:\")\n+    print(f\"  Type: {type(data)}\")\n+    print(f\"  Value: {repr(data)}\")\n+    try:\n+        result = original_encode_params(data)\n+        print(f\"  Result: {repr(result)}\")\n+        return result\n+    except Exception as e:\n+        print(f\"  ERROR: {type(e).__name__}: {e}\")\n+        raise\n+\n+requests.models.RequestEncodingMixin._encode_params = debug_encode_params\n+\n+print(\"Test 1: Binary data in request body\")\n+print(\"=\" * 50)\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 2: Params as bytes\")\n+print(\"=\" * 50)\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 0000000..2ef27dc\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,34 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test case from PR description\n+try:\n+    print(\"Testing: requests.put with binary data containing non-ASCII characters\")\n+    # This should fail with requests 2.9\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success! No error occurred.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+\n+# Additional test cases\n+print(\"\\nTesting with ASCII-only binary data:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=b\"hello world\")\n+    print(\"Success! No error occurred.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTesting with string data:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"hello world\")\n+    print(\"Success! No error occurred.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTesting with unicode string:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\")\n+    print(\"Success! No error occurred.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/requests/models.py b/requests/models.py\nindex 9c624d3..1f1f1cf 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -80,8 +80,17 @@ class RequestEncodingMixin(object):\n         if parameters are supplied as a dict.\n         \"\"\"\n \n-        if isinstance(data, (str, bytes)):\n+        if isinstance(data, str):\n             return to_native_string(data)\n+        elif isinstance(data, bytes):\n+            # Try to decode bytes as UTF-8 first, then Latin-1 as fallback\n+            # Latin-1 can decode any byte, so this will never fail\n+            try:\n+                return data.decode('utf-8')\n+            except UnicodeDecodeError:\n+                # If UTF-8 fails, try Latin-1\n+                # Latin-1 can decode any byte, so this will always succeed\n+                return data.decode('latin-1')\n         elif hasattr(data, 'read'):\n             return data\n         elif hasattr(data, '__iter__'):\ndiff --git a/test_basic_auth.py b/test_basic_auth.py\nnew file mode 100644\nindex 0000000..d8bcf58\n--- /dev/null\n+++ b/test_basic_auth.py\n@@ -0,0 +1,28 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+from requests.auth import HTTPBasicAuth\n+\n+# Test basic auth with ASCII credentials\n+print(\"Test 1: Basic auth with ASCII credentials\")\n+try:\n+    response = requests.get(\"http://httpbin.org/basic-auth/user/passwd\", auth=HTTPBasicAuth('user', 'passwd'))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test basic auth with non-ASCII credentials\n+print(\"\\nTest 2: Basic auth with non-ASCII credentials\")\n+try:\n+    response = requests.get(\"http://httpbin.org/basic-auth/user/passwd\", auth=HTTPBasicAuth('user', 'p\u00e4sswd'))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test basic auth with non-ASCII username\n+print(\"\\nTest 3: Basic auth with non-ASCII username\")\n+try:\n+    response = requests.get(\"http://httpbin.org/basic-auth/user/passwd\", auth=HTTPBasicAuth('\u00fcs\u00e9r', 'passwd'))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_binary_simple.py b/test_binary_simple.py\nnew file mode 100644\nindex 0000000..2339879\n--- /dev/null\n+++ b/test_binary_simple.py\n@@ -0,0 +1,44 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+import io\n+\n+# Test with binary data\n+print(\"Test with binary data:\")\n+binary_data = bytes([0x00, 0x01, 0x02, 0x80, 0x81, 0x82, 0xff])\n+print(f\"Original data (hex): {binary_data.hex()}\")\n+\n+# Create a simple test server\n+from http.server import HTTPServer, BaseHTTPRequestHandler\n+import threading\n+\n+class TestHandler(BaseHTTPRequestHandler):\n+    def do_PUT(self):\n+        content_length = int(self.headers['Content-Length'])\n+        body = self.rfile.read(content_length)\n+        self.send_response(200)\n+        self.send_header('Content-Type', 'text/plain')\n+        self.end_headers()\n+        self.wfile.write(f\"Received {len(body)} bytes: {body.hex()}\".encode())\n+    \n+    def log_message(self, format, *args):\n+        pass  # Suppress log messages\n+\n+server = HTTPServer(('localhost', 8888), TestHandler)\n+thread = threading.Thread(target=server.serve_forever)\n+thread.daemon = True\n+thread.start()\n+\n+try:\n+    response = requests.put(\"http://localhost:8888/\", data=binary_data)\n+    print(f\"Response: {response.text}\")\n+    \n+    # Check what was received\n+    if \"Received 7 bytes: 000102808182ff\" in response.text:\n+        print(\"Binary data preserved correctly!\")\n+    else:\n+        print(f\"Data changed! Response: {response.text}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+finally:\n+    server.shutdown()\n\\ No newline at end of file\ndiff --git a/test_comprehensive.py b/test_comprehensive.py\nnew file mode 100644\nindex 0000000..1c61ecf\n--- /dev/null\n+++ b/test_comprehensive.py\n@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+test_cases = [\n+    # (description, callable, should_pass)\n+    (\"Original issue: UTF-8 bytes as data\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\")), True),\n+    \n+    (\"ASCII bytes as data\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=b\"hello\"), True),\n+     \n+    (\"String as data\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=\"hello\"), True),\n+     \n+    (\"Unicode string as data\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\"), True),\n+     \n+    (\"Params as ASCII bytes\", \n+     lambda: requests.get(\"http://httpbin.org/get\", params=b\"test=foo\"), True),\n+     \n+    (\"Params as UTF-8 bytes\", \n+     lambda: requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\"), True),\n+     \n+    (\"Params as Latin-1 bytes\", \n+     lambda: requests.get(\"http://httpbin.org/get\", params=\"caf\u00e9=caf\u00e9\".encode(\"latin-1\")), True),\n+     \n+    (\"Params as string\", \n+     lambda: requests.get(\"http://httpbin.org/get\", params=\"test=foo\"), True),\n+     \n+    (\"Form data as dict\", \n+     lambda: requests.post(\"http://httpbin.org/post\", data={\"key\": \"value\"}), True),\n+     \n+    (\"Form data as list of tuples\", \n+     lambda: requests.post(\"http://httpbin.org/post\", data=[(\"key\", \"value\")]), True),\n+     \n+    (\"Form data as bytes\", \n+     lambda: requests.post(\"http://httpbin.org/post\", data=b\"key=value&foo=bar\"), True),\n+     \n+    (\"Form data as string\", \n+     lambda: requests.post(\"http://httpbin.org/post\", data=\"key=value&foo=bar\"), True),\n+     \n+    (\"Binary data (non-UTF-8)\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=bytes([0x80, 0x81, 0x82])), True),\n+     \n+    (\"File-like object\", \n+     lambda: requests.put(\"http://httpbin.org/put\", data=open('/dev/null', 'rb')), True),\n+]\n+\n+all_passed = True\n+for description, test_func, should_pass in test_cases:\n+    print(f\"\\n{description}:\")\n+    try:\n+        response = test_func()\n+        if hasattr(response, 'status_code'):\n+            print(f\"  Success! Status: {response.status_code}\")\n+        else:\n+            print(f\"  Success!\")\n+        if not should_pass:\n+            print(f\"  ERROR: Should have failed but didn't!\")\n+            all_passed = False\n+    except Exception as e:\n+        if should_pass:\n+            print(f\"  ERROR: {type(e).__name__}: {e}\")\n+            all_passed = False\n+        else:\n+            print(f\"  Expected failure: {type(e).__name__}: {e}\")\n+\n+if all_passed:\n+    print(\"\\nAll tests passed!\")\n+else:\n+    print(\"\\nSome tests failed!\")\n+    exit(1)\n\\ No newline at end of file\ndiff --git a/test_detailed.py b/test_detailed.py\nnew file mode 100644\nindex 0000000..659a136\n--- /dev/null\n+++ b/test_detailed.py\n@@ -0,0 +1,54 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test 1: Binary data in request body (should work)\n+print(\"Test 1: Binary data in request body\")\n+try:\n+    # This is what the PR description shows failing\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test 2: String data in request body (should work)\n+print(\"\\nTest 2: String data in request body\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"hello world\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test 3: Unicode string in request body (should work)\n+print(\"\\nTest 3: Unicode string in request body\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test 4: ASCII-only binary data in request body (should work)\n+print(\"\\nTest 4: ASCII-only binary data in request body\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=b\"hello world\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test 5: Params as bytes (should work - issue #2844)\n+print(\"\\nTest 5: Params as bytes\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test 6: Params as string (should work)\n+print(\"\\nTest 6: Params as string\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=\"test=foo\")\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_edge_cases.py b/test_edge_cases.py\nnew file mode 100644\nindex 0000000..5f36ec2\n--- /dev/null\n+++ b/test_edge_cases.py\n@@ -0,0 +1,53 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test edge cases\n+print(\"Test 1: UTF-8 encoded bytes as data\")\n+try:\n+    # These are UTF-8 encoded bytes that represent text\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"Hello world\".encode(\"utf-8\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTest 2: Non-UTF-8 bytes as data\")\n+try:\n+    # These bytes are not valid UTF-8\n+    response = requests.put(\"http://httpbin.org/put\", data=bytes([0x80, 0x81, 0x82]))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTest 3: Latin-1 encoded bytes as data\")\n+try:\n+    # Latin-1 encoded bytes\n+    response = requests.put(\"http://httpbin.org/put\", data=\"caf\u00e9\".encode(\"latin-1\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTest 4: Params as Latin-1 encoded bytes\")\n+try:\n+    # Latin-1 encoded bytes as params\n+    response = requests.get(\"http://httpbin.org/get\", params=\"caf\u00e9=caf\u00e9\".encode(\"latin-1\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTest 5: Headers with non-ASCII characters\")\n+try:\n+    # Headers should be ASCII, but let's test\n+    response = requests.get(\"http://httpbin.org/get\", headers={\"X-Test\": \"caf\u00e9\"})\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTest 6: Method with non-ASCII characters (should fail)\")\n+try:\n+    # HTTP method should be ASCII\n+    response = requests.request(\"G\u00c9T\", \"http://httpbin.org/get\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_final.py b/test_final.py\nnew file mode 100644\nindex 0000000..6a14ea3\n--- /dev/null\n+++ b/test_final.py\n@@ -0,0 +1,61 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test the exact example from the PR description\n+print(\"Test from PR description:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+    # Check that the data was sent correctly\n+    data = response.json()\n+    print(f\"Data sent: {data.get('data', '')}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with other encodings\n+print(\"\\nTest with Latin-1 encoded bytes:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"caf\u00e9\".encode(\"latin-1\"))\n+    print(f\"Success! Status: {response.status_code}\")\n+    data = response.json()\n+    print(f\"Data sent: {data.get('data', '')}\")\n+    print(f\"Data sent (hex): {data.get('data', '').encode('latin-1').hex()}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with binary data\n+print(\"\\nTest with binary data:\")\n+try:\n+    binary_data = bytes([0x00, 0x01, 0x02, 0x80, 0x81, 0x82, 0xff])\n+    response = requests.put(\"http://httpbin.org/put\", data=binary_data)\n+    print(f\"Success! Status: {response.status_code}\")\n+    data = response.json()\n+    print(f\"Data sent (hex): {data.get('data', '').encode('latin-1').hex()}\")\n+    print(f\"Original data (hex): {binary_data.hex()}\")\n+    # Check that the data was sent correctly\n+    sent_data = data.get('data', '').encode('latin-1')\n+    if sent_data == binary_data:\n+        print(\"Data preserved correctly!\")\n+    else:\n+        print(f\"Data changed! Original: {binary_data.hex()}, Sent: {sent_data.hex()}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test params as bytes\n+print(\"\\nTest params as bytes:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test params as UTF-8 bytes\n+print(\"\\nTest params as UTF-8 bytes:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! Status: {response.status_code}\")\n+    print(f\"URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_fix.py b/test_fix.py\nnew file mode 100644\nindex 0000000..7e3c7c1\n--- /dev/null\n+++ b/test_fix.py\n@@ -0,0 +1,56 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+import sys\n+\n+# Test the current behavior\n+print(\"Testing current behavior:\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Now let's test what happens if we change to_native_string to use 'utf-8' instead of 'ascii'\n+print(\"\\n\\nTesting with modified to_native_string:\")\n+import requests.utils\n+\n+original_to_native_string = requests.utils.to_native_string\n+\n+def patched_to_native_string(string, encoding='utf-8'):\n+    \"\"\"Patched version that uses utf-8 by default instead of ascii\"\"\"\n+    out = None\n+\n+    if isinstance(string, requests.compat.builtin_str):\n+        out = string\n+    else:\n+        if requests.compat.is_py2:\n+            out = string.encode(encoding)\n+        else:\n+            out = string.decode(encoding)\n+\n+    return out\n+\n+requests.utils.to_native_string = patched_to_native_string\n+\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test params as bytes\n+print(\"\\n\\nTesting params as bytes with patched version:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test params as bytes with non-ASCII\n+print(\"\\n\\nTesting params as bytes with non-ASCII:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_fix2.py b/test_fix2.py\nnew file mode 100644\nindex 0000000..3f468d0\n--- /dev/null\n+++ b/test_fix2.py\n@@ -0,0 +1,48 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests.utils\n+import requests.models\n+\n+# Monkey patch to_native_string in the module where it's used\n+original_to_native_string = requests.utils.to_native_string\n+\n+def patched_to_native_string(string, encoding='utf-8'):\n+    \"\"\"Patched version that uses utf-8 by default instead of ascii\"\"\"\n+    out = None\n+\n+    if isinstance(string, requests.compat.builtin_str):\n+        out = string\n+    else:\n+        if requests.compat.is_py2:\n+            out = string.encode(encoding)\n+        else:\n+            out = string.decode(encoding)\n+\n+    return out\n+\n+# Patch it in both places\n+requests.utils.to_native_string = patched_to_native_string\n+requests.models.to_native_string = patched_to_native_string\n+\n+# Now test\n+print(\"Testing with patched to_native_string (utf-8 default):\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTesting params as bytes with non-ASCII:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with ASCII params\n+print(\"\\n\\nTesting params as bytes with ASCII:\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_form_data.py b/test_form_data.py\nnew file mode 100644\nindex 0000000..2d5482f\n--- /dev/null\n+++ b/test_form_data.py\n@@ -0,0 +1,35 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test with form data (dict)\n+print(\"Test 1: Form data as dict\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data={\"key\": \"value\", \"test\": u\"\u00f6\u00f6\u00f6\"})\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with form data (list of tuples)\n+print(\"\\nTest 2: Form data as list of tuples\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data=[(\"key\", \"value\"), (\"test\", u\"\u00f6\u00f6\u00f6\")])\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with form data (bytes) - this should fail\n+print(\"\\nTest 3: Form data as bytes\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data=b\"key=value&test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with form data (string)\n+print(\"\\nTest 4: Form data as string\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data=\"key=value&test=\u00f6\u00f6\u00f6\")\n+    print(f\"Success! Status: {response.status_code}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_latin1.py b/test_latin1.py\nnew file mode 100644\nindex 0000000..df01a11\n--- /dev/null\n+++ b/test_latin1.py\n@@ -0,0 +1,24 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test Latin-1 encoded bytes\n+print(\"Test Latin-1 encoded bytes as params:\")\n+latin1_bytes = \"caf\u00e9=caf\u00e9\".encode(\"latin-1\")\n+print(f\"Latin-1 bytes: {latin1_bytes!r}\")\n+print(f\"Decoded as Latin-1: {latin1_bytes.decode('latin-1')!r}\")\n+print(f\"Decoded as UTF-8 (should fail): \", end=\"\")\n+try:\n+    print(latin1_bytes.decode('utf-8'))\n+except UnicodeDecodeError as e:\n+    print(f\"Error: {e}\")\n+\n+response = requests.get(\"http://httpbin.org/get\", params=latin1_bytes)\n+print(f\"URL: {response.url}\")\n+print(f\"Expected: caf\u00e9=caf\u00e9 encoded in the URL\")\n+print(f\"Actual: {response.url}\")\n+\n+# Check what was sent\n+if response.status_code == 200:\n+    data = response.json()\n+    print(f\"Args received: {data.get('args', {})}\")\n\\ No newline at end of file\ndiff --git a/test_no_decode.py b/test_no_decode.py\nnew file mode 100644\nindex 0000000..51f3e8d\n--- /dev/null\n+++ b/test_no_decode.py\n@@ -0,0 +1,69 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests.utils\n+import requests.models\n+\n+# Monkey patch _encode_params to return bytes as-is\n+original_encode_params = requests.models.RequestEncodingMixin._encode_params\n+\n+@staticmethod\n+def patched_encode_params(data):\n+    \"\"\"Patched version that returns bytes as-is\"\"\"\n+    if isinstance(data, bytes):\n+        # Return bytes as-is\n+        return data\n+    elif isinstance(data, str):\n+        return requests.utils.to_native_string(data)\n+    elif hasattr(data, 'read'):\n+        return data\n+    elif hasattr(data, '__iter__'):\n+        result = []\n+        for k, vs in requests.utils.to_key_val_list(data):\n+            if isinstance(vs, requests.compat.basestring) or not hasattr(vs, '__iter__'):\n+                vs = [vs]\n+            for v in vs:\n+                if v is not None:\n+                    result.append(\n+                        (k.encode('utf-8') if isinstance(k, str) else k,\n+                         v.encode('utf-8') if isinstance(v, str) else v))\n+        return requests.compat.urlencode(result, doseq=True)\n+    else:\n+        return data\n+\n+requests.models.RequestEncodingMixin._encode_params = patched_encode_params\n+\n+# Now test\n+print(\"Test 1: Binary data in request body\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 2: Params as bytes\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 3: Params as bytes with non-ASCII\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 4: Form data as dict\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data={\"key\": \"value\"})\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 5: String data\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"hello\")\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_params_bytes.py b/test_params_bytes.py\nnew file mode 100644\nindex 0000000..7f06ce6\n--- /dev/null\n+++ b/test_params_bytes.py\n@@ -0,0 +1,33 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests\n+\n+# Test case from issue #2844\n+print(\"Testing params as bytes:\")\n+try:\n+    request = requests.Request('GET', 'http://example.com', params=b'test=foo').prepare()\n+    print(f\"URL: {request.url}\")\n+    assert request.url == 'http://example.com/?test=foo'\n+    print(\"Success! Params bytes are encoded correctly.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+\n+# Test with non-ASCII bytes in params\n+print(\"\\nTesting params as bytes with non-ASCII characters:\")\n+try:\n+    # This should probably fail or be encoded properly\n+    request = requests.Request('GET', 'http://example.com', params=b'test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6').prepare()\n+    print(f\"URL: {request.url}\")\n+    print(\"Success! No error occurred.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n+\n+# Test with string params\n+print(\"\\nTesting params as string:\")\n+try:\n+    request = requests.Request('GET', 'http://example.com', params='test=foo').prepare()\n+    print(f\"URL: {request.url}\")\n+    assert request.url == 'http://example.com/?test=foo'\n+    print(\"Success! String params work.\")\n+except Exception as e:\n+    print(f\"Error occurred: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_to_native_string.py b/test_to_native_string.py\nnew file mode 100644\nindex 0000000..4a89c31\n--- /dev/null\n+++ b/test_to_native_string.py\n@@ -0,0 +1,26 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import sys\n+sys.path.insert(0, '/testbed')\n+\n+from requests.utils import to_native_string\n+from requests.compat import is_py2, builtin_str\n+\n+print(f\"Python 2: {is_py2}\")\n+print(f\"builtin_str: {builtin_str}\")\n+\n+test_cases = [\n+    (\"ASCII string\", \"hello\"),\n+    (\"Unicode string\", u\"hello\"),\n+    (\"ASCII bytes\", b\"hello\"),\n+    (\"Non-ASCII bytes\", u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\")),\n+    (\"Non-ASCII string\", u\"\u00f6\u00f6\u00f6\"),\n+]\n+\n+for name, value in test_cases:\n+    print(f\"\\nTesting {name}: {repr(value)} (type: {type(value)})\")\n+    try:\n+        result = to_native_string(value)\n+        print(f\"  Result: {repr(result)} (type: {type(result)})\")\n+    except Exception as e:\n+        print(f\"  ERROR: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_try_decode.py b/test_try_decode.py\nnew file mode 100644\nindex 0000000..153bbf8\n--- /dev/null\n+++ b/test_try_decode.py\n@@ -0,0 +1,86 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests.models\n+\n+# Monkey patch _encode_params to try decoding bytes as UTF-8\n+original_encode_params = requests.models.RequestEncodingMixin._encode_params\n+\n+@staticmethod\n+def patched_encode_params(data):\n+    \"\"\"Encode parameters in a piece of data.\n+\n+    Will successfully encode parameters when passed as a dict or a list of\n+    2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n+    if parameters are supplied as a dict.\n+    \"\"\"\n+    if isinstance(data, bytes):\n+        # Try to decode as UTF-8, but if it fails, return bytes as-is\n+        try:\n+            return data.decode('utf-8')\n+        except UnicodeDecodeError:\n+            # Probably binary data, return as-is\n+            return data\n+    elif isinstance(data, str):\n+        return requests.models.to_native_string(data)\n+    elif hasattr(data, 'read'):\n+        return data\n+    elif hasattr(data, '__iter__'):\n+        result = []\n+        for k, vs in requests.models.to_key_val_list(data):\n+            if isinstance(vs, requests.compat.basestring) or not hasattr(vs, '__iter__'):\n+                vs = [vs]\n+            for v in vs:\n+                if v is not None:\n+                    result.append(\n+                        (k.encode('utf-8') if isinstance(k, str) else k,\n+                         v.encode('utf-8') if isinstance(v, str) else v))\n+        return requests.compat.urlencode(result, doseq=True)\n+    else:\n+        return data\n+\n+requests.models.RequestEncodingMixin._encode_params = patched_encode_params\n+\n+# Now test\n+print(\"Test 1: Binary data in request body (UTF-8 encoded)\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 2: Binary data in request body (non-UTF-8)\")\n+try:\n+    # Create some random binary data that's not valid UTF-8\n+    binary_data = bytes([0x80, 0x81, 0x82, 0x83])\n+    response = requests.put(\"http://httpbin.org/put\", data=binary_data)\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 3: Params as bytes\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 4: Params as bytes with non-ASCII\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 5: String data\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"hello\")\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 6: Form data as dict\")\n+try:\n+    response = requests.post(\"http://httpbin.org/post\", data={\"key\": \"value\", \"test\": u\"\u00f6\u00f6\u00f6\"})\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_utf8_fix.py b/test_utf8_fix.py\nnew file mode 100644\nindex 0000000..4066306\n--- /dev/null\n+++ b/test_utf8_fix.py\n@@ -0,0 +1,78 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import requests.models\n+\n+# Monkey patch _encode_params to use utf-8 encoding\n+original_encode_params = requests.models.RequestEncodingMixin._encode_params\n+\n+@staticmethod\n+def patched_encode_params(data):\n+    \"\"\"Encode parameters in a piece of data.\n+\n+    Will successfully encode parameters when passed as a dict or a list of\n+    2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n+    if parameters are supplied as a dict.\n+    \"\"\"\n+\n+    if isinstance(data, (str, bytes)):\n+        return requests.models.to_native_string(data, 'utf-8')\n+    elif hasattr(data, 'read'):\n+        return data\n+    elif hasattr(data, '__iter__'):\n+        result = []\n+        for k, vs in requests.models.to_key_val_list(data):\n+            if isinstance(vs, requests.compat.basestring) or not hasattr(vs, '__iter__'):\n+                vs = [vs]\n+            for v in vs:\n+                if v is not None:\n+                    result.append(\n+                        (k.encode('utf-8') if isinstance(k, str) else k,\n+                         v.encode('utf-8') if isinstance(v, str) else v))\n+        return requests.compat.urlencode(result, doseq=True)\n+    else:\n+        return data\n+\n+requests.models.RequestEncodingMixin._encode_params = patched_encode_params\n+\n+# Now test\n+print(\"Test 1: Binary data in request body\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\".encode(\"utf-8\"))\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 2: Params as bytes\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=foo\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 3: Params as bytes with non-ASCII\")\n+try:\n+    response = requests.get(\"http://httpbin.org/get\", params=b\"test=\\xc3\\xb6\\xc3\\xb6\\xc3\\xb6\")\n+    print(f\"Success! URL: {response.url}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 4: String data\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=\"hello\")\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 5: Unicode string\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=u\"\u00f6\u00f6\u00f6\")\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTest 6: ASCII-only binary data\")\n+try:\n+    response = requests.put(\"http://httpbin.org/put\", data=b\"hello\")\n+    print(\"Success!\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\n"
    },
    "psf__requests-5414": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-5414",
        "model_patch": "diff --git a/debug_flow.py b/debug_flow.py\nnew file mode 100644\nindex 00000000..1b597ccb\n--- /dev/null\n+++ b/debug_flow.py\n@@ -0,0 +1,45 @@\n+import requests\n+from requests.models import PreparedRequest\n+\n+# Create a PreparedRequest and manually call prepare_url\n+req = PreparedRequest()\n+\n+print(\"Testing prepare_url with http://.example.com...\")\n+try:\n+    req.prepare_url(\"http://.example.com\", params=None)\n+    print(f\"prepare_url succeeded, url is: {req.url}\")\n+except Exception as e:\n+    print(f\"Exception in prepare_url: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTesting prepare_url with http://*.example.com...\")\n+try:\n+    req = PreparedRequest()\n+    req.prepare_url(\"http://*.example.com\", params=None)\n+    print(f\"prepare_url succeeded, url is: {req.url}\")\n+except Exception as e:\n+    print(f\"Exception in prepare_url: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTesting prepare_url with http://example.com...\")\n+try:\n+    req = PreparedRequest()\n+    req.prepare_url(\"http://example.com\", params=None)\n+    print(f\"prepare_url succeeded, url is: {req.url}\")\n+except Exception as e:\n+    print(f\"Exception in prepare_url: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTesting IDNA encoding directly...\")\n+from requests.models import PreparedRequest\n+try:\n+    # Test _get_idna_encoded_host directly\n+    host = \".example.com\"\n+    print(f\"Testing _get_idna_encoded_host with host: '{host}'\")\n+    encoded = PreparedRequest._get_idna_encoded_host(host)\n+    print(f\"Encoded host: {encoded}\")\n+except Exception as e:\n+    print(f\"Exception in _get_idna_encoded_host: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nTesting if host starts with dot...\")\n+host = \".example.com\"\n+print(f\"host: '{host}'\")\n+print(f\"host.startswith('*'): {host.startswith('*')}\")\n+print(f\"host.startswith('.'): {host.startswith('.')}\")\n\\ No newline at end of file\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..65cc3f7f\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,25 @@\n+import requests\n+\n+print(\"Testing http://.example.com...\")\n+try:\n+    response = requests.get(\"http://.example.com\")\n+    print(\"No exception raised\")\n+except Exception as e:\n+    print(f\"Exception type: {type(e).__name__}\")\n+    print(f\"Exception message: {e}\")\n+\n+print(\"\\nTesting http://*.example.com...\")\n+try:\n+    response = requests.get(\"http://*.example.com\")\n+    print(\"No exception raised\")\n+except Exception as e:\n+    print(f\"Exception type: {type(e).__name__}\")\n+    print(f\"Exception message: {e}\")\n+\n+print(\"\\nTesting http://example.com...\")\n+try:\n+    response = requests.get(\"http://example.com\")\n+    print(\"No exception raised\")\n+except Exception as e:\n+    print(f\"Exception type: {type(e).__name__}\")\n+    print(f\"Exception message: {e}\")\n\\ No newline at end of file\ndiff --git a/test_idna.py b/test_idna.py\nnew file mode 100644\nindex 00000000..3d451123\n--- /dev/null\n+++ b/test_idna.py\n@@ -0,0 +1,14 @@\n+import idna\n+\n+print(\"Testing IDNA encoding directly...\")\n+hosts = [\".example.com\", \"example.com\", \"*.example.com\", \"xn--example.com\"]\n+\n+for host in hosts:\n+    print(f\"\\nTesting host: '{host}'\")\n+    try:\n+        encoded = idna.encode(host, uts46=True)\n+        print(f\"  Encoded: {encoded}\")\n+        decoded = encoded.decode('utf-8')\n+        print(f\"  Decoded: {decoded}\")\n+    except Exception as e:\n+        print(f\"  Exception: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_idna_error.py b/test_idna_error.py\nnew file mode 100644\nindex 00000000..467f981c\n--- /dev/null\n+++ b/test_idna_error.py\n@@ -0,0 +1,16 @@\n+import idna\n+\n+print(\"Testing IDNA error types...\")\n+hosts = [\".example.com\", \"*.example.com\"]\n+\n+for host in hosts:\n+    print(f\"\\nTesting host: '{host}'\")\n+    try:\n+        encoded = idna.encode(host, uts46=True)\n+        print(f\"  Encoded: {encoded}\")\n+    except idna.IDNAError as e:\n+        print(f\"  IDNAError: {type(e).__name__}: {e}\")\n+        print(f\"  Is it idna.core.InvalidCodepoint? {isinstance(e, idna.core.InvalidCodepoint)}\")\n+        print(f\"  Is it idna.core.IDNAError? {isinstance(e, idna.core.IDNAError)}\")\n+    except Exception as e:\n+        print(f\"  Other Exception: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/trace_issue.py b/trace_issue.py\nnew file mode 100644\nindex 00000000..ed9bfd10\n--- /dev/null\n+++ b/trace_issue.py\n@@ -0,0 +1,27 @@\n+import requests\n+import traceback\n+\n+print(\"Testing http://.example.com...\")\n+try:\n+    response = requests.get(\"http://.example.com\")\n+    print(\"No exception raised\")\n+except Exception as e:\n+    print(f\"Exception type: {type(e).__name__}\")\n+    print(f\"Exception message: {e}\")\n+    print(\"\\nFull traceback:\")\n+    traceback.print_exc()\n+\n+print(\"\\n\\nLet's test parse_url directly:\")\n+from urllib3.util import parse_url\n+try:\n+    result = parse_url(\"http://.example.com\")\n+    print(f\"parse_url succeeded: {result}\")\n+except Exception as e:\n+    print(f\"Exception from parse_url: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\\nLet's test with just the host part:\")\n+try:\n+    result = parse_url(\".example.com\")\n+    print(f\"parse_url succeeded: {result}\")\n+except Exception as e:\n+    print(f\"Exception from parse_url: {type(e).__name__}: {e}\")\n\\ No newline at end of file\n"
    },
    "psf__requests-6028": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "psf__requests-6028",
        "model_patch": "diff --git a/test_proxy_auth.py b/test_proxy_auth.py\nnew file mode 100644\nindex 00000000..0858b55d\n--- /dev/null\n+++ b/test_proxy_auth.py\n@@ -0,0 +1,51 @@\n+import requests\n+from requests.auth import HTTPProxyAuth\n+import sys\n+\n+print(f\"Python version: {sys.version}\")\n+print(f\"Requests version: {requests.__version__}\")\n+\n+# Test with a proxy URL that has authentication\n+proxy_url = \"http://user:pass@proxy.example.com:8080\"\n+\n+# Parse the URL using urlparse\n+from urllib.parse import urlparse\n+parsed = urlparse(proxy_url)\n+print(f\"\\nParsed proxy URL:\")\n+print(f\"  Scheme: {parsed.scheme}\")\n+print(f\"  Netloc: {parsed.netloc}\")\n+print(f\"  Username: {parsed.username}\")\n+print(f\"  Password: {parsed.password}\")\n+print(f\"  Hostname: {parsed.hostname}\")\n+print(f\"  Port: {parsed.port}\")\n+\n+# Test get_auth_from_url\n+from requests.utils import get_auth_from_url\n+username, password = get_auth_from_url(proxy_url)\n+print(f\"\\nget_auth_from_url result:\")\n+print(f\"  Username: '{username}'\")\n+print(f\"  Password: '{password}'\")\n+\n+# Test with proxy that has special characters in password\n+proxy_url2 = \"http://user:pass@word@proxy.example.com:8080\"\n+parsed2 = urlparse(proxy_url2)\n+print(f\"\\nParsed proxy URL with @ in password:\")\n+print(f\"  Scheme: {parsed2.scheme}\")\n+print(f\"  Netloc: {parsed2.netloc}\")\n+print(f\"  Username: {parsed2.username}\")\n+print(f\"  Password: {parsed2.password}\")\n+print(f\"  Hostname: {parsed2.hostname}\")\n+print(f\"  Port: {parsed2.port}\")\n+\n+username2, password2 = get_auth_from_url(proxy_url2)\n+print(f\"\\nget_auth_from_url result with @ in password:\")\n+print(f\"  Username: '{username2}'\")\n+print(f\"  Password: '{password2}'\")\n+\n+# Test with urllib3's parse_url\n+from urllib3.util import parse_url\n+parsed3 = parse_url(proxy_url2)\n+print(f\"\\nurllib3 parse_url result with @ in password:\")\n+print(f\"  Username: {parsed3.auth}\")\n+if parsed3.auth:\n+    print(f\"  Auth split: {parsed3.auth.split(':')}\")\n\\ No newline at end of file\ndiff --git a/test_python38_urlparse.py b/test_python38_urlparse.py\nnew file mode 100644\nindex 00000000..c1682586\n--- /dev/null\n+++ b/test_python38_urlparse.py\n@@ -0,0 +1,36 @@\n+import sys\n+from urllib.parse import urlparse\n+\n+# Test different proxy URL formats\n+test_urls = [\n+    \"http://user:pass@proxy.example.com:8080\",\n+    \"http://user:pass%40word@proxy.example.com:8080\",  # URL-encoded @\n+    \"http://user:pass@word@proxy.example.com:8080\",   # Unencoded @ in password\n+    \"http://user:@proxy.example.com:8080\",  # Empty password\n+    \"http://:pass@proxy.example.com:8080\",   # Empty username\n+    \"http://user:pass:w@rd@proxy.example.com:8080\",  # Multiple special chars\n+]\n+\n+print(f\"Python version: {sys.version_info}\")\n+print(f\"Python version string: {sys.version}\\n\")\n+\n+for url in test_urls:\n+    print(f\"Testing URL: {url}\")\n+    parsed = urlparse(url)\n+    print(f\"  Parsed username: {repr(parsed.username)}\")\n+    print(f\"  Parsed password: {repr(parsed.password)}\")\n+    print(f\"  Parsed netloc: {repr(parsed.netloc)}\")\n+    print()\n+\n+# Let's also test what happens with urllib3's parse_url\n+try:\n+    from urllib3.util import parse_url\n+    print(\"\\nTesting with urllib3's parse_url:\")\n+    for url in test_urls:\n+        print(f\"\\nTesting URL: {url}\")\n+        parsed = parse_url(url)\n+        print(f\"  Parsed auth: {repr(parsed.auth)}\")\n+        if parsed.auth:\n+            print(f\"  Auth parts: {parsed.auth.split(':')}\")\n+except ImportError:\n+    print(\"urllib3 not available\")\n\\ No newline at end of file\ndiff --git a/test_urlparse_issue.py b/test_urlparse_issue.py\nnew file mode 100644\nindex 00000000..9ac69300\n--- /dev/null\n+++ b/test_urlparse_issue.py\n@@ -0,0 +1,58 @@\n+import sys\n+from urllib.parse import urlparse, unquote\n+\n+# Test what Python 3.8.12 might have changed\n+# According to Python docs, in 3.8.12: \"urlparse() no longer raises an exception when parsing URLs containing characters that decompose under IDNA2008 to the same character\"\n+# But there might be other changes...\n+\n+# Let's test with various proxy URLs\n+test_cases = [\n+    # (url, expected_username, expected_password)\n+    (\"http://user:pass@proxy.com:8080\", \"user\", \"pass\"),\n+    (\"http://user:pass%40word@proxy.com:8080\", \"user\", \"pass@word\"),  # @ encoded as %40\n+    (\"http://user:pass@word@proxy.com:8080\", \"user\", \"pass@word\"),    # @ not encoded\n+    (\"http://user:pass%3Aword@proxy.com:8080\", \"user\", \"pass:word\"),  # : encoded as %3A\n+    (\"http://user:pass:word@proxy.com:8080\", \"user\", \"pass:word\"),    # : not encoded\n+    (\"http://user:pass%2Fword@proxy.com:8080\", \"user\", \"pass/word\"),  # / encoded as %2F\n+    (\"http://user:pass/word@proxy.com:8080\", \"user\", \"pass/word\"),    # / not encoded\n+]\n+\n+print(f\"Python version: {sys.version}\")\n+print()\n+\n+for url, expected_user, expected_pass in test_cases:\n+    print(f\"Testing: {url}\")\n+    parsed = urlparse(url)\n+    print(f\"  urlparse username: {repr(parsed.username)}\")\n+    print(f\"  urlparse password: {repr(parsed.password)}\")\n+    \n+    # What get_auth_from_url would return\n+    try:\n+        auth = (unquote(parsed.username) if parsed.username else '', \n+                unquote(parsed.password) if parsed.password else '')\n+    except (AttributeError, TypeError):\n+        auth = ('', '')\n+    \n+    print(f\"  get_auth_from_url: {repr(auth)}\")\n+    print(f\"  Expected: ({repr(expected_user)}, {repr(expected_pass)})\")\n+    \n+    if auth[0] != expected_user or auth[1] != expected_pass:\n+        print(f\"  MISMATCH!\")\n+    print()\n+\n+# Now let's test what happens when we construct a Proxy-Authorization header\n+from requests.auth import _basic_auth_str\n+\n+print(\"\\nTesting _basic_auth_str:\")\n+for url, expected_user, expected_pass in test_cases[:3]:\n+    parsed = urlparse(url)\n+    username = unquote(parsed.username) if parsed.username else ''\n+    password = unquote(parsed.password) if parsed.password else ''\n+    \n+    if username:\n+        auth_header = _basic_auth_str(username, password)\n+        print(f\"URL: {url}\")\n+        print(f\"  Username: {repr(username)}\")\n+        print(f\"  Password: {repr(password)}\")\n+        print(f\"  Proxy-Authorization: {auth_header}\")\n+        print()\n\\ No newline at end of file\n"
    },
    "pydata__xarray-2905": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-2905",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..22e8a0c7\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,21 @@\n+import xarray as xr\n+import numpy as np\n+\n+# Create test case from PR description\n+good_indexed, bad_indexed = xr.DataArray([None]), xr.DataArray([None])\n+\n+class HasValues(object):\n+    values = 5\n+    \n+good_indexed.loc[{'dim_0': 0}] = set()\n+bad_indexed.loc[{'dim_0': 0}] = HasValues()\n+\n+print(\"good_indexed.values:\", good_indexed.values)\n+print(\"Type of good_indexed.values[0]:\", type(good_indexed.values[0]))\n+print()\n+print(\"bad_indexed.values:\", bad_indexed.values)\n+print(\"Type of bad_indexed.values[0]:\", type(bad_indexed.values[0]))\n+print()\n+print(\"Expected: bad_indexed.values[0] should be a HasValues instance\")\n+print(\"Actual: bad_indexed.values[0] is:\", bad_indexed.values[0])\n+print(\"bad_indexed.values[0] == 5:\", bad_indexed.values[0] == 5)\n\\ No newline at end of file\n"
    },
    "pydata__xarray-3095": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-3095",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..c51da4f7\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,55 @@\n+import xarray\n+import copy as cp\n+\n+print(\"Testing Dataset.copy() with unicode indices...\")\n+ds = xarray.Dataset(\n+    coords={'x': ['foo'], 'y': ('x', ['bar'])},\n+    data_vars={'z': ('x', ['baz'])})\n+\n+print(\"\\nOriginal dataset:\")\n+print(ds)\n+print(f\"Original x dtype: {ds.x.dtype}\")\n+print(f\"Original y dtype: {ds.y.dtype}\")\n+print(f\"Original z dtype: {ds.z.dtype}\")\n+\n+print(\"\\n\\nDataset.copy(deep=False):\")\n+ds_shallow = ds.copy(deep=False)\n+print(ds_shallow)\n+print(f\"Shallow copy x dtype: {ds_shallow.x.dtype}\")\n+print(f\"Shallow copy y dtype: {ds_shallow.y.dtype}\")\n+print(f\"Shallow copy z dtype: {ds_shallow.z.dtype}\")\n+\n+print(\"\\n\\nDataset.copy(deep=True):\")\n+ds_deep = ds.copy(deep=True)\n+print(ds_deep)\n+print(f\"Deep copy x dtype: {ds_deep.x.dtype}\")\n+print(f\"Deep copy y dtype: {ds_deep.y.dtype}\")\n+print(f\"Deep copy z dtype: {ds_deep.z.dtype}\")\n+\n+print(\"\\n\\nTesting DataArray.copy()...\")\n+print(\"\\nOriginal ds.z:\")\n+print(ds.z)\n+print(f\"Original ds.z.x dtype: {ds.z.x.dtype}\")\n+\n+print(\"\\n\\nds.z.copy(deep=False):\")\n+da_shallow = ds.z.copy(deep=False)\n+print(da_shallow)\n+print(f\"Shallow copy x dtype: {da_shallow.x.dtype}\")\n+\n+print(\"\\n\\nds.z.copy(deep=True):\")\n+da_deep = ds.z.copy(deep=True)\n+print(da_deep)\n+print(f\"Deep copy x dtype: {da_deep.x.dtype}\")\n+\n+print(\"\\n\\nTesting copy.copy() and copy.deepcopy()...\")\n+print(\"\\ncopy.copy(ds):\")\n+ds_copy = cp.copy(ds)\n+print(f\"copy.copy x dtype: {ds_copy.x.dtype}\")\n+\n+print(\"\\ncopy.deepcopy(ds):\")\n+ds_deepcopy = cp.deepcopy(ds)\n+print(f\"copy.deepcopy x dtype: {ds_deepcopy.x.dtype}\")\n+\n+print(\"\\n\\nChecking if dtypes match:\")\n+print(f\"Original x dtype == Deep copy x dtype: {ds.x.dtype == ds_deep.x.dtype}\")\n+print(f\"Expected: <U3, Got: {ds_deep.x.dtype}\")\n\\ No newline at end of file\n"
    },
    "pydata__xarray-3151": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-3151",
        "model_patch": "diff --git a/xarray/core/combine.py b/xarray/core/combine.py\nindex 37ae903b..3a41c38e 100644\n--- a/xarray/core/combine.py\n+++ b/xarray/core/combine.py\n@@ -501,7 +501,8 @@ def combine_by_coords(datasets, compat='no_conflicts', data_vars='all',\n                                    fill_value=fill_value)\n \n         # Check the overall coordinates are monotonically increasing\n-        for dim in concatenated.dims:\n+        # Only check dimensions that were actually concatenated\n+        for dim in concat_dims:\n             if dim in concatenated:\n                 indexes = concatenated.indexes.get(dim)\n                 if not (indexes.is_monotonic_increasing\n"
    },
    "pydata__xarray-3305": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-3305",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..52e38356\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,20 @@\n+import xarray as xr\n+\n+# Create a DataArray with attributes\n+da = xr.DataArray([0, 0], dims=\"x\", attrs={'units':'K'})\n+\n+# Test quantile with keep_attrs=True\n+out = da.quantile(.9, dim='x', keep_attrs=True)\n+\n+print(\"Input DataArray attributes:\", da.attrs)\n+print(\"Output DataArray attributes:\", out.attrs)\n+print(\"Expected: OrderedDict([('units', 'K')])\")\n+print(\"Actual:\", out.attrs)\n+\n+# Also test without keep_attrs to see default behavior\n+out2 = da.quantile(.9, dim='x')\n+print(\"\\nWithout keep_attrs (default):\", out2.attrs)\n+\n+# Test with keep_attrs=False explicitly\n+out3 = da.quantile(.9, dim='x', keep_attrs=False)\n+print(\"With keep_attrs=False:\", out3.attrs)\n\\ No newline at end of file\ndiff --git a/test_dataset_attrs.py b/test_dataset_attrs.py\nnew file mode 100644\nindex 00000000..9427013f\n--- /dev/null\n+++ b/test_dataset_attrs.py\n@@ -0,0 +1,26 @@\n+import xarray as xr\n+import numpy as np\n+\n+# Create a dataset with variable attributes\n+ds = xr.Dataset({\n+    'temperature': xr.DataArray([0, 0], dims=\"x\", attrs={'units':'K'}),\n+    'pressure': xr.DataArray([1, 1], dims=\"x\", attrs={'units':'Pa'})\n+}, attrs={'source': 'test'})\n+\n+print(\"Dataset attributes:\", ds.attrs)\n+print(\"Temperature variable attributes:\", ds['temperature'].attrs)\n+print(\"Pressure variable attributes:\", ds['pressure'].attrs)\n+\n+# Test quantile with keep_attrs=True\n+out = ds.quantile(.9, dim='x', keep_attrs=True)\n+print(\"\\nAfter quantile with keep_attrs=True:\")\n+print(\"Dataset attributes:\", out.attrs)\n+print(\"Temperature variable attributes:\", out['temperature'].attrs)\n+print(\"Pressure variable attributes:\", out['pressure'].attrs)\n+\n+# Test quantile with keep_attrs=False\n+out2 = ds.quantile(.9, dim='x', keep_attrs=False)\n+print(\"\\nAfter quantile with keep_attrs=False:\")\n+print(\"Dataset attributes:\", out2.attrs)\n+print(\"Temperature variable attributes:\", out2['temperature'].attrs)\n+print(\"Pressure variable attributes:\", out2['pressure'].attrs)\n\\ No newline at end of file\n"
    },
    "pydata__xarray-3677": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-3677",
        "model_patch": "diff --git a/reproduce_error.py b/reproduce_error.py\nnew file mode 100644\nindex 00000000..fce37aa6\n--- /dev/null\n+++ b/reproduce_error.py\n@@ -0,0 +1,17 @@\n+import xarray as xr\n+\n+ds = xr.Dataset({'a': 0})\n+da = xr.DataArray(1, name='b')\n+\n+print(\"Testing xr.merge([ds, da]):\")\n+expected = xr.merge([ds, da])  # works fine\n+print(expected)\n+print()\n+\n+print(\"Testing ds.merge(da):\")\n+try:\n+    actual = ds.merge(da)  # fails\n+    print(\"Success!\")\n+    print(actual)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_consistency.py b/test_consistency.py\nnew file mode 100644\nindex 00000000..d2024bc5\n--- /dev/null\n+++ b/test_consistency.py\n@@ -0,0 +1,77 @@\n+import xarray as xr\n+import numpy as np\n+\n+print(\"Test consistency between xr.merge and ds.merge for DataArrays\")\n+\n+# Test 1: Simple case\n+ds = xr.Dataset({'a': 0})\n+da = xr.DataArray(1, name='b')\n+\n+result1 = xr.merge([ds, da])\n+result2 = ds.merge(da)\n+\n+print(\"Test 1 - Simple case:\")\n+print(\"xr.merge([ds, da]):\", result1)\n+print(\"ds.merge(da):\", result2)\n+print(\"Are they equal?\", result1.equals(result2))\n+print()\n+\n+# Test 2: With dimensions\n+ds2 = xr.Dataset({'a': ('x', [1, 2, 3])})\n+da2 = xr.DataArray([4, 5, 6], dims=['x'], name='b')\n+\n+result3 = xr.merge([ds2, da2])\n+result4 = ds2.merge(da2)\n+\n+print(\"Test 2 - With dimensions:\")\n+print(\"xr.merge([ds2, da2]):\", result3)\n+print(\"ds2.merge(da2):\", result4)\n+print(\"Are they equal?\", result3.equals(result4))\n+print()\n+\n+# Test 3: With coordinates\n+ds3 = xr.Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2]})\n+da3 = xr.DataArray([4, 5, 6], dims=['x'], coords={'x': [0, 1, 2], 'y': ('x', [7, 8, 9])}, name='b')\n+\n+result5 = xr.merge([ds3, da3])\n+result6 = ds3.merge(da3)\n+\n+print(\"Test 3 - With coordinates:\")\n+print(\"xr.merge([ds3, da3]):\", result5)\n+print(\"ds3.merge(da3):\", result6)\n+print(\"Are they equal?\", result5.equals(result6))\n+print()\n+\n+# Test 4: With overwrite_vars\n+ds4 = xr.Dataset({'a': 0, 'b': 1})\n+da4 = xr.DataArray(2, name='b')\n+\n+# Note: xr.merge doesn't support overwrite_vars, so we need to test differently\n+print(\"Test 4 - With overwrite_vars:\")\n+print(\"ds4.merge(da4, overwrite_vars='b'):\", ds4.merge(da4, overwrite_vars='b'))\n+print()\n+\n+# Test 5: Multiple DataArrays\n+ds5 = xr.Dataset({'a': 0})\n+da5a = xr.DataArray(1, name='b')\n+da5b = xr.DataArray(2, name='c')\n+\n+result7 = xr.merge([ds5, da5a, da5b])\n+result8 = ds5.merge(da5a).merge(da5b)\n+\n+print(\"Test 5 - Multiple DataArrays:\")\n+print(\"xr.merge([ds5, da5a, da5b]):\", result7)\n+print(\"ds5.merge(da5a).merge(da5b):\", result8)\n+print(\"Are they equal?\", result7.equals(result8))\n+print()\n+\n+# Test 6: update method\n+ds6 = xr.Dataset({'a': 0, 'b': 1})\n+da6 = xr.DataArray(2, name='b')\n+\n+print(\"Test 6 - Update method:\")\n+ds6_copy = ds6.copy()\n+ds6_copy.update(da6)\n+print(\"ds6.update(da6):\", ds6_copy)\n+print(\"Should be same as merge with overwrite_vars='b':\", ds6.merge(da6, overwrite_vars='b'))\n+print(\"Are they equal?\", ds6_copy.equals(ds6.merge(da6, overwrite_vars='b')))\n\\ No newline at end of file\ndiff --git a/test_dataarray_merge.py b/test_dataarray_merge.py\nnew file mode 100644\nindex 00000000..a87b15ef\n--- /dev/null\n+++ b/test_dataarray_merge.py\n@@ -0,0 +1,61 @@\n+import xarray as xr\n+\n+# Test 1: Basic DataArray merge with dataset.merge()\n+print(\"Test 1: Basic DataArray merge with dataset.merge()\")\n+ds = xr.Dataset({'a': 0})\n+da = xr.DataArray(1, name='b')\n+\n+print(\"Testing xr.merge([ds, da]):\")\n+expected = xr.merge([ds, da])  # works fine\n+print(expected)\n+print()\n+\n+print(\"Testing ds.merge(da):\")\n+try:\n+    actual = ds.merge(da)  # fails\n+    print(\"Success!\")\n+    print(actual)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+print()\n+\n+# Test 2: DataArray with dimensions\n+print(\"\\nTest 2: DataArray with dimensions\")\n+ds2 = xr.Dataset({'a': ('x', [1, 2, 3])})\n+da2 = xr.DataArray([4, 5, 6], dims=['x'], name='b')\n+\n+print(\"Testing xr.merge([ds2, da2]):\")\n+expected2 = xr.merge([ds2, da2])\n+print(expected2)\n+print()\n+\n+print(\"Testing ds2.merge(da2):\")\n+try:\n+    actual2 = ds2.merge(da2)\n+    print(\"Success!\")\n+    print(actual2)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+print()\n+\n+# Test 3: DataArray without name\n+print(\"\\nTest 3: DataArray without name\")\n+ds3 = xr.Dataset({'a': 0})\n+da3 = xr.DataArray(1)  # No name\n+\n+print(\"Testing xr.merge([ds3, da3]):\")\n+try:\n+    expected3 = xr.merge([ds3, da3])\n+    print(\"Success!\")\n+    print(expected3)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+print()\n+\n+print(\"Testing ds3.merge(da3):\")\n+try:\n+    actual3 = ds3.merge(da3)\n+    print(\"Success!\")\n+    print(actual3)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_dict_with_dataarray.py b/test_dict_with_dataarray.py\nnew file mode 100644\nindex 00000000..fff54fed\n--- /dev/null\n+++ b/test_dict_with_dataarray.py\n@@ -0,0 +1,50 @@\n+import xarray as xr\n+\n+print(\"Test dict with DataArray values in update method\")\n+\n+# Test 1: dict with DataArray value\n+ds1 = xr.Dataset({'a': 0})\n+da1 = xr.DataArray(1, name='b')\n+\n+print(\"Test 1: dict with DataArray\")\n+print(\"ds1:\", ds1)\n+print(\"dict with da1:\", {'b': da1})\n+result1 = ds1.update({'b': da1})\n+print(\"ds1.update({'b': da1}):\", result1)\n+print()\n+\n+# Test 2: dict with multiple DataArrays\n+ds2 = xr.Dataset({'a': 0})\n+da2a = xr.DataArray(1, name='b')\n+da2b = xr.DataArray(2, name='c')\n+\n+print(\"Test 2: dict with multiple DataArrays\")\n+result2 = ds2.update({'b': da2a, 'c': da2b})\n+print(\"ds2.update({'b': da2a, 'c': da2b}):\", result2)\n+print()\n+\n+# Test 3: dict with DataArray that has conflicting coordinates\n+ds3 = xr.Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2]})\n+da3 = xr.DataArray([4, 5, 6], dims=['x'], coords={'x': [0, 1, 2], 'y': ('x', [7, 8, 9])}, name='b')\n+\n+print(\"Test 3: DataArray with coordinates\")\n+print(\"ds3:\", ds3)\n+print(\"da3:\", da3)\n+result3 = ds3.update({'b': da3})\n+print(\"ds3.update({'b': da3}):\", result3)\n+print(\"Note: y coordinate should be added\")\n+print()\n+\n+# Test 4: dict with DataArray that has non-dimension coordinate that conflicts\n+ds4 = xr.Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2], 'y': 10})\n+da4 = xr.DataArray([4, 5, 6], dims=['x'], coords={'x': [0, 1, 2], 'y': ('x', [7, 8, 9])}, name='b')\n+\n+print(\"Test 4: DataArray with conflicting non-dimension coordinate\")\n+print(\"ds4 has scalar coordinate y=10\")\n+print(\"da4 has coordinate y as data variable with shape (3,)\")\n+result4 = ds4.update({'b': da4})\n+print(\"ds4.update({'b': da4}):\", result4)\n+print(\"Note: y coordinate from da4 should be dropped (conflict)\")\n+print()\n+\n+print(\"All tests passed!\")\n\\ No newline at end of file\ndiff --git a/test_edge_cases.py b/test_edge_cases.py\nnew file mode 100644\nindex 00000000..26b56826\n--- /dev/null\n+++ b/test_edge_cases.py\n@@ -0,0 +1,88 @@\n+import xarray as xr\n+import numpy as np\n+\n+print(\"Test 1: DataArray with coordinates\")\n+ds = xr.Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2]})\n+da = xr.DataArray([4, 5, 6], dims=['x'], coords={'x': [0, 1, 2], 'y': ('x', [7, 8, 9])}, name='b')\n+\n+print(\"ds:\", ds)\n+print(\"da:\", da)\n+print()\n+\n+print(\"Testing ds.merge(da):\")\n+result = ds.merge(da)\n+print(result)\n+print()\n+\n+print(\"Test 2: DataArray with overwrite_vars\")\n+ds2 = xr.Dataset({'a': 0, 'b': 1})\n+da2 = xr.DataArray(2, name='b')\n+\n+print(\"ds2:\", ds2)\n+print(\"da2:\", da2)\n+print()\n+\n+print(\"Testing ds2.merge(da2, overwrite_vars='b'):\")\n+result2 = ds2.merge(da2, overwrite_vars='b')\n+print(result2)\n+print()\n+\n+print(\"Test 3: DataArray with overwrite_vars as set\")\n+ds3 = xr.Dataset({'a': 0, 'b': 1, 'c': 2})\n+da3 = xr.DataArray(3, name='b')\n+\n+print(\"Testing ds3.merge(da3, overwrite_vars={'b'}):\")\n+result3 = ds3.merge(da3, overwrite_vars={'b'})\n+print(result3)\n+print()\n+\n+print(\"Test 4: DataArray with multiple overwrite_vars\")\n+ds4 = xr.Dataset({'a': 0, 'b': 1, 'c': 2})\n+da4 = xr.DataArray(3, name='b')\n+\n+print(\"Testing ds4.merge(da4, overwrite_vars=['b', 'c']):\")\n+try:\n+    result4 = ds4.merge(da4, overwrite_vars=['b', 'c'])\n+    print(result4)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+print()\n+\n+print(\"Test 5: DataArray update\")\n+ds5 = xr.Dataset({'a': 0, 'b': 1})\n+da5 = xr.DataArray(2, name='b')\n+\n+print(\"Testing ds5.update(da5):\")\n+ds5_copy = ds5.copy()\n+ds5_copy.update(da5)\n+print(ds5_copy)\n+print()\n+\n+print(\"Test 6: Unnamed DataArray should fail\")\n+ds6 = xr.Dataset({'a': 0})\n+da6 = xr.DataArray(1)  # No name\n+\n+print(\"Testing ds6.merge(da6):\")\n+try:\n+    result6 = ds6.merge(da6)\n+    print(result6)\n+except ValueError as e:\n+    print(f\"Expected ValueError: {e}\")\n+print()\n+\n+print(\"Test 7: DataArray with compat='identical'\")\n+ds7 = xr.Dataset({'a': 0})\n+da7 = xr.DataArray(1, name='b')\n+\n+print(\"Testing ds7.merge(da7, compat='identical'):\")\n+result7 = ds7.merge(da7, compat='identical')\n+print(result7)\n+print()\n+\n+print(\"Test 8: DataArray with join='inner' on overlapping dimensions\")\n+ds8 = xr.Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2]})\n+da8 = xr.DataArray([4, 5], dims=['x'], coords={'x': [1, 2]}, name='b')\n+\n+print(\"Testing ds8.merge(da8, join='inner'):\")\n+result8 = ds8.merge(da8, join='inner')\n+print(result8)\n\\ No newline at end of file\ndiff --git a/test_overwrite_vars.py b/test_overwrite_vars.py\nnew file mode 100644\nindex 00000000..a7b1d2d1\n--- /dev/null\n+++ b/test_overwrite_vars.py\n@@ -0,0 +1,70 @@\n+import xarray as xr\n+\n+print(\"Test overwrite_vars with DataArray\")\n+\n+# Test 1: overwrite_vars matches DataArray name\n+ds1 = xr.Dataset({'a': 0, 'b': 1})\n+da1 = xr.DataArray(2, name='b')\n+\n+print(\"Test 1: overwrite_vars='b' (matches DataArray name)\")\n+print(\"ds1:\", ds1)\n+print(\"da1:\", da1)\n+result1 = ds1.merge(da1, overwrite_vars='b')\n+print(\"ds1.merge(da1, overwrite_vars='b'):\", result1)\n+print(\"Expected: b should be 2\")\n+assert result1['b'].values == 2\n+print()\n+\n+# Test 2: overwrite_vars doesn't match DataArray name\n+ds2 = xr.Dataset({'a': 0, 'b': 1})\n+da2 = xr.DataArray(2, name='c')\n+\n+print(\"Test 2: overwrite_vars='b' (doesn't match DataArray name 'c')\")\n+print(\"ds2:\", ds2)\n+print(\"da2:\", da2)\n+result2 = ds2.merge(da2, overwrite_vars='b')\n+print(\"ds2.merge(da2, overwrite_vars='b'):\", result2)\n+print(\"Expected: b should be 1 (not overwritten), c should be 2\")\n+assert result2['b'].values == 1\n+assert result2['c'].values == 2\n+print()\n+\n+# Test 3: overwrite_vars is empty set\n+ds3 = xr.Dataset({'a': 0, 'b': 1})\n+da3 = xr.DataArray(2, name='c')\n+\n+print(\"Test 3: overwrite_vars=set()\")\n+result3 = ds3.merge(da3, overwrite_vars=set())\n+print(\"ds3.merge(da3, overwrite_vars=set()):\", result3)\n+print(\"Expected: a=0, b=1, c=2\")\n+assert result3['a'].values == 0\n+assert result3['b'].values == 1\n+assert result3['c'].values == 2\n+print()\n+\n+# Test 4: overwrite_vars is list with DataArray name\n+ds4 = xr.Dataset({'a': 0, 'b': 1, 'c': 3})\n+da4 = xr.DataArray(2, name='b')\n+\n+print(\"Test 4: overwrite_vars=['b', 'c']\")\n+result4 = ds4.merge(da4, overwrite_vars=['b', 'c'])\n+print(\"ds4.merge(da4, overwrite_vars=['b', 'c']):\", result4)\n+print(\"Expected: a=0, b=2 (overwritten), c=3 (not in DataArray, so stays)\")\n+assert result4['a'].values == 0\n+assert result4['b'].values == 2\n+assert result4['c'].values == 3\n+print()\n+\n+# Test 5: overwrite_vars contains name not in dataset\n+ds5 = xr.Dataset({'a': 0})\n+da5 = xr.DataArray(1, name='b')\n+\n+print(\"Test 5: overwrite_vars='c' (not in dataset or DataArray)\")\n+try:\n+    result5 = ds5.merge(da5, overwrite_vars='c')\n+    print(\"ds5.merge(da5, overwrite_vars='c'):\", result5)\n+except Exception as e:\n+    print(f\"Expected error: {type(e).__name__}: {e}\")\n+print()\n+\n+print(\"All tests passed!\")\n\\ No newline at end of file\ndiff --git a/test_update.py b/test_update.py\nnew file mode 100644\nindex 00000000..fa6c7fe6\n--- /dev/null\n+++ b/test_update.py\n@@ -0,0 +1,12 @@\n+import xarray as xr\n+\n+ds = xr.Dataset({'a': 0})\n+da = xr.DataArray(1, name='b')\n+\n+print(\"Testing ds.update(da):\")\n+try:\n+    ds.update(da)\n+    print(\"Success!\")\n+    print(ds)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex ac0a923d..fe700e7b 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -3515,7 +3515,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n             result = result._unstack_once(dim, fill_value, sparse)\n         return result\n \n-    def update(self, other: \"CoercibleMapping\", inplace: bool = None) -> \"Dataset\":\n+    def update(self, other: Union[\"DataArray\", \"CoercibleMapping\"], inplace: bool = None) -> \"Dataset\":\n         \"\"\"Update this dataset's variables with those from another dataset.\n \n         Parameters\n@@ -3547,7 +3547,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n \n     def merge(\n         self,\n-        other: \"CoercibleMapping\",\n+        other: Union[\"DataArray\", \"CoercibleMapping\"],\n         inplace: bool = None,\n         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),\n         compat: str = \"no_conflicts\",\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex 10c7804d..380ac74b 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -797,7 +797,7 @@ def merge(\n \n def dataset_merge_method(\n     dataset: \"Dataset\",\n-    other: \"CoercibleMapping\",\n+    other: Union[\"DataArray\", \"CoercibleMapping\"],\n     overwrite_vars: Union[Hashable, Iterable[Hashable]],\n     compat: str,\n     join: str,\n@@ -805,6 +805,8 @@ def dataset_merge_method(\n ) -> _MergeResult:\n     \"\"\"Guts of the Dataset.merge method.\n     \"\"\"\n+    from .dataarray import DataArray\n+    \n     # we are locked into supporting overwrite_vars for the Dataset.merge\n     # method due for backwards compatibility\n     # TODO: consider deprecating it?\n@@ -814,6 +816,10 @@ def dataset_merge_method(\n     else:\n         overwrite_vars = {overwrite_vars}\n \n+    # Convert DataArray to Dataset for compatibility with merge_core\n+    if isinstance(other, DataArray):\n+        other = other.to_dataset()\n+\n     if not overwrite_vars:\n         objs = [dataset, other]\n         priority_arg = None\n@@ -837,7 +843,7 @@ def dataset_merge_method(\n \n \n def dataset_update_method(\n-    dataset: \"Dataset\", other: \"CoercibleMapping\"\n+    dataset: \"Dataset\", other: Union[\"DataArray\", \"CoercibleMapping\"]\n ) -> _MergeResult:\n     \"\"\"Guts of the Dataset.update method.\n \n@@ -848,6 +854,10 @@ def dataset_update_method(\n     from .dataarray import DataArray\n     from .dataset import Dataset\n \n+    # Convert DataArray to Dataset for compatibility\n+    if isinstance(other, DataArray):\n+        other = other.to_dataset()\n+    \n     if not isinstance(other, Dataset):\n         other = dict(other)\n         for key, value in other.items():\n"
    },
    "pydata__xarray-3993": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-3993",
        "model_patch": "diff --git a/final_test.py b/final_test.py\nnew file mode 100644\nindex 00000000..d73e83b2\n--- /dev/null\n+++ b/final_test.py\n@@ -0,0 +1,103 @@\n+import xarray as xr\n+import numpy as np\n+import warnings\n+import pandas as pd\n+\n+print(\"Final comprehensive test of the DataArray.integrate API fix\")\n+print(\"=\" * 70)\n+\n+# Create test data\n+da = xr.DataArray(\n+    np.arange(12).reshape(4, 3),\n+    dims=['x', 'y'],\n+    coords={'x': [0, 0.1, 1.1, 1.2], 'y': [0, 1, 2]}\n+)\n+\n+ds = xr.Dataset({'var': da})\n+\n+print(\"\\n1. Testing API consistency:\")\n+print(\"-\" * 40)\n+\n+# Check that all methods now use 'coord' parameter consistently\n+methods_to_test = [\n+    (\"DataArray.differentiate\", da.differentiate),\n+    (\"DataArray.integrate\", da.integrate),\n+    (\"Dataset.differentiate\", ds.differentiate),\n+    (\"Dataset.integrate\", ds.integrate),\n+]\n+\n+for method_name, method in methods_to_test:\n+    # Test with coord parameter\n+    try:\n+        if 'differentiate' in method_name:\n+            result = method(coord='x')\n+        else:\n+            result = method(coord='x')\n+        print(f\"\u2713 {method_name}(coord='x') works\")\n+    except Exception as e:\n+        print(f\"\u2717 {method_name}(coord='x') failed: {e}\")\n+\n+print(\"\\n2. Testing backward compatibility for DataArray.integrate:\")\n+print(\"-\" * 40)\n+\n+# Test that dim parameter still works with deprecation warning\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    result = da.integrate(dim='x')\n+    if w and issubclass(w[0].category, FutureWarning):\n+        print(\"\u2713 DataArray.integrate(dim='x') works with deprecation warning\")\n+        print(f\"  Warning message: {w[0].message}\")\n+    else:\n+        print(\"\u2717 DataArray.integrate(dim='x') should raise deprecation warning\")\n+\n+print(\"\\n3. Testing that DataArray.differentiate never accepted 'dim':\")\n+print(\"-\" * 40)\n+\n+# Test that differentiate never accepted dim parameter\n+try:\n+    result = da.differentiate(dim='x')\n+    print(\"\u2717 DataArray.differentiate(dim='x') should have failed\")\n+except TypeError as e:\n+    print(f\"\u2713 DataArray.differentiate(dim='x') correctly raises TypeError: {e}\")\n+\n+print(\"\\n4. Testing positional arguments:\")\n+print(\"-\" * 40)\n+\n+# Test positional arguments work for all methods\n+for method_name, method in methods_to_test:\n+    try:\n+        if 'differentiate' in method_name:\n+            result = method('x')\n+        else:\n+            result = method('x')\n+        print(f\"\u2713 {method_name}('x') (positional) works\")\n+    except Exception as e:\n+        print(f\"\u2717 {method_name}('x') failed: {e}\")\n+\n+print(\"\\n5. Testing error handling:\")\n+print(\"-\" * 40)\n+\n+# Test error cases\n+test_cases = [\n+    (\"No arguments\", lambda: da.integrate(), TypeError),\n+    (\"Both coord and dim\", lambda: da.integrate(coord='x', dim='x'), TypeError),\n+    (\"Invalid keyword\", lambda: da.integrate(coord='x', invalid='x'), TypeError),\n+]\n+\n+for desc, func, expected_error in test_cases:\n+    try:\n+        func()\n+        print(f\"\u2717 {desc} should have raised {expected_error.__name__}\")\n+    except expected_error:\n+        print(f\"\u2713 {desc} correctly raises {expected_error.__name__}\")\n+    except Exception as e:\n+        print(f\"\u2717 {desc} raised wrong exception: {type(e).__name__}: {e}\")\n+\n+print(\"\\n\" + \"=\" * 70)\n+print(\"Summary:\")\n+print(\"- All integrate and differentiate methods now use 'coord' parameter\")\n+print(\"- DataArray.integrate maintains backward compatibility with 'dim'\")\n+print(\"- DataArray.differentiate never accepted 'dim' (unchanged)\")\n+print(\"- Positional arguments work for all methods\")\n+print(\"- Error handling is correct\")\n+print(\"\\nThe API inconsistency has been successfully fixed!\")\n\\ No newline at end of file\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..02189de3\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,66 @@\n+import xarray as xr\n+import numpy as np\n+\n+# Create a simple dataset and dataarray\n+ds = xr.Dataset(\n+    {\"a\": (\"x\", [1, 2, 3, 4]), \"b\": (\"x\", [5, 6, 7, 8])},\n+    coords={\"x\": [0, 1, 2, 3]}\n+)\n+\n+da = ds[\"a\"]\n+\n+print(\"Testing current API:\")\n+print(\"=\" * 50)\n+\n+# Test Dataset methods\n+print(\"Dataset.differentiate(coord='x'):\")\n+try:\n+    result = ds.differentiate(coord='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+print(\"\\nDataset.integrate(coord='x'):\")\n+try:\n+    result = ds.integrate(coord='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+# Test DataArray methods\n+print(\"\\nDataArray.differentiate(coord='x'):\")\n+try:\n+    result = da.differentiate(coord='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+print(\"\\nDataArray.integrate(dim='x'):\")\n+try:\n+    result = da.integrate(dim='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+# Try DataArray.integrate with 'coord' parameter (should fail currently)\n+print(\"\\nDataArray.integrate(coord='x'):\")\n+try:\n+    result = da.integrate(coord='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+# Try DataArray.differentiate with 'dim' parameter (should fail currently)\n+print(\"\\nDataArray.differentiate(dim='x'):\")\n+try:\n+    result = da.differentiate(dim='x')\n+    print(\"  Success\")\n+except Exception as e:\n+    print(f\"  Error: {e}\")\n+\n+print(\"\\n\" + \"=\" * 50)\n+print(\"Summary of API inconsistency:\")\n+print(\"Dataset.differentiate(coord='x') - uses 'coord'\")\n+print(\"Dataset.integrate(coord='x') - uses 'coord'\")\n+print(\"DataArray.differentiate(coord='x') - uses 'coord'\")\n+print(\"DataArray.integrate(dim='x') - uses 'dim' (inconsistent!)\")\n\\ No newline at end of file\ndiff --git a/test_fix.py b/test_fix.py\nnew file mode 100644\nindex 00000000..f388e6be\n--- /dev/null\n+++ b/test_fix.py\n@@ -0,0 +1,81 @@\n+import xarray as xr\n+import numpy as np\n+import warnings\n+\n+# Create a simple dataset and dataarray\n+ds = xr.Dataset(\n+    {\"a\": (\"x\", [1, 2, 3, 4]), \"b\": (\"x\", [5, 6, 7, 8])},\n+    coords={\"x\": [0, 1, 2, 3]}\n+)\n+\n+da = ds[\"a\"]\n+\n+print(\"Testing the fix for DataArray.integrate API inconsistency\")\n+print(\"=\" * 60)\n+\n+# Test 1: DataArray.integrate with coord parameter (new way)\n+print(\"\\nTest 1: DataArray.integrate(coord='x')\")\n+result = da.integrate(coord='x')\n+print(f\"  Result: {result.values}\")\n+print(\"  \u2713 Should work without warning\")\n+\n+# Test 2: DataArray.integrate with dim parameter (old way, deprecated)\n+print(\"\\nTest 2: DataArray.integrate(dim='x')\")\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    result = da.integrate(dim='x')\n+    print(f\"  Result: {result.values}\")\n+    if w and issubclass(w[0].category, FutureWarning):\n+        print(f\"  \u2713 Warning raised: {w[0].message}\")\n+    else:\n+        print(\"  \u2717 No warning raised (should have raised FutureWarning)\")\n+\n+# Test 3: DataArray.integrate with positional argument\n+print(\"\\nTest 3: DataArray.integrate('x') (positional)\")\n+result = da.integrate('x')\n+print(f\"  Result: {result.values}\")\n+print(\"  \u2713 Should work without warning\")\n+\n+# Test 4: DataArray.differentiate with coord parameter\n+print(\"\\nTest 4: DataArray.differentiate(coord='x')\")\n+result = da.differentiate(coord='x')\n+print(f\"  Result shape: {result.shape}\")\n+print(\"  \u2713 Should work\")\n+\n+# Test 5: Dataset.integrate with coord parameter\n+print(\"\\nTest 5: Dataset.integrate(coord='x')\")\n+result = ds.integrate(coord='x')\n+print(f\"  Result a: {result['a'].values}\")\n+print(f\"  Result b: {result['b'].values}\")\n+print(\"  \u2713 Should work\")\n+\n+# Test 6: Dataset.differentiate with coord parameter\n+print(\"\\nTest 6: Dataset.differentiate(coord='x')\")\n+result = ds.differentiate(coord='x')\n+print(f\"  Result a shape: {result['a'].shape}\")\n+print(f\"  Result b shape: {result['b'].shape}\")\n+print(\"  \u2713 Should work\")\n+\n+# Test 7: DataArray.integrate with dim and coord both specified (should error)\n+print(\"\\nTest 7: DataArray.integrate(coord='x', dim='x')\")\n+try:\n+    result = da.integrate(coord='x', dim='x')\n+    print(\"  \u2717 Should have raised TypeError\")\n+except TypeError as e:\n+    print(f\"  \u2713 Correctly raised TypeError: {e}\")\n+\n+# Test 8: DataArray.integrate with invalid keyword argument\n+print(\"\\nTest 8: DataArray.integrate(coord='x', invalid='x')\")\n+try:\n+    result = da.integrate(coord='x', invalid='x')\n+    print(\"  \u2717 Should have raised TypeError\")\n+except TypeError as e:\n+    print(f\"  \u2713 Correctly raised TypeError: {e}\")\n+\n+print(\"\\n\" + \"=\" * 60)\n+print(\"Summary:\")\n+print(\"- DataArray.integrate now accepts 'coord' parameter (consistent with Dataset.integrate)\")\n+print(\"- DataArray.integrate still accepts 'dim' parameter with deprecation warning\")\n+print(\"- All methods now use 'coord' parameter name consistently\")\n+print(\"- DataArray.differentiate and Dataset.differentiate already used 'coord'\")\n+print(\"- Dataset.integrate already used 'coord'\")\n\\ No newline at end of file\ndiff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex 2fef3edb..d8b30436 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -3481,21 +3481,26 @@ class DataArray(AbstractArray, DataWithCoords):\n         return self._from_temp_dataset(ds)\n \n     def integrate(\n-        self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n+        self, coord: Union[Hashable, Sequence[Hashable]] = None, datetime_unit: str = None, **kwargs\n     ) -> \"DataArray\":\n         \"\"\" integrate the array with the trapezoidal rule.\n \n         .. note::\n-            This feature is limited to simple cartesian geometry, i.e. dim\n+            This feature is limited to simple cartesian geometry, i.e. coord\n             must be one dimensional.\n \n         Parameters\n         ----------\n-        dim : hashable, or sequence of hashable\n+        coord : hashable, or sequence of hashable\n             Coordinate(s) used for the integration.\n         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \\\n                          \"ps\", \"fs\", \"as\"}, optional\n             Can be used to specify the unit if datetime coordinate is used.\n+        \n+        Other Parameters\n+        ----------------\n+        dim : hashable, or sequence of hashable, deprecated\n+            Deprecated in favor of `coord`.\n \n         Returns\n         -------\n@@ -3528,7 +3533,25 @@ class DataArray(AbstractArray, DataWithCoords):\n         array([5.4, 6.6, 7.8])\n         Dimensions without coordinates: y\n         \"\"\"\n-        ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n+        # Handle backward compatibility for 'dim' parameter\n+        if \"dim\" in kwargs:\n+            if coord is not None:\n+                raise TypeError(\"integrate() got multiple values for argument 'coord'\")\n+            warnings.warn(\n+                \"The 'dim' parameter name for DataArray.integrate has been deprecated \"\n+                \"and will be removed in a future version. Please use 'coord' instead.\",\n+                FutureWarning,\n+                stacklevel=2,\n+            )\n+            coord = kwargs.pop(\"dim\")\n+        \n+        if coord is None:\n+            raise TypeError(\"integrate() missing required argument 'coord'\")\n+        \n+        if kwargs:\n+            raise TypeError(f\"integrate() got unexpected keyword argument(s): {', '.join(kwargs.keys())}\")\n+        \n+        ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n         return self._from_temp_dataset(ds)\n \n     def unify_chunks(self) -> \"DataArray\":\ndiff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex a73e299e..f418ec7f 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -5904,7 +5904,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         attrs = self.attrs if keep_attrs else None\n         return self._replace(variables, coord_names, attrs=attrs)\n \n-    def differentiate(self, coord, edge_order=1, datetime_unit=None):\n+    def differentiate(\n+        self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n+    ) -> \"Dataset\":\n         \"\"\" Differentiate with the second order accurate central\n         differences.\n \n@@ -5963,7 +5965,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n                 variables[k] = v\n         return self._replace(variables)\n \n-    def integrate(self, coord, datetime_unit=None):\n+    def integrate(\n+        self, coord: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n+    ) -> \"Dataset\":\n         \"\"\" integrate the array with the trapezoidal rule.\n \n         .. note::\ndiff --git a/xarray/tests/test_units.py b/xarray/tests/test_units.py\nindex bb3127e9..76dd830d 100644\n--- a/xarray/tests/test_units.py\n+++ b/xarray/tests/test_units.py\n@@ -3681,7 +3681,7 @@ class TestDataArray:\n         (\n             method(\"diff\", dim=\"x\"),\n             method(\"differentiate\", coord=\"x\"),\n-            method(\"integrate\", dim=\"x\"),\n+            method(\"integrate\", coord=\"x\"),\n             method(\"quantile\", q=[0.25, 0.75]),\n             method(\"reduce\", func=np.sum, dim=\"x\"),\n             pytest.param(lambda x: x.dot(x), id=\"method_dot\"),\n"
    },
    "pydata__xarray-4075": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4075",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..f3e0c6a6\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,41 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing boolean weights issue...\")\n+dta = xr.DataArray([1., 1., 1.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=np.bool))\n+\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Weights dtype: {wgt.dtype}\")\n+\n+result = dta.weighted(wgt).mean()\n+print(f\"\\nResult: {result.values}\")\n+print(f\"Expected: 1.0\")\n+\n+# Let's also test the sum_of_weights calculation\n+print(\"\\n--- Testing sum_of_weights calculation ---\")\n+mask = dta.notnull()\n+print(f\"mask: {mask.values}\")\n+print(f\"mask dtype: {mask.dtype}\")\n+\n+# Test dot product directly\n+from xarray.core.computation import dot\n+dot_result = dot(mask, wgt)\n+print(f\"\\ndot(mask, wgt): {dot_result.values}\")\n+print(f\"dot(mask, wgt) dtype: {dot_result.dtype}\")\n+\n+# Test with conversion\n+dot_result_converted = dot(mask, wgt * 1)\n+print(f\"\\ndot(mask, wgt * 1): {dot_result_converted.values}\")\n+print(f\"dot(mask, wgt * 1) dtype: {dot_result_converted.dtype}\")\n+\n+# Let's also test weighted sum\n+weighted_sum = dta.weighted(wgt).sum()\n+print(f\"\\nWeighted sum: {weighted_sum.values}\")\n+print(f\"Expected weighted sum: 2.0\")\n+\n+# Test sum_of_weights method\n+sum_of_weights = dta.weighted(wgt).sum_of_weights()\n+print(f\"\\nsum_of_weights(): {sum_of_weights.values}\")\n+print(f\"Expected sum_of_weights: 2.0\")\n\\ No newline at end of file\ndiff --git a/test_dot_boolean_math.py b/test_dot_boolean_math.py\nnew file mode 100644\nindex 00000000..6c87301e\n--- /dev/null\n+++ b/test_dot_boolean_math.py\n@@ -0,0 +1,23 @@\n+import numpy as np\n+\n+# Test numpy dot with boolean arrays\n+a = np.array([True, True, True])\n+b = np.array([True, True, False])\n+\n+print(f\"a: {a}\")\n+print(f\"b: {b}\")\n+print(f\"np.dot(a, b): {np.dot(a, b)}\")\n+print(f\"np.dot(a, b).dtype: {np.dot(a, b).dtype}\")\n+print(f\"float(np.dot(a, b)): {float(np.dot(a, b))}\")\n+\n+# What's actually happening?\n+print(f\"\\na.astype(int): {a.astype(int)}\")\n+print(f\"b.astype(int): {b.astype(int)}\")\n+print(f\"np.dot(a.astype(int), b.astype(int)): {np.dot(a.astype(int), b.astype(int))}\")\n+\n+# Test with einsum\n+print(f\"\\nnp.einsum('i,i->', a, b): {np.einsum('i,i->', a, b)}\")\n+print(f\"np.einsum('i,i->', a, b).dtype: {np.einsum('i,i->', a, b).dtype}\")\n+\n+# So numpy.dot with boolean arrays returns a boolean scalar!\n+# True is being treated as 1, False as 0, but the result is boolean True (1)\n\\ No newline at end of file\ndiff --git a/test_dot_types.py b/test_dot_types.py\nnew file mode 100644\nindex 00000000..619baa30\n--- /dev/null\n+++ b/test_dot_types.py\n@@ -0,0 +1,39 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Test dot with different dtype combinations\n+print(\"Testing dot with different dtype combinations:\")\n+\n+# Boolean x Boolean\n+mask = xr.DataArray([True, True, True])\n+weights = xr.DataArray([True, True, False])\n+result = xr.dot(mask, weights)\n+print(f\"Boolean x Boolean: {result.values}, dtype: {result.dtype}\")\n+\n+# Boolean x Int\n+weights_int = xr.DataArray([1, 1, 0])\n+result = xr.dot(mask, weights_int)\n+print(f\"Boolean x Int: {result.values}, dtype: {result.dtype}\")\n+\n+# Boolean x Float\n+weights_float = xr.DataArray([1.0, 1.0, 0.0])\n+result = xr.dot(mask, weights_float)\n+print(f\"Boolean x Float: {result.values}, dtype: {result.dtype}\")\n+\n+# Int x Boolean\n+mask_int = xr.DataArray([1, 1, 1])\n+result = xr.dot(mask_int, weights)\n+print(f\"Int x Boolean: {result.values}, dtype: {result.dtype}\")\n+\n+# Float x Boolean\n+mask_float = xr.DataArray([1.0, 1.0, 1.0])\n+result = xr.dot(mask_float, weights)\n+print(f\"Float x Boolean: {result.values}, dtype: {result.dtype}\")\n+\n+# Test with conversion\n+print(\"\\nWith conversion:\")\n+result = xr.dot(mask, weights.astype(float))\n+print(f\"Boolean x Boolean (converted to float): {result.values}, dtype: {result.dtype}\")\n+\n+result = xr.dot(mask.astype(float), weights)\n+print(f\"Boolean (converted to float) x Boolean: {result.values}, dtype: {result.dtype}\")\n\\ No newline at end of file\ndiff --git a/test_edge_cases.py b/test_edge_cases.py\nnew file mode 100644\nindex 00000000..8bce830f\n--- /dev/null\n+++ b/test_edge_cases.py\n@@ -0,0 +1,86 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing edge cases for boolean weights...\")\n+\n+# Test 1: Original issue\n+print(\"\\n=== Test 1: Original issue ===\")\n+dta = xr.DataArray([1., 1., 1.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: 1.0\")\n+assert np.isclose(result.values, 1.0), f\"Expected 1.0, got {result.values}\"\n+\n+# Test 2: Boolean data with boolean weights\n+print(\"\\n=== Test 2: Boolean data with boolean weights ===\")\n+dta = xr.DataArray([True, True, True])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).sum()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: 2.0\")\n+assert np.isclose(result.values, 2.0), f\"Expected 2.0, got {result.values}\"\n+\n+# Test 3: Mixed types\n+print(\"\\n=== Test 3: Integer data with boolean weights ===\")\n+dta = xr.DataArray([1, 2, 3])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: 1.5\")\n+assert np.isclose(result.values, 1.5), f\"Expected 1.5, got {result.values}\"\n+\n+# Test 4: All False weights\n+print(\"\\n=== Test 4: All False weights ===\")\n+dta = xr.DataArray([1., 2., 3.])\n+wgt = xr.DataArray(np.array([0, 0, 0], dtype=bool))\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: nan\")\n+assert np.isnan(result.values), f\"Expected nan, got {result.values}\"\n+\n+# Test 5: With NaN in data\n+print(\"\\n=== Test 5: With NaN in data ===\")\n+dta = xr.DataArray([1., np.nan, 3.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).mean(skipna=True)\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: 1.0\")\n+assert np.isclose(result.values, 1.0), f\"Expected 1.0, got {result.values}\"\n+\n+# Test 6: sum_of_weights with boolean weights\n+print(\"\\n=== Test 6: sum_of_weights with boolean weights ===\")\n+dta = xr.DataArray([1., 2., 3.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).sum_of_weights()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"sum_of_weights: {result.values}\")\n+print(f\"Expected: 2.0\")\n+assert np.isclose(result.values, 2.0), f\"Expected 2.0, got {result.values}\"\n+\n+# Test 7: Dataset with boolean weights\n+print(\"\\n=== Test 7: Dataset with boolean weights ===\")\n+ds = xr.Dataset({'a': xr.DataArray([1., 2., 3.]), 'b': xr.DataArray([4., 5., 6.])})\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = ds.weighted(wgt).mean()\n+print(f\"Dataset a: {ds.a.values}\")\n+print(f\"Dataset b: {ds.b.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result a: {result.a.values}\")\n+print(f\"Result b: {result.b.values}\")\n+print(f\"Expected a: 1.5, b: 4.5\")\n+assert np.isclose(result.a.values, 1.5), f\"Expected 1.5, got {result.a.values}\"\n+assert np.isclose(result.b.values, 4.5), f\"Expected 4.5, got {result.b.values}\"\n+\n+print(\"\\n=== All tests passed! ===\")\n\\ No newline at end of file\ndiff --git a/test_einsum.py b/test_einsum.py\nnew file mode 100644\nindex 00000000..2fcab783\n--- /dev/null\n+++ b/test_einsum.py\n@@ -0,0 +1,32 @@\n+import numpy as np\n+\n+# Test numpy einsum with boolean arrays\n+a = np.array([True, True, True])\n+b = np.array([True, True, False])\n+\n+print(\"Testing numpy einsum with boolean arrays:\")\n+print(f\"a: {a}\")\n+print(f\"b: {b}\")\n+print(f\"np.einsum('i,i->', a, b): {np.einsum('i,i->', a, b)}\")\n+print(f\"Type: {type(np.einsum('i,i->', a, b))}\")\n+\n+# Test with int conversion\n+print(\"\\nWith int conversion:\")\n+print(f\"np.einsum('i,i->', a.astype(int), b.astype(int)): {np.einsum('i,i->', a.astype(int), b.astype(int))}\")\n+print(f\"Type: {type(np.einsum('i,i->', a.astype(int), b.astype(int)))}\")\n+\n+# Test with float conversion\n+print(\"\\nWith float conversion:\")\n+print(f\"np.einsum('i,i->', a.astype(float), b.astype(float)): {np.einsum('i,i->', a.astype(float), b.astype(float))}\")\n+print(f\"Type: {type(np.einsum('i,i->', a.astype(float), b.astype(float)))}\")\n+\n+# Test dot product\n+print(\"\\nTesting dot product:\")\n+print(f\"np.dot(a, b): {np.dot(a, b)}\")\n+print(f\"Type: {type(np.dot(a, b))}\")\n+\n+print(f\"np.dot(a.astype(int), b): {np.dot(a.astype(int), b)}\")\n+print(f\"Type: {type(np.dot(a.astype(int), b))}\")\n+\n+print(f\"np.dot(a, b.astype(int)): {np.dot(a, b.astype(int))}\")\n+print(f\"Type: {type(np.dot(a, b.astype(int)))}\")\n\\ No newline at end of file\ndiff --git a/test_integer_overflow.py b/test_integer_overflow.py\nnew file mode 100644\nindex 00000000..3788455a\n--- /dev/null\n+++ b/test_integer_overflow.py\n@@ -0,0 +1,32 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing for integer overflow issues...\")\n+\n+# Test with large integer values\n+print(\"\\n=== Test 1: Large integer values ===\")\n+da = xr.DataArray([1000000, 2000000, 3000000], dtype=np.int32)\n+wgt = xr.DataArray([1000000, 1000000, 0], dtype=np.int32)\n+result = da.weighted(wgt).mean()\n+print(f\"Data: {da.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Result dtype: {result.dtype}\")\n+expected = (1000000*1000000 + 2000000*1000000) / (1000000 + 1000000)\n+print(f\"Expected: {expected}\")\n+assert np.isclose(result.values, expected), f\"Expected {expected}, got {result.values}\"\n+\n+# Test with boolean weights and large integer data\n+print(\"\\n=== Test 2: Boolean weights with large integer data ===\")\n+da = xr.DataArray([1000000, 2000000, 3000000], dtype=np.int64)\n+wgt = xr.DataArray([True, True, False])\n+result = da.weighted(wgt).mean()\n+print(f\"Data: {da.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Result dtype: {result.dtype}\")\n+expected = (1000000 + 2000000) / 2\n+print(f\"Expected: {expected}\")\n+assert np.isclose(result.values, expected), f\"Expected {expected}, got {result.values}\"\n+\n+print(\"\\n=== All tests passed! ===\")\n\\ No newline at end of file\ndiff --git a/test_integer_weights.py b/test_integer_weights.py\nnew file mode 100644\nindex 00000000..d2bf4e1c\n--- /dev/null\n+++ b/test_integer_weights.py\n@@ -0,0 +1,42 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing integer weights...\")\n+\n+# Test with integer weights (not boolean)\n+print(\"\\n=== Test 1: Integer weights ===\")\n+dta = xr.DataArray([1., 2., 3.])\n+wgt = xr.DataArray([1, 2, 3])  # Integer weights\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+print(f\"Expected: (1*1 + 2*2 + 3*3) / (1+2+3) = 14/6 = 2.333...\")\n+expected = (1*1 + 2*2 + 3*3) / (1+2+3)\n+assert np.isclose(result.values, expected), f\"Expected {expected}, got {result.values}\"\n+\n+# Test with float weights\n+print(\"\\n=== Test 2: Float weights ===\")\n+dta = xr.DataArray([1., 2., 3.])\n+wgt = xr.DataArray([1.5, 2.5, 3.5])  # Float weights\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+expected = (1*1.5 + 2*2.5 + 3*3.5) / (1.5+2.5+3.5)\n+assert np.isclose(result.values, expected), f\"Expected {expected}, got {result.values}\"\n+\n+# Test with complex data types\n+print(\"\\n=== Test 3: Complex data types ===\")\n+dta = xr.DataArray([1+2j, 3+4j, 5+6j])\n+wgt = xr.DataArray([1., 2., 3.])  # Float weights\n+result = dta.weighted(wgt).mean()\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result: {result.values}\")\n+expected_numerator = (1+2j)*1 + (3+4j)*2 + (5+6j)*3\n+expected_denominator = 1 + 2 + 3\n+expected = expected_numerator / expected_denominator\n+assert np.isclose(result.values, expected), f\"Expected {expected}, got {result.values}\"\n+\n+print(\"\\n=== All tests passed! ===\")\n\\ No newline at end of file\ndiff --git a/test_skipna_behavior.py b/test_skipna_behavior.py\nnew file mode 100644\nindex 00000000..ccd45811\n--- /dev/null\n+++ b/test_skipna_behavior.py\n@@ -0,0 +1,51 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing skipna behavior with boolean weights...\")\n+\n+# Test with NaN in data and boolean weights\n+print(\"\\n=== Test 1: skipna=True with boolean weights ===\")\n+dta = xr.DataArray([1., np.nan, 3.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).mean(skipna=True)\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result (skipna=True): {result.values}\")\n+print(f\"Expected: 1.0 (only first element contributes)\")\n+assert np.isclose(result.values, 1.0), f\"Expected 1.0, got {result.values}\"\n+\n+# Test with skipna=False\n+print(\"\\n=== Test 2: skipna=False with boolean weights ===\")\n+result = dta.weighted(wgt).mean(skipna=False)\n+print(f\"Result (skipna=False): {result.values}\")\n+print(f\"Expected: nan (second element is nan)\")\n+assert np.isnan(result.values), f\"Expected nan, got {result.values}\"\n+\n+# Test with all NaN and boolean weights\n+print(\"\\n=== Test 3: All NaN with boolean weights ===\")\n+dta = xr.DataArray([np.nan, np.nan, np.nan])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).mean(skipna=True)\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result (skipna=True): {result.values}\")\n+print(f\"Expected: nan (all data is nan)\")\n+assert np.isnan(result.values), f\"Expected nan, got {result.values}\"\n+\n+# Test weighted sum with skipna\n+print(\"\\n=== Test 4: Weighted sum with skipna and boolean weights ===\")\n+dta = xr.DataArray([1., np.nan, 3.])\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=bool))\n+result = dta.weighted(wgt).sum(skipna=True)\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+print(f\"Result (skipna=True): {result.values}\")\n+print(f\"Expected: 1.0 (only first element contributes)\")\n+assert np.isclose(result.values, 1.0), f\"Expected 1.0, got {result.values}\"\n+\n+result = dta.weighted(wgt).sum(skipna=False)\n+print(f\"Result (skipna=False): {result.values}\")\n+print(f\"Expected: nan (second element is nan)\")\n+assert np.isnan(result.values), f\"Expected nan, got {result.values}\"\n+\n+print(\"\\n=== All tests passed! ===\")\n\\ No newline at end of file\ndiff --git a/test_various_dtypes.py b/test_various_dtypes.py\nnew file mode 100644\nindex 00000000..ec3f0d22\n--- /dev/null\n+++ b/test_various_dtypes.py\n@@ -0,0 +1,15 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing weighted operations with various dtypes...\")\n+\n+dtypes = [bool, np.int8, np.int16, np.int32, np.int64, np.float32, np.float64, np.complex64, np.complex128]\n+\n+for dtype in dtypes:\n+    da = xr.DataArray([1, 2, 3]).astype(dtype)\n+    wgt = xr.DataArray([1, 1, 0]).astype(dtype if dtype != bool else bool)\n+    try:\n+        result = da.weighted(wgt).mean()\n+        print(f\"{dtype.__name__}: {result.values} ({result.dtype})\")\n+    except Exception as e:\n+        print(f\"{dtype.__name__}: Error - {e}\")\n\\ No newline at end of file\ndiff --git a/test_weighted_sum_boolean.py b/test_weighted_sum_boolean.py\nnew file mode 100644\nindex 00000000..561c890d\n--- /dev/null\n+++ b/test_weighted_sum_boolean.py\n@@ -0,0 +1,37 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing weighted sum with boolean data and boolean weights...\")\n+dta = xr.DataArray([True, True, True])  # Boolean data\n+wgt = xr.DataArray(np.array([1, 1, 0], dtype=np.bool))\n+\n+print(f\"Data: {dta.values}\")\n+print(f\"Weights: {wgt.values}\")\n+\n+result = dta.weighted(wgt).sum()\n+print(f\"\\nWeighted sum: {result.values}\")\n+print(f\"Expected: 2.0 (sum of weights where data is True)\")\n+\n+# Also test with float data and boolean weights\n+print(\"\\n\\nTesting weighted sum with float data and boolean weights...\")\n+dta2 = xr.DataArray([1.0, 2.0, 3.0])\n+wgt2 = xr.DataArray(np.array([1, 1, 0], dtype=np.bool))\n+\n+print(f\"Data: {dta2.values}\")\n+print(f\"Weights: {wgt2.values}\")\n+\n+result2 = dta2.weighted(wgt2).sum()\n+print(f\"\\nWeighted sum: {result2.values}\")\n+print(f\"Expected: 3.0 (1.0*1 + 2.0*1 + 3.0*0)\")\n+\n+# Test weighted mean with float data and boolean weights\n+print(\"\\n\\nTesting weighted mean with float data and boolean weights...\")\n+dta3 = xr.DataArray([1.0, 2.0, 3.0])\n+wgt3 = xr.DataArray(np.array([1, 1, 0], dtype=np.bool))\n+\n+print(f\"Data: {dta3.values}\")\n+print(f\"Weights: {wgt3.values}\")\n+\n+result3 = dta3.weighted(wgt3).mean()\n+print(f\"\\nWeighted mean: {result3.values}\")\n+print(f\"Expected: 1.5 ((1.0*1 + 2.0*1 + 3.0*0) / 2)\")\n\\ No newline at end of file\ndiff --git a/xarray/core/weighted.py b/xarray/core/weighted.py\nindex 996d2e4c..0b713cea 100644\n--- a/xarray/core/weighted.py\n+++ b/xarray/core/weighted.py\n@@ -129,6 +129,13 @@ class Weighted:\n         if skipna or (skipna is None and da.dtype.kind in \"cfO\"):\n             da = da.fillna(0.0)\n \n+        # Convert boolean arrays to float to avoid boolean output from dot\n+        # when both inputs are boolean\n+        if da.dtype == bool:\n+            da = da.astype(float)\n+        if weights.dtype == bool:\n+            weights = weights.astype(float)\n+\n         # `dot` does not broadcast arrays, so this avoids creating a large\n         # DataArray (if `weights` has additional dimensions)\n         # maybe add fasttrack (`(da * weights).sum(dims=dim, skipna=skipna)`)\n"
    },
    "pydata__xarray-4094": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4094",
        "model_patch": "diff --git a/analyze_issue.py b/analyze_issue.py\nnew file mode 100644\nindex 00000000..95fcd289\n--- /dev/null\n+++ b/analyze_issue.py\n@@ -0,0 +1,62 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Test 1: Original failing case\n+print(\"Test 1: Original failing case\")\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+print(f\"Stacked shape: {stacked.shape}\")\n+print(f\"Stacked dims: {stacked.dims}\")\n+print(f\"Stacked coords: {list(stacked.coords.keys())}\")\n+print(f\"Stacked indexes: {stacked.indexes}\")\n+\n+# Let's look at the MultiIndex\n+idx = stacked.indexes['y']\n+print(f\"\\nMultiIndex levels: {idx.levels}\")\n+print(f\"MultiIndex names: {idx.names}\")\n+print(f\"MultiIndex codes: {idx.codes}\")\n+\n+# Test 2: Multi-dimensional case that works\n+print(\"\\n\\nTest 2: Multi-dimensional case that works\")\n+arr1 = xr.DataArray(\n+    np.arange(6).reshape(2, 3),\n+    coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])],\n+)\n+arr2 = xr.DataArray(\n+    np.arange(2),\n+    coords=[(\"x\", [0, 1])],\n+)\n+data2 = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+stacked2 = data2.to_stacked_array('z', sample_dims=['x'])\n+print(f\"Stacked2 shape: {stacked2.shape}\")\n+print(f\"Stacked2 dims: {stacked2.dims}\")\n+print(f\"Stacked2 coords: {list(stacked2.coords.keys())}\")\n+\n+idx2 = stacked2.indexes['z']\n+print(f\"\\nMultiIndex levels: {idx2.levels}\")\n+print(f\"MultiIndex names: {idx2.names}\")\n+print(f\"MultiIndex codes: {idx2.codes}\")\n+\n+# The key difference is that in Test 1, after selection we get a scalar 'y' coordinate\n+# In Test 2, after selection we don't have a 'z' coordinate at all\n+\n+# Let's check what happens in to_stacked_array for single-dim variables\n+print(\"\\n\\nChecking to_stacked_array implementation...\")\n+# When we have single-dim variables, the stacking_dims is empty tuple\n+stacking_dims = tuple(dim for dim in data.dims if dim not in ['x'])\n+print(f\"Stacking dims for Test 1: {stacking_dims}\")\n+\n+# In ensure_stackable function, for a variable with dims=['x']:\n+# - stacking_dims = () (empty tuple)\n+# - assign_coords = {variable_dim: 'a', **{dim: None for dim in stacking_dims if dim not in val.dims}}\n+#   Since stacking_dims is empty, assign_coords = {variable_dim: 'a'}\n+# - expand_dims = set(stacking_dims).difference(set(val.dims)) = set().difference({'x'}) = set()\n+# - expand_dims.add(variable_dim) = {'variable'}\n+# - So we expand_dims(['variable']) and stack({new_dim: ('variable',)})\n+\n+# This creates a MultiIndex with just one level (variable)\n+# When we select with variable_dim='a', we get a DataArray with scalar coordinate 'y'\n\\ No newline at end of file\ndiff --git a/debug_error.py b/debug_error.py\nnew file mode 100644\nindex 00000000..cf1a324b\n--- /dev/null\n+++ b/debug_error.py\n@@ -0,0 +1,49 @@\n+import numpy as np\n+import xarray as xr\n+import pandas as pd\n+\n+# Reproduce the error from the PR description\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+print(\"Original dataset:\")\n+print(data)\n+print(\"\\nStacking...\")\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+print(\"Stacked array:\")\n+print(stacked)\n+print(\"\\nStacked array indexes:\")\n+print(f\"Indexes: {stacked.indexes}\")\n+print(f\"Index 'y': {stacked.indexes['y']}\")\n+print(f\"Index 'y' type: {type(stacked.indexes['y'])}\")\n+print(f\"Index 'y' levels: {stacked.indexes['y'].levels}\")\n+print(f\"Index 'y' names: {stacked.indexes['y'].names}\")\n+\n+print(\"\\nTrying to unstack...\")\n+try:\n+    # Let's trace through the to_unstacked_dataset method\n+    idx = stacked.indexes['y']\n+    print(f\"\\nIndex is MultiIndex: {isinstance(idx, pd.MultiIndex)}\")\n+    level_number = idx._get_level_number(0)\n+    print(f\"Level number: {level_number}\")\n+    variables = idx.levels[level_number]\n+    print(f\"Variables: {variables}\")\n+    variable_dim = idx.names[level_number]\n+    print(f\"Variable dim: {variable_dim}\")\n+    \n+    # Let's see what happens when we select\n+    for k in variables:\n+        print(f\"\\nSelecting variable_dim={k}:\")\n+        selected = stacked.sel({variable_dim: k})\n+        print(f\"Selected shape: {selected.shape}\")\n+        print(f\"Selected dims: {selected.dims}\")\n+        squeezed = selected.squeeze(drop=True)\n+        print(f\"After squeeze shape: {squeezed.shape}\")\n+        print(f\"After squeeze dims: {squeezed.dims}\")\n+        \n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n\\ No newline at end of file\ndiff --git a/debug_error2.py b/debug_error2.py\nnew file mode 100644\nindex 00000000..2f63a757\n--- /dev/null\n+++ b/debug_error2.py\n@@ -0,0 +1,52 @@\n+import numpy as np\n+import xarray as xr\n+import pandas as pd\n+\n+# Reproduce the error from the PR description\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+print(\"Original dataset:\")\n+print(data)\n+print(\"\\nStacking...\")\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+print(\"Stacked array:\")\n+print(stacked)\n+\n+print(\"\\nTrying to unstack manually...\")\n+# Let's manually do what to_unstacked_dataset does\n+idx = stacked.indexes['y']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+# pull variables out of datarray\n+data_dict = {}\n+for k in variables:\n+    data_dict[k] = stacked.sel({variable_dim: k}).squeeze(drop=True)\n+    print(f\"\\nVariable {k}:\")\n+    print(f\"  DataArray: {data_dict[k]}\")\n+    print(f\"  dims: {data_dict[k].dims}\")\n+    print(f\"  coords: {data_dict[k].coords}\")\n+\n+print(\"\\nTrying to create Dataset from data_dict...\")\n+try:\n+    result = xr.Dataset(data_dict)\n+    print(\"Success!\")\n+    print(result)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n+    \n+print(\"\\n\\nLet's check what happens when we create Dataset with merge='override'\")\n+try:\n+    result = xr.Dataset(data_dict, compat='override')\n+    print(\"Success with compat='override'!\")\n+    print(result)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n\\ No newline at end of file\ndiff --git a/debug_error3.py b/debug_error3.py\nnew file mode 100644\nindex 00000000..a29e2f4d\n--- /dev/null\n+++ b/debug_error3.py\n@@ -0,0 +1,62 @@\n+import numpy as np\n+import xarray as xr\n+import pandas as pd\n+\n+# Reproduce the error from the PR description\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+print(\"Original dataset:\")\n+print(data)\n+print(\"\\nStacking...\")\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+print(\"Stacked array:\")\n+print(stacked)\n+\n+print(\"\\nLet's trace through what happens...\")\n+# When we select with variable_dim, we get a scalar coordinate 'y'\n+idx = stacked.indexes['y']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+print(f\"\\nVariable dim: {variable_dim}\")\n+print(f\"Variables: {variables}\")\n+\n+for k in variables:\n+    selected = stacked.sel({variable_dim: k})\n+    print(f\"\\nSelected {k}:\")\n+    print(f\"  Shape: {selected.shape}\")\n+    print(f\"  Dims: {selected.dims}\")\n+    print(f\"  Coords: {selected.coords}\")\n+    print(f\"  Has 'y' coord: {'y' in selected.coords}\")\n+    if 'y' in selected.coords:\n+        print(f\"  y coord value: {selected.coords['y'].values}\")\n+    \n+    squeezed = selected.squeeze(drop=True)\n+    print(f\"\\nAfter squeeze:\")\n+    print(f\"  Shape: {squeezed.shape}\")\n+    print(f\"  Dims: {squeezed.dims}\")\n+    print(f\"  Coords: {squeezed.coords}\")\n+    print(f\"  Has 'y' coord: {'y' in squeezed.coords}\")\n+    if 'y' in squeezed.coords:\n+        print(f\"  y coord value: {squeezed.coords['y'].values}\")\n+\n+print(\"\\n\\nLet's try dropping the 'y' coordinate before creating the dataset:\")\n+data_dict = {}\n+for k in variables:\n+    data_dict[k] = stacked.sel({variable_dim: k}).squeeze(drop=True).drop_vars('y')\n+    print(f\"\\nVariable {k} after dropping 'y':\")\n+    print(f\"  Coords: {data_dict[k].coords}\")\n+\n+print(\"\\nTrying to create Dataset from data_dict with 'y' dropped...\")\n+try:\n+    result = xr.Dataset(data_dict)\n+    print(\"Success!\")\n+    print(result)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n\\ No newline at end of file\ndiff --git a/debug_sel_issue.py b/debug_sel_issue.py\nnew file mode 100644\nindex 00000000..56a7f1fe\n--- /dev/null\n+++ b/debug_sel_issue.py\n@@ -0,0 +1,58 @@\n+import numpy as np\n+import xarray as xr\n+import pandas as pd\n+\n+print(\"Debugging the selection issue...\")\n+\n+# Create a simple test case\n+arr = xr.DataArray(np.arange(3), coords=[(\"x\", [0, 1, 2])])\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+\n+print(\"Stacked array:\")\n+print(stacked)\n+print(f\"\\nStacked indexes['y']: {stacked.indexes['y']}\")\n+\n+# When we select with variable_dim='a', what happens?\n+idx = stacked.indexes['y']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+print(f\"\\nVariable dim: {variable_dim}\")\n+print(f\"Variables: {variables}\")\n+\n+# Let's manually select 'a'\n+print(\"\\nManually selecting 'a':\")\n+selected_a = stacked.sel({variable_dim: 'a'})\n+print(f\"Selected shape: {selected_a.shape}\")\n+print(f\"Selected dims: {selected_a.dims}\")\n+print(f\"Selected coords: {selected_a.coords}\")\n+\n+# The issue is that after selection, we still have dimension 'y' \n+# but it's now a scalar coordinate with value ('a',)\n+print(f\"\\n'y' in selected_a.coords: {'y' in selected_a.coords}\")\n+if 'y' in selected_a.coords:\n+    print(f\"y coord value: {selected_a.coords['y'].values}\")\n+    print(f\"y coord shape: {selected_a.coords['y'].shape}\")\n+    print(f\"y coord dims: {selected_a.coords['y'].dims}\")\n+\n+# After squeeze\n+squeezed_a = selected_a.squeeze(drop=True)\n+print(f\"\\nAfter squeeze:\")\n+print(f\"Shape: {squeezed_a.shape}\")\n+print(f\"Dims: {squeezed_a.dims}\")\n+print(f\"Coords: {squeezed_a.coords}\")\n+print(f\"'y' in squeezed_a.coords: {'y' in squeezed_a.coords}\")\n+if 'y' in squeezed_a.coords:\n+    print(f\"y coord value: {squeezed_a.coords['y'].values}\")\n+    print(f\"y coord shape: {squeezed_a.coords['y'].shape}\")\n+    print(f\"y coord dims: {squeezed_a.coords['y'].dims}\")\n+\n+# Now let's see what happens with our fix\n+print(\"\\n\\nWith our fix (dropping 'y' coordinate):\")\n+if 'y' in squeezed_a.coords and 'y' not in squeezed_a.dims:\n+    fixed_a = squeezed_a.drop_vars('y')\n+    print(f\"After dropping 'y':\")\n+    print(f\"Coords: {fixed_a.coords}\")\n+    print(f\"'y' in coords: {'y' in fixed_a.coords}\")\n\\ No newline at end of file\ndiff --git a/debug_test5.py b/debug_test5.py\nnew file mode 100644\nindex 00000000..c8902ead\n--- /dev/null\n+++ b/debug_test5.py\n@@ -0,0 +1,48 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Debugging Test 5: Variables with 3 dimensions\")\n+arr5 = xr.DataArray(np.arange(24).reshape(2, 3, 4), \n+                   coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2]), (\"z\", [0, 1, 2, 3])])\n+data5 = xr.Dataset({\"a\": arr5, \"b\": arr5.isel(z=0)})\n+print(\"Original dataset:\")\n+print(data5)\n+\n+stacked5 = data5.to_stacked_array('w', sample_dims=['x'])\n+print(\"\\nStacked array:\")\n+print(stacked5)\n+print(f\"\\nStacked indexes['w']: {stacked5.indexes['w']}\")\n+print(f\"Stacked indexes['w'] levels: {stacked5.indexes['w'].levels}\")\n+print(f\"Stacked indexes['w'] names: {stacked5.indexes['w'].names}\")\n+\n+unstacked5 = stacked5.to_unstacked_dataset('w')\n+print(\"\\nUnstacked dataset:\")\n+print(unstacked5)\n+\n+print(\"\\n\\nChecking if they're identical...\")\n+print(f\"Are they identical? {data5.identical(unstacked5)}\")\n+\n+# Let's check what's different\n+print(\"\\nChecking differences...\")\n+print(\"Original 'a' shape:\", data5['a'].shape)\n+print(\"Unstacked 'a' shape:\", unstacked5['a'].shape)\n+print(\"Original 'b' shape:\", data5['b'].shape)\n+print(\"Unstacked 'b' shape:\", unstacked5['b'].shape)\n+\n+print(\"\\nOriginal 'a' dtype:\", data5['a'].dtype)\n+print(\"Unstacked 'a' dtype:\", unstacked5['a'].dtype)\n+print(\"Original 'b' dtype:\", data5['b'].dtype)\n+print(\"Unstacked 'b' dtype:\", unstacked5['b'].dtype)\n+\n+# Check if values are the same\n+print(\"\\nAre 'a' values equal?\", np.allclose(data5['a'].values, unstacked5['a'].values))\n+print(\"Are 'b' values equal?\", np.allclose(data5['b'].values, unstacked5['b'].values))\n+\n+# Check coordinates\n+print(\"\\nOriginal coordinates:\", list(data5.coords.keys()))\n+print(\"Unstacked coordinates:\", list(unstacked5.coords.keys()))\n+\n+# Check if 'z' coordinate has NaN values\n+print(\"\\nOriginal 'z' coordinate:\", data5.coords['z'].values)\n+print(\"Unstacked 'z' coordinate:\", unstacked5.coords['z'].values)\n+print(\"Does unstacked 'z' have NaN?\", np.any(np.isnan(unstacked5.coords['z'].values)))\n\\ No newline at end of file\ndiff --git a/debug_test5_detailed.py b/debug_test5_detailed.py\nnew file mode 100644\nindex 00000000..fb2c8cc1\n--- /dev/null\n+++ b/debug_test5_detailed.py\n@@ -0,0 +1,57 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Debugging Test 5: Variables with 3 dimensions\")\n+arr5 = xr.DataArray(np.arange(24).reshape(2, 3, 4), \n+                   coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2]), (\"z\", [0, 1, 2, 3])])\n+data5 = xr.Dataset({\"a\": arr5, \"b\": arr5.isel(z=0)})\n+print(\"Original dataset:\")\n+print(data5)\n+\n+stacked5 = data5.to_stacked_array('w', sample_dims=['x'])\n+print(\"\\nStacked array:\")\n+print(stacked5)\n+\n+# Let's trace through what to_unstacked_dataset does\n+print(\"\\n\\nTracing through to_unstacked_dataset...\")\n+idx = stacked5.indexes['w']\n+print(f\"MultiIndex levels: {idx.levels}\")\n+print(f\"MultiIndex names: {idx.names}\")\n+\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+print(f\"\\nVariable dim: {variable_dim}\")\n+print(f\"Variables: {variables}\")\n+\n+# Check what happens when we select\n+for k in variables:\n+    selected = stacked5.sel({variable_dim: k})\n+    print(f\"\\nSelected {k}:\")\n+    print(f\"  Shape: {selected.shape}\")\n+    print(f\"  Dims: {selected.dims}\")\n+    print(f\"  Coords: {list(selected.coords.keys())}\")\n+    \n+    squeezed = selected.squeeze(drop=True)\n+    print(f\"  After squeeze shape: {squeezed.shape}\")\n+    print(f\"  After squeeze dims: {squeezed.dims}\")\n+    print(f\"  After squeeze coords: {list(squeezed.coords.keys())}\")\n+    \n+    # Check if 'w' is in coords\n+    if 'w' in squeezed.coords:\n+        print(f\"  'w' in coords: True\")\n+        print(f\"  'w' shape: {squeezed.coords['w'].shape}\")\n+    else:\n+        print(f\"  'w' in coords: False\")\n+\n+# Now let's see what the actual unstacked dataset looks like\n+print(\"\\n\\nActual unstacked dataset:\")\n+unstacked5 = stacked5.to_unstacked_dataset('w')\n+print(unstacked5)\n+\n+# The issue seems to be that we're not getting the right shape back\n+# Let me check the test that's in the actual test suite\n+print(\"\\n\\nLooking at the actual test in test_dataset.py...\")\n+# The test uses create_test_stacked_array which creates 2D arrays\n+# Let me check if the issue is with 3D arrays specifically\n\\ No newline at end of file\ndiff --git a/debug_working_case.py b/debug_working_case.py\nnew file mode 100644\nindex 00000000..0ed95e35\n--- /dev/null\n+++ b/debug_working_case.py\n@@ -0,0 +1,58 @@\n+import numpy as np\n+import xarray as xr\n+import pandas as pd\n+\n+# Let's look at the working case from the tests\n+print(\"Working case from tests:\")\n+# This is similar to create_test_stacked_array\n+x = xr.DataArray(pd.Index(np.r_[:10], name=\"x\"))\n+y = xr.DataArray(pd.Index(np.r_[:20], name=\"y\"))\n+a = x * y\n+b = x * y * y\n+D = xr.Dataset({\"a\": a, \"b\": b})\n+print(\"Original dataset:\")\n+print(D)\n+\n+sample_dims = [\"x\"]\n+stacked = D.to_stacked_array(\"features\", sample_dims)\n+print(\"\\nStacked array:\")\n+print(stacked)\n+\n+unstacked = stacked.to_unstacked_dataset(\"features\")\n+print(\"\\nUnstacked dataset:\")\n+print(unstacked)\n+\n+print(f\"\\nAre they identical? {D.identical(unstacked)}\")\n+\n+# Now let's trace through what happens\n+print(\"\\n\\nTracing through to_unstacked_dataset...\")\n+idx = stacked.indexes['features']\n+print(f\"MultiIndex levels: {idx.levels}\")\n+print(f\"MultiIndex names: {idx.names}\")\n+\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+print(f\"\\nVariable dim: {variable_dim}\")\n+print(f\"Variables: {variables}\")\n+\n+# Check what happens when we select\n+for k in variables:\n+    selected = stacked.sel({variable_dim: k})\n+    print(f\"\\nSelected {k}:\")\n+    print(f\"  Shape: {selected.shape}\")\n+    print(f\"  Dims: {selected.dims}\")\n+    print(f\"  Coords: {list(selected.coords.keys())}\")\n+    \n+    squeezed = selected.squeeze(drop=True)\n+    print(f\"  After squeeze shape: {squeezed.shape}\")\n+    print(f\"  After squeeze dims: {squeezed.dims}\")\n+    print(f\"  After squeeze coords: {list(squeezed.coords.keys())}\")\n+    \n+    # Check if 'features' is in coords\n+    if 'features' in squeezed.coords:\n+        print(f\"  'features' in coords: True\")\n+        print(f\"  'features' shape: {squeezed.coords['features'].shape}\")\n+    else:\n+        print(f\"  'features' in coords: False\")\n\\ No newline at end of file\ndiff --git a/reproduce_error.py b/reproduce_error.py\nnew file mode 100644\nindex 00000000..3557fce6\n--- /dev/null\n+++ b/reproduce_error.py\n@@ -0,0 +1,22 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Reproduce the error from the PR description\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+print(\"Original dataset:\")\n+print(data)\n+print(\"\\nStacking...\")\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+print(\"Stacked array:\")\n+print(stacked)\n+print(\"\\nUnstacking...\")\n+try:\n+    unstacked = stacked.to_unstacked_dataset('y')\n+    print(\"Unstacked dataset:\")\n+    print(unstacked)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\ndiff --git a/test_comprehensive.py b/test_comprehensive.py\nnew file mode 100644\nindex 00000000..8cd8e8b4\n--- /dev/null\n+++ b/test_comprehensive.py\n@@ -0,0 +1,70 @@\n+import numpy as np\n+import xarray as xr\n+\n+print(\"Testing comprehensive edge cases for to_unstacked_dataset fix\\n\")\n+\n+# Test 1: Original failing case - single-dim variables\n+print(\"Test 1: Single-dim variables\")\n+arr = xr.DataArray(np.arange(3), coords=[(\"x\", [0, 1, 2])])\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+unstacked = stacked.to_unstacked_dataset('y')\n+print(f\"  Original and unstacked identical? {data.identical(unstacked)}\")\n+\n+# Test 2: Multi-dim variables with different dimensions\n+print(\"\\nTest 2: Multi-dim variables with different dimensions\")\n+arr1 = xr.DataArray(np.arange(6).reshape(2, 3), coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])])\n+arr2 = xr.DataArray(np.arange(2), coords=[(\"x\", [0, 1])])\n+data2 = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+stacked2 = data2.to_stacked_array('z', sample_dims=['x'])\n+unstacked2 = stacked2.to_unstacked_dataset('z')\n+print(f\"  Original and unstacked identical? {data2.identical(unstacked2)}\")\n+\n+# Test 3: All variables have same multi-dimensions\n+print(\"\\nTest 3: All variables have same multi-dimensions\")\n+arr3 = xr.DataArray(np.arange(6).reshape(2, 3), coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])])\n+data3 = xr.Dataset({\"a\": arr3, \"b\": arr3 * 2})\n+stacked3 = data3.to_stacked_array('z', sample_dims=['x'])\n+unstacked3 = stacked3.to_unstacked_dataset('z')\n+print(f\"  Original and unstacked identical? {data3.identical(unstacked3)}\")\n+\n+# Test 4: Three variables with single dimension\n+print(\"\\nTest 4: Three variables with single dimension\")\n+arr4 = xr.DataArray(np.arange(3), coords=[(\"x\", [0, 1, 2])])\n+data4 = xr.Dataset({\"a\": arr4, \"b\": arr4 * 2, \"c\": arr4 * 3})\n+stacked4 = data4.to_stacked_array('y', sample_dims=['x'])\n+unstacked4 = stacked4.to_unstacked_dataset('y')\n+print(f\"  Original and unstacked identical? {data4.identical(unstacked4)}\")\n+\n+# Test 5: Variables with 3 dimensions\n+print(\"\\nTest 5: Variables with 3 dimensions\")\n+arr5 = xr.DataArray(np.arange(24).reshape(2, 3, 4), \n+                   coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2]), (\"z\", [0, 1, 2, 3])])\n+data5 = xr.Dataset({\"a\": arr5, \"b\": arr5.isel(z=0)})\n+stacked5 = data5.to_stacked_array('w', sample_dims=['x'])\n+unstacked5 = stacked5.to_unstacked_dataset('w')\n+print(f\"  Original and unstacked identical? {data5.identical(unstacked5)}\")\n+\n+# Test 6: Using different level\n+print(\"\\nTest 6: Using different level (should raise error for single-level MultiIndex)\")\n+try:\n+    # For single-level MultiIndex, level 0 is the only valid level\n+    stacked.to_unstacked_dataset('y', level=0)\n+    print(\"  Level 0 works (as expected)\")\n+except Exception as e:\n+    print(f\"  Error with level 0: {e}\")\n+\n+# Test 7: Check that we don't drop coordinates that should be kept\n+print(\"\\nTest 7: Verify we don't drop dimension coordinates\")\n+# Create a dataset with additional coordinates\n+arr7 = xr.DataArray(np.arange(6).reshape(2, 3), \n+                   coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])])\n+# Add extra coordinate after creating the array\n+arr7 = arr7.assign_coords(extra=(\"x\", [10, 20]))\n+data7 = xr.Dataset({\"a\": arr7, \"b\": arr7.isel(y=0)})\n+stacked7 = data7.to_stacked_array('z', sample_dims=['x'])\n+unstacked7 = stacked7.to_unstacked_dataset('z')\n+print(f\"  Original and unstacked identical? {data7.identical(unstacked7)}\")\n+print(f\"  'extra' coordinate preserved? {'extra' in unstacked7.coords}\")\n+\n+print(\"\\nAll tests completed!\")\n\\ No newline at end of file\ndiff --git a/test_edge_cases.py b/test_edge_cases.py\nnew file mode 100644\nindex 00000000..81775ce7\n--- /dev/null\n+++ b/test_edge_cases.py\n@@ -0,0 +1,63 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Test edge case: what if 'y' is already a dimension in the original data?\n+print(\"Test 1: 'y' is already a dimension\")\n+arr = xr.DataArray(\n+     np.arange(6).reshape(2, 3),\n+     coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+print(\"Original dataset:\")\n+print(data)\n+\n+# If we stack with sample_dims=['x'], then 'y' is a stacking dimension\n+stacked = data.to_stacked_array('z', sample_dims=['x'])\n+print(\"\\nStacked array:\")\n+print(stacked)\n+print(f\"\\nStacked indexes['z']: {stacked.indexes['z']}\")\n+\n+# Now when we unstack, we should get 'y' back as a dimension\n+idx = stacked.indexes['z']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+data_dict = {}\n+for k in variables:\n+    selected = stacked.sel({variable_dim: k}).squeeze(drop=True)\n+    print(f\"\\nSelected {k}:\")\n+    print(f\"  dims: {selected.dims}\")\n+    print(f\"  coords: {list(selected.coords.keys())}\")\n+    if 'z' in selected.coords:\n+        print(f\"  'z' coord: {selected.coords['z'].values}\")\n+    if 'y' in selected.coords:\n+        print(f\"  'y' coord: {selected.coords['y'].values}\")\n+    \n+    # In this case, 'z' is not in coords after squeeze because it was a dimension\n+    # 'y' should be in coords as a dimension coordinate\n+\n+# Test 2: What if we have multiple stacking dimensions?\n+print(\"\\n\\nTest 2: Multiple stacking dimensions\")\n+arr1 = xr.DataArray(\n+    np.arange(12).reshape(2, 3, 2),\n+    coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2]), (\"z\", [0, 1])],\n+)\n+arr2 = xr.DataArray(\n+    np.arange(6).reshape(2, 3),\n+    coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])],\n+)\n+data2 = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+print(\"Original dataset:\")\n+print(data2)\n+\n+stacked2 = data2.to_stacked_array('w', sample_dims=['x'])\n+print(\"\\nStacked array:\")\n+print(stacked2)\n+\n+idx2 = stacked2.indexes['w']\n+print(f\"\\nMultiIndex levels: {idx2.levels}\")\n+print(f\"MultiIndex names: {idx2.names}\")\n+\n+# When we select variable='a', we should get back dimensions x, y, z\n+# When we select variable='b', we should get back dimensions x, y\n\\ No newline at end of file\ndiff --git a/test_fix.py b/test_fix.py\nnew file mode 100644\nindex 00000000..2ec9bf1b\n--- /dev/null\n+++ b/test_fix.py\n@@ -0,0 +1,70 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Test the fix\n+arr = xr.DataArray(\n+     np.arange(3),\n+     coords=[(\"x\", [0, 1, 2])],\n+ )\n+data = xr.Dataset({\"a\": arr, \"b\": arr})\n+stacked = data.to_stacked_array('y', sample_dims=['x'])\n+\n+# Let's manually implement the fix\n+idx = stacked.indexes['y']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+# pull variables out of datarray\n+data_dict = {}\n+for k in variables:\n+    # Select and squeeze\n+    selected = stacked.sel({variable_dim: k}).squeeze(drop=True)\n+    \n+    # Drop the stacked dimension coordinate if it exists\n+    # (it will be a scalar coordinate after squeeze)\n+    if 'y' in selected.coords:\n+        selected = selected.drop_vars('y')\n+    \n+    data_dict[k] = selected\n+\n+# unstacked dataset\n+result = xr.Dataset(data_dict)\n+print(\"Result:\")\n+print(result)\n+print(\"\\nOriginal:\")\n+print(data)\n+print(f\"\\nAre they identical? {data.identical(result)}\")\n+\n+# Also test with multi-dimensional case\n+print(\"\\n\\nTesting multi-dimensional case...\")\n+arr1 = xr.DataArray(\n+    np.arange(6).reshape(2, 3),\n+    coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])],\n+)\n+arr2 = xr.DataArray(\n+    np.arange(2),\n+    coords=[(\"x\", [0, 1])],\n+)\n+data2 = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+stacked2 = data2.to_stacked_array('z', sample_dims=['x'])\n+\n+idx2 = stacked2.indexes['z']\n+level_number2 = idx2._get_level_number(0)\n+variables2 = idx2.levels[level_number2]\n+variable_dim2 = idx2.names[level_number2]\n+\n+data_dict2 = {}\n+for k in variables2:\n+    selected = stacked2.sel({variable_dim2: k}).squeeze(drop=True)\n+    # Drop the stacked dimension coordinate if it exists\n+    if 'z' in selected.coords:\n+        selected = selected.drop_vars('z')\n+    data_dict2[k] = selected\n+\n+result2 = xr.Dataset(data_dict2)\n+print(\"Result2:\")\n+print(result2)\n+print(\"\\nOriginal2:\")\n+print(data2)\n+print(f\"\\nAre they identical? {data2.identical(result2)}\")\n\\ No newline at end of file\ndiff --git a/test_hypothesis.py b/test_hypothesis.py\nnew file mode 100644\nindex 00000000..9bda1fa9\n--- /dev/null\n+++ b/test_hypothesis.py\n@@ -0,0 +1,62 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Hypothesis: The issue only happens when the MultiIndex has only one level\n+print(\"Testing hypothesis...\")\n+\n+# Case 1: Single-dim variables -> MultiIndex with 1 level\n+arr = xr.DataArray(np.arange(3), coords=[(\"x\", [0, 1, 2])])\n+data1 = xr.Dataset({\"a\": arr, \"b\": arr})\n+stacked1 = data1.to_stacked_array('y', sample_dims=['x'])\n+print(f\"Case 1 - MultiIndex levels: {len(stacked1.indexes['y'].levels)}\")\n+\n+# Check if 'y' is in coords after selection\n+idx1 = stacked1.indexes['y']\n+level_number1 = idx1._get_level_number(0)\n+variables1 = idx1.levels[level_number1]\n+variable_dim1 = idx1.names[level_number1]\n+\n+for k in variables1:\n+    selected = stacked1.sel({variable_dim1: k}).squeeze(drop=True)\n+    print(f\"  Selected {k}: 'y' in coords? {'y' in selected.coords}\")\n+\n+# Case 2: Multi-dim variables -> MultiIndex with >1 level  \n+arr1 = xr.DataArray(np.arange(6).reshape(2, 3), coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])])\n+arr2 = xr.DataArray(np.arange(2), coords=[(\"x\", [0, 1])])\n+data2 = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+stacked2 = data2.to_stacked_array('z', sample_dims=['x'])\n+print(f\"\\nCase 2 - MultiIndex levels: {len(stacked2.indexes['z'].levels)}\")\n+\n+# Check if 'z' is in coords after selection\n+idx2 = stacked2.indexes['z']\n+level_number2 = idx2._get_level_number(0)\n+variables2 = idx2.levels[level_number2]\n+variable_dim2 = idx2.names[level_number2]\n+\n+for k in variables2:\n+    selected = stacked2.sel({variable_dim2: k}).squeeze(drop=True)\n+    print(f\"  Selected {k}: 'z' in coords? {'z' in selected.coords}\")\n+\n+# So the issue is: when MultiIndex has only one level, \n+# after .sel({variable_dim: k}).squeeze(drop=True), \n+# the stacked dimension becomes a scalar coordinate\n+\n+# But we should check: is this coordinate always scalar?\n+# Let's test with 2D variables but only one stacking dim\n+print(\"\\n\\nCase 3: 2D variables but only one stacking dim\")\n+arr3 = xr.DataArray(np.arange(6).reshape(2, 3), coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])])\n+data3 = xr.Dataset({\"a\": arr3, \"b\": arr3})\n+stacked3 = data3.to_stacked_array('z', sample_dims=['x'])\n+print(f\"Case 3 - MultiIndex levels: {len(stacked3.indexes['z'].levels)}\")\n+\n+idx3 = stacked3.indexes['z']\n+level_number3 = idx3._get_level_number(0)\n+variables3 = idx3.levels[level_number3]\n+variable_dim3 = idx3.names[level_number3]\n+\n+for k in variables3:\n+    selected = stacked3.sel({variable_dim3: k}).squeeze(drop=True)\n+    print(f\"  Selected {k}: 'z' in coords? {'z' in selected.coords}\")\n+    if 'z' in selected.coords:\n+        print(f\"    'z' shape: {selected.coords['z'].shape}\")\n+        print(f\"    'z' is scalar? {selected.coords['z'].shape == ()}\")\n\\ No newline at end of file\ndiff --git a/test_multi_dim.py b/test_multi_dim.py\nnew file mode 100644\nindex 00000000..1b5e770c\n--- /dev/null\n+++ b/test_multi_dim.py\n@@ -0,0 +1,56 @@\n+import numpy as np\n+import xarray as xr\n+\n+# Test with multi-dimensional variables\n+arr1 = xr.DataArray(\n+    np.arange(6).reshape(2, 3),\n+    coords=[(\"x\", [0, 1]), (\"y\", [0, 1, 2])],\n+)\n+arr2 = xr.DataArray(\n+    np.arange(2),\n+    coords=[(\"x\", [0, 1])],\n+)\n+data = xr.Dataset({\"a\": arr1, \"b\": arr2})\n+print(\"Original dataset:\")\n+print(data)\n+print(\"\\nStacking with sample_dims=['x']...\")\n+stacked = data.to_stacked_array('z', sample_dims=['x'])\n+print(\"Stacked array:\")\n+print(stacked)\n+print(\"\\nUnstacking...\")\n+try:\n+    unstacked = stacked.to_unstacked_dataset('z')\n+    print(\"Success!\")\n+    print(unstacked)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+    import traceback\n+    traceback.print_exc()\n+\n+print(\"\\n\\nNow let's trace through what happens...\")\n+idx = stacked.indexes['z']\n+level_number = idx._get_level_number(0)\n+variables = idx.levels[level_number]\n+variable_dim = idx.names[level_number]\n+\n+print(f\"\\nVariable dim: {variable_dim}\")\n+print(f\"Variables: {variables}\")\n+\n+for k in variables:\n+    selected = stacked.sel({variable_dim: k})\n+    print(f\"\\nSelected {k}:\")\n+    print(f\"  Shape: {selected.shape}\")\n+    print(f\"  Dims: {selected.dims}\")\n+    print(f\"  Coords: {selected.coords}\")\n+    print(f\"  Has 'z' coord: {'z' in selected.coords}\")\n+    if 'z' in selected.coords:\n+        print(f\"  z coord value: {selected.coords['z'].values}\")\n+    \n+    squeezed = selected.squeeze(drop=True)\n+    print(f\"\\nAfter squeeze:\")\n+    print(f\"  Shape: {squeezed.shape}\")\n+    print(f\"  Dims: {squeezed.dims}\")\n+    print(f\"  Coords: {squeezed.coords}\")\n+    print(f\"  Has 'z' coord: {'z' in squeezed.coords}\")\n+    if 'z' in squeezed.coords:\n+        print(f\"  z coord value: {squeezed.coords['z'].values}\")\n\\ No newline at end of file\ndiff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex b0df8749..8ae2773c 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -1962,6 +1962,10 @@ class DataArray(AbstractArray, DataWithCoords):\n         data_dict = {}\n         for k in variables:\n             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n+            # Drop the stacked dimension coordinate if it exists as a scalar coordinate\n+            # This happens when the MultiIndex has only one level\n+            if dim in data_dict[k].coords and dim not in data_dict[k].dims:\n+                data_dict[k] = data_dict[k].drop_vars(dim)\n \n         # unstacked dataset\n         return Dataset(data_dict)\n"
    },
    "pydata__xarray-4356": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4356",
        "model_patch": "diff --git a/reproduce_error.py b/reproduce_error.py\nnew file mode 100644\nindex 00000000..3fc8757c\n--- /dev/null\n+++ b/reproduce_error.py\n@@ -0,0 +1,39 @@\n+import xarray as xr\n+import numpy as np\n+\n+# Test case from PR description\n+da = xr.DataArray([[1., 2, 3], [4, 5, 6]])\n+print(\"DataArray:\")\n+print(da)\n+print()\n+\n+# This should fail with current code\n+try:\n+    result = da.sum([\"dim_0\", \"dim_1\"], min_count=1)\n+    print(\"Result:\", result)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Let's also test with single dimension to see it works\n+print(\"\\nTesting with single dimension:\")\n+try:\n+    result = da.sum(\"dim_0\", min_count=1)\n+    print(\"Result:\", result)\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+# Test with numpy array directly to understand the issue\n+print(\"\\nTesting numpy behavior:\")\n+a = np.array([[1., 2, 3], [4, 5, 6]])\n+mask = np.isnan(a)\n+print(f\"mask.shape: {mask.shape}\")\n+print(f\"mask.shape[0]: {mask.shape[0]}\")\n+print(f\"mask.shape[(0, 1)]: This will fail\")\n+try:\n+    print(mask.shape[(0, 1)])\n+except Exception as e:\n+    print(f\"Error accessing mask.shape[(0, 1)]: {e}\")\n+\n+# Test np.take approach\n+print(f\"\\nnp.take(mask.shape, (0, 1)): {np.take(mask.shape, (0, 1))}\")\n+print(f\"np.take(mask.shape, (0, 1)).prod(): {np.take(mask.shape, (0, 1)).prod()}\")\n\\ No newline at end of file\n"
    },
    "pydata__xarray-4629": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4629",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..d918eafc\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,41 @@\n+import xarray as xr\n+\n+# Test case from the PR description\n+xds1 = xr.Dataset(attrs={'a':'b'})\n+xds2 = xr.Dataset(attrs={'a':'c'})\n+print(f\"a1: {xds1.attrs['a']}, a2: {xds2.attrs['a']}\")\n+\n+xds3 = xr.merge([xds1, xds2], combine_attrs='override')\n+print(f\"a1: {xds1.attrs['a']}, a2: {xds2.attrs['a']}, a3: {xds3.attrs['a']}\")\n+\n+xds3.attrs['a'] = 'd'\n+print(f\"a1: {xds1.attrs['a']}, a2: {xds2.attrs['a']}, a3: {xds3.attrs['a']}\")\n+\n+# Check if they're the same object\n+print(f\"\\nAre xds1.attrs and xds3.attrs the same object? {xds1.attrs is xds3.attrs}\")\n+print(f\"Are xds1.attrs and xds3.attrs equal? {xds1.attrs == xds3.attrs}\")\n+\n+# Also test other combine_attrs options to see if they have the same issue\n+print(\"\\n--- Testing combine_attrs='no_conflicts' ---\")\n+xds4 = xr.Dataset(attrs={'a':'b'})\n+xds5 = xr.Dataset(attrs={'a':'b'})  # Same value for 'no_conflicts' to work\n+xds6 = xr.merge([xds4, xds5], combine_attrs='no_conflicts')\n+xds6.attrs['a'] = 'e'\n+print(f\"a4: {xds4.attrs['a']}, a5: {xds5.attrs['a']}, a6: {xds6.attrs['a']}\")\n+print(f\"Are xds4.attrs and xds6.attrs the same object? {xds4.attrs is xds6.attrs}\")\n+\n+print(\"\\n--- Testing combine_attrs='identical' ---\")\n+xds7 = xr.Dataset(attrs={'a':'b'})\n+xds8 = xr.Dataset(attrs={'a':'b'})  # Same value for 'identical' to work\n+xds9 = xr.merge([xds7, xds8], combine_attrs='identical')\n+xds9.attrs['a'] = 'f'\n+print(f\"a7: {xds7.attrs['a']}, a8: {xds8.attrs['a']}, a9: {xds9.attrs['a']}\")\n+print(f\"Are xds7.attrs and xds9.attrs the same object? {xds7.attrs is xds9.attrs}\")\n+\n+print(\"\\n--- Testing combine_attrs='drop' ---\")\n+xds10 = xr.Dataset(attrs={'a':'b'})\n+xds11 = xr.Dataset(attrs={'a':'c'})\n+xds12 = xr.merge([xds10, xds11], combine_attrs='drop')\n+print(f\"xds12.attrs: {xds12.attrs}\")\n+xds12.attrs['new'] = 'value'\n+print(f\"After modifying xds12.attrs: xds10.attrs['a'] = {xds10.attrs.get('a')}\")\n\\ No newline at end of file\ndiff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex ca4e29b6..dff00804 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -501,7 +501,7 @@ def merge_attrs(variable_attrs, combine_attrs):\n     if combine_attrs == \"drop\":\n         return {}\n     elif combine_attrs == \"override\":\n-        return variable_attrs[0]\n+        return dict(variable_attrs[0])\n     elif combine_attrs == \"no_conflicts\":\n         result = dict(variable_attrs[0])\n         for attrs in variable_attrs[1:]:\n"
    },
    "pydata__xarray-4687": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4687",
        "model_patch": ""
    },
    "pydata__xarray-4695": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4695",
        "model_patch": "diff --git a/reproduce_error.py b/reproduce_error.py\nnew file mode 100644\nindex 00000000..f997b2bd\n--- /dev/null\n+++ b/reproduce_error.py\n@@ -0,0 +1,20 @@\n+import numpy as np\n+from xarray import DataArray\n+\n+empty = np.zeros((2,2))\n+D1 = DataArray(empty, dims=['dim1', 'dim2'],   coords={'dim1':['x', 'y'], 'dim2':['a', 'b']})\n+D2 = DataArray(empty, dims=['dim1', 'method'], coords={'dim1':['x', 'y'], 'method':['a', 'b']})\n+\n+print(\"Testing D1 with dims=['dim1', 'dim2']:\")\n+try:\n+    result = D1.loc[dict(dim1='x', dim2='a')]\n+    print(f\"Success: {result}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n+\n+print(\"\\nTesting D2 with dims=['dim1', 'method']:\")\n+try:\n+    result = D2.loc[dict(dim1='x', method='a')]\n+    print(f\"Success: {result}\")\n+except Exception as e:\n+    print(f\"Error: {type(e).__name__}: {e}\")\n\\ No newline at end of file\n"
    },
    "pydata__xarray-4966": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-4966",
        "model_patch": "diff --git a/debug_dtype.py b/debug_dtype.py\nnew file mode 100644\nindex 00000000..3266d0eb\n--- /dev/null\n+++ b/debug_dtype.py\n@@ -0,0 +1,57 @@\n+import xarray as xr\n+import numpy as np\n+from xarray.coding.variables import UnsignedIntegerCoder\n+\n+# Let's understand what's happening with the pydap engine\n+# The issue is that pydap returns unsigned bytes (uint8) but with _Unsigned='false'\n+# This means we need to convert uint8 to int8 when _Unsigned='false'\n+\n+print(\"Understanding the issue:\")\n+print(\"=\" * 80)\n+\n+# Simulate what pydap returns: unsigned bytes with _Unsigned='false'\n+print(\"\\n1. Simulating pydap output: uint8 data with _Unsigned='false'\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'false'})\n+print(f\"   Data dtype: {data.dtype}\")\n+print(f\"   Data values: {data}\")\n+print(f\"   Data kind: {data.dtype.kind}\")  # Should be 'u' for unsigned\n+\n+# Check current coder behavior\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"   Decoded dtype: {decoded.dtype}\")\n+print(f\"   Decoded values: {decoded.values}\")\n+\n+# The issue is in the decode method of UnsignedIntegerCoder\n+# It only handles the case when data.dtype.kind == \"i\" and unsigned == \"true\"\n+# But we need to handle when data.dtype.kind == \"u\" and unsigned == \"false\"\n+\n+print(\"\\n2. Looking at the current logic in UnsignedIntegerCoder.decode():\")\n+print(\"   - Line 311: if data.dtype.kind == 'i':\")\n+print(\"   - Line 312: if unsigned == 'true': convert to unsigned\")\n+print(\"   - Line 319-325: else: warning about non-integer type\")\n+print(\"\\n   The problem: we need to handle data.dtype.kind == 'u' and unsigned == 'false'\")\n+\n+print(\"\\n3. Testing what should happen:\")\n+print(\"   When we have uint8 with _Unsigned='false', we should convert to int8\")\n+print(\"   This means values > 127 should become negative\")\n+\n+# Let's manually test the conversion\n+print(\"\\n4. Manual conversion test:\")\n+uint8_data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+print(f\"   Original uint8: {uint8_data}\")\n+# Convert to int8 view\n+int8_data = uint8_data.astype(np.int8)\n+print(f\"   As int8: {int8_data}\")\n+print(f\"   This is correct: 128 -> -128, 255 -> -1\")\n+\n+print(\"\\n5. Testing the fix:\")\n+print(\"   We need to add a case in UnsignedIntegerCoder.decode():\")\n+print(\"   if data.dtype.kind == 'u' and unsigned == 'false':\")\n+print(\"       convert to signed dtype\")\n+\n+# Let's also check the encode method\n+print(\"\\n6. Checking encode method:\")\n+print(\"   The encode method handles _Unsigned='true' for signed->unsigned conversion\")\n+print(\"   It should also handle _Unsigned='false' for unsigned->signed conversion\")\n\\ No newline at end of file\ndiff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..a624a37a\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,79 @@\n+import xarray as xr\n+import numpy as np\n+\n+# Test URL from the PR description\n+test_url = \"https://observations.ipsl.fr/thredds/dodsC/EUREC4A/PRODUCTS/testdata/netcdf_testfiles/test_NC_BYTE_neg.nc\"\n+\n+print(\"Testing with netcdf4 engine:\")\n+try:\n+    ds_nc4 = xr.open_dataset(test_url, engine=\"netcdf4\")\n+    print(f\"Values with netcdf4: {ds_nc4['test'].values}\")\n+except Exception as e:\n+    print(f\"Error with netcdf4: {e}\")\n+\n+print(\"\\nTesting with pydap engine:\")\n+try:\n+    ds_pydap = xr.open_dataset(test_url, engine=\"pydap\")\n+    print(f\"Values with pydap: {ds_pydap['test'].values}\")\n+except Exception as e:\n+    print(f\"Error with pydap: {e}\")\n+\n+# Let's also create a simple test file to understand the issue better\n+print(\"\\n\\nCreating a test file to understand the issue...\")\n+import netCDF4 as nc\n+\n+# Create a test file with signed bytes\n+with nc.Dataset('/testbed/test_signed_bytes.nc', 'w') as f:\n+    f.createDimension('test', 7)\n+    var = f.createVariable('test', 'b', ('test',))\n+    var[:] = np.array([-128, -1, 0, 1, 2, -99, 127], dtype=np.int8)\n+    # Add _Unsigned=False attribute as per the hack\n+    var._Unsigned = 'false'\n+    \n+print(\"Created test file with signed bytes and _Unsigned='false'\")\n+\n+# Now let's test reading it back\n+print(\"\\nReading test file with netcdf4:\")\n+ds_test_nc4 = xr.open_dataset('/testbed/test_signed_bytes.nc', engine='netcdf4')\n+print(f\"Values: {ds_test_nc4['test'].values}\")\n+print(f\"Attributes: {ds_test_nc4['test'].attrs}\")\n+\n+print(\"\\nReading test file with pydap (if available):\")\n+try:\n+    import pydap\n+    # We can't test pydap with local file easily, but let's check the logic\n+    print(\"Pydap available, but testing local file might not work\")\n+except ImportError:\n+    print(\"Pydap not available\")\n+\n+# Let's also test the current UnsignedIntegerCoder behavior\n+print(\"\\n\\nTesting current UnsignedIntegerCoder behavior:\")\n+from xarray.coding.variables import UnsignedIntegerCoder\n+import xarray as xr\n+\n+# Test case 1: unsigned bytes with _Unsigned='true'\n+print(\"\\nTest 1: unsigned bytes with _Unsigned='true'\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"Original dtype: {data.dtype}, Decoded dtype: {decoded.dtype}\")\n+print(f\"Original values: {data}, Decoded values: {decoded.values}\")\n+\n+# Test case 2: signed bytes with _Unsigned='false' (the new case)\n+print(\"\\nTest 2: signed bytes with _Unsigned='false' (should be handled)\")\n+data = np.array([-128, -1, 0, 1, 127], dtype=np.int8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'false'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"Original dtype: {data.dtype}, Decoded dtype: {decoded.dtype}\")\n+print(f\"Original values: {data}, Decoded values: {decoded.values}\")\n+\n+# Test case 3: unsigned bytes with _Unsigned='false' (should convert to signed)\n+print(\"\\nTest 3: unsigned bytes with _Unsigned='false' (should convert to signed)\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'false'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"Original dtype: {data.dtype}, Decoded dtype: {decoded.dtype}\")\n+print(f\"Original values: {data}, Decoded values: {decoded.values}\")\n\\ No newline at end of file\ndiff --git a/test_signed_bytes.nc b/test_signed_bytes.nc\nnew file mode 100644\nindex 00000000..ec3655aa\nBinary files /dev/null and b/test_signed_bytes.nc differ\ndiff --git a/test_unsigned_fix.py b/test_unsigned_fix.py\nnew file mode 100644\nindex 00000000..830dccc1\n--- /dev/null\n+++ b/test_unsigned_fix.py\n@@ -0,0 +1,71 @@\n+import xarray as xr\n+import numpy as np\n+from xarray.coding.variables import UnsignedIntegerCoder\n+\n+# Test the current behavior and what we need to fix\n+print(\"Testing UnsignedIntegerCoder behavior\")\n+print(\"=\" * 80)\n+\n+# Test 1: Current behavior - signed int with _Unsigned='true'\n+print(\"\\nTest 1: Signed int8 with _Unsigned='true' (already works)\")\n+data = np.array([-128, -1, 0, 1, 127], dtype=np.int8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {decoded.dtype}, values: {decoded.values}\")\n+print(f\"  Expected: uint8 [128, 255, 0, 1, 127]\")\n+\n+# Test 2: New case - unsigned int with _Unsigned='false' (needs fixing)\n+print(\"\\nTest 2: Unsigned uint8 with _Unsigned='false' (needs fixing)\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'false'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {decoded.dtype}, values: {decoded.values}\")\n+print(f\"  Expected: int8 [0, 127, -128, -1]\")\n+\n+# Test 3: Signed int with _Unsigned='false' (should stay signed)\n+print(\"\\nTest 3: Signed int8 with _Unsigned='false' (should stay signed)\")\n+data = np.array([-128, -1, 0, 1, 127], dtype=np.int8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'false'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {decoded.dtype}, values: {decoded.values}\")\n+print(f\"  Expected: int8 [-128, -1, 0, 1, 127]\")\n+\n+# Test 4: Unsigned int with _Unsigned='true' (should stay unsigned)\n+print(\"\\nTest 4: Unsigned uint8 with _Unsigned='true' (should stay unsigned)\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {decoded.dtype}, values: {decoded.values}\")\n+print(f\"  Expected: uint8 [0, 127, 128, 255]\")\n+\n+# Test 5: Non-integer type with _Unsigned attribute (should warn)\n+print(\"\\nTest 5: Float with _Unsigned='true' (should warn)\")\n+data = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n+var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n+coder = UnsignedIntegerCoder()\n+decoded = coder.decode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {decoded.dtype}, values: {decoded.values}\")\n+print(f\"  Expected: float32 [1.0, 2.0, 3.0] with warning\")\n+\n+# Test encode method too\n+print(\"\\n\\nTesting encode method:\")\n+print(\"=\" * 80)\n+\n+# Test encode: unsigned with _Unsigned='false' in encoding\n+print(\"\\nTest encode: Unsigned uint8 with encoding _Unsigned='false'\")\n+data = np.array([0, 127, 128, 255], dtype=np.uint8)\n+var = xr.Variable('x', data, encoding={'_Unsigned': 'false'})\n+coder = UnsignedIntegerCoder()\n+encoded = coder.encode(var, name='test')\n+print(f\"  Input dtype: {data.dtype}, values: {data}\")\n+print(f\"  Output dtype: {encoded.dtype}, values: {encoded.values}\")\n+print(f\"  Expected: int8 [0, 127, -128, -1]\")\n\\ No newline at end of file\ndiff --git a/xarray/coding/variables.py b/xarray/coding/variables.py\nindex b035ff82..dd6bddd2 100644\n--- a/xarray/coding/variables.py\n+++ b/xarray/coding/variables.py\n@@ -292,13 +292,23 @@ class UnsignedIntegerCoder(VariableCoder):\n         # https://www.unidata.ucar.edu/software/netcdf/docs/BestPractices.html\n         #     \"_Unsigned = \"true\" to indicate that\n         #      integer data should be treated as unsigned\"\n-        if encoding.get(\"_Unsigned\", \"false\") == \"true\":\n-            pop_to(encoding, attrs, \"_Unsigned\")\n-            signed_dtype = np.dtype(\"i%s\" % data.dtype.itemsize)\n-            if \"_FillValue\" in attrs:\n-                new_fill = signed_dtype.type(attrs[\"_FillValue\"])\n-                attrs[\"_FillValue\"] = new_fill\n-            data = duck_array_ops.around(data).astype(signed_dtype)\n+        if \"_Unsigned\" in encoding:\n+            unsigned = pop_to(encoding, attrs, \"_Unsigned\")\n+            \n+            if unsigned == \"true\":\n+                # Convert signed to unsigned\n+                signed_dtype = np.dtype(\"i%s\" % data.dtype.itemsize)\n+                if \"_FillValue\" in attrs:\n+                    new_fill = signed_dtype.type(attrs[\"_FillValue\"])\n+                    attrs[\"_FillValue\"] = new_fill\n+                data = duck_array_ops.around(data).astype(signed_dtype)\n+            elif unsigned == \"false\" and data.dtype.kind == \"u\":\n+                # Convert unsigned to signed (for OPeNDAP hack)\n+                signed_dtype = np.dtype(\"i%s\" % data.dtype.itemsize)\n+                if \"_FillValue\" in attrs:\n+                    new_fill = signed_dtype.type(attrs[\"_FillValue\"])\n+                    attrs[\"_FillValue\"] = new_fill\n+                data = duck_array_ops.around(data).astype(signed_dtype)\n \n         return Variable(dims, data, attrs, encoding)\n \n@@ -316,6 +326,14 @@ class UnsignedIntegerCoder(VariableCoder):\n                     if \"_FillValue\" in attrs:\n                         new_fill = unsigned_dtype.type(attrs[\"_FillValue\"])\n                         attrs[\"_FillValue\"] = new_fill\n+            elif data.dtype.kind == \"u\":\n+                if unsigned == \"false\":\n+                    signed_dtype = np.dtype(\"i%s\" % data.dtype.itemsize)\n+                    transform = partial(np.asarray, dtype=signed_dtype)\n+                    data = lazy_elemwise_func(data, transform, signed_dtype)\n+                    if \"_FillValue\" in attrs:\n+                        new_fill = signed_dtype.type(attrs[\"_FillValue\"])\n+                        attrs[\"_FillValue\"] = new_fill\n             else:\n                 warnings.warn(\n                     \"variable %r has _Unsigned attribute but is not \"\n"
    },
    "pydata__xarray-6461": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-6461",
        "model_patch": ""
    },
    "pydata__xarray-6599": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-6599",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..cdc8202b\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,40 @@\n+import xarray as xr\n+import numpy as np\n+\n+values = np.array(\n+    [\n+        \"2021-04-01T05:25:19.000000000\",\n+        \"2021-04-01T05:25:29.000000000\",\n+        \"2021-04-01T05:25:39.000000000\",\n+        \"2021-04-01T05:25:49.000000000\",\n+        \"2021-04-01T05:25:59.000000000\",\n+        \"2021-04-01T05:26:09.000000000\",\n+    ],\n+    dtype=\"datetime64[ns]\",\n+)\n+azimuth_time = xr.DataArray(\n+    values, name=\"azimuth_time\", coords={\"azimuth_time\": values - values[0]}\n+)\n+\n+polyfit_coefficients = xr.DataArray(\n+    [\n+        [2.33333335e-43, 1.62499999e-43, 2.79166678e-43],\n+        [-1.15316667e-30, 1.49518518e-31, 9.08833333e-31],\n+        [-2.50272583e-18, -1.23851062e-18, -2.99098229e-18],\n+        [5.83965193e-06, -1.53321770e-07, -4.84640242e-06],\n+        [4.44739216e06, 1.45053974e06, 5.29960857e06],\n+    ],\n+    dims=(\"degree\", \"axis\"),\n+    coords={\"axis\": [0, 1, 2], \"degree\": [4, 3, 2, 1, 0]},\n+)\n+\n+print(\"Testing polyval with timedelta64 coordinates:\")\n+print(\"=\" * 60)\n+result = xr.polyval(azimuth_time, polyfit_coefficients)\n+print(result)\n+print(\"\\nResult shape:\", result.shape)\n+print(\"Result dtype:\", result.dtype)\n+print(\"Result coordinates:\")\n+print(result.coords)\n+print(\"\\nFirst few values:\")\n+print(result.values[:3, :3] if result.shape[0] > 2 else result.values)\n\\ No newline at end of file\n"
    },
    "pydata__xarray-6721": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-6721",
        "model_patch": ""
    },
    "pydata__xarray-6744": {
        "model_name_or_path": "openrouter_deepseek_deepseek-v3.2",
        "instance_id": "pydata__xarray-6744",
        "model_patch": "diff --git a/reproduce_issue.py b/reproduce_issue.py\nnew file mode 100644\nindex 00000000..27fca561\n--- /dev/null\n+++ b/reproduce_issue.py\n@@ -0,0 +1,38 @@\n+import xarray as xr\n+import numpy as np\n+\n+print(\"Testing the issue where 'center' kwarg is ignored when manually iterating over DataArrayRolling\")\n+print(\"=\" * 80)\n+\n+# Create test data\n+my_data = xr.DataArray(np.arange(1, 10), dims=\"x\")\n+\n+print(\"Original data:\")\n+print(my_data.values)\n+print()\n+\n+# Option 1: take a center-justified rolling average\n+print(\"Option 1: take a center-justified rolling average\")\n+result1 = my_data.rolling(x=3, center=True).mean().values\n+print(\"result1 = my_data.rolling(x=3, center=True).mean().values\")\n+print(f\"Result: {result1}\")\n+print()\n+\n+# Option 2: try to manually iterate, but the result is not centered\n+print(\"Option 2: try to manually iterate, but the result is not centered\")\n+my_data_rolling = my_data.rolling(x=3, center=True)\n+result2 = [window.mean().values.item() for label, window in my_data_rolling]\n+print(\"result2 = [window.mean().values.item() for label, window in my_data_rolling]\")\n+print(f\"Result: {result2}\")\n+print()\n+\n+# Check if they match\n+print(\"Do they match?\")\n+print(f\"Arrays equal: {np.array_equal(result1, result2, equal_nan=True)}\")\n+print()\n+\n+# Let's also check what windows we get when iterating\n+print(\"Let's examine the windows we get when iterating:\")\n+my_data_rolling = my_data.rolling(x=3, center=True)\n+for i, (label, window) in enumerate(my_data_rolling):\n+    print(f\"Window {i} (label={label}): {window.values} -> mean = {window.mean().values.item()}\")\n\\ No newline at end of file\n"
    }
}